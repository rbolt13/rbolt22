[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Coming Soon…"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nArticle Review\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPackage Building\n\n\nR\n\n\nNBA\n\n\nSportsObserveR\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nWeb-Scraping\n\n\nNBA\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNBA\n\n\nWeb-Scraping\n\n\nR\n\n\nSportsObserveR\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nNBA\n\n\nWeb-Scraping\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGeometry\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nNLP\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGeometry\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nGeometry\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nProof\n\n\nStatistics\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nConditional Probabilty\n\n\nTree Diagrams\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nGenerative Art\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidycensus\n\n\nAPI\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGeometry\n\n\nMath\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2021\n\n\nRandi Bolt\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGeometry\n\n\nMath\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2021\n\n\nRandi Bolt\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2021\n\n\nRandi Bolt\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "I am a data analyst who uses data science and machine learning techniques to extrapolate insight from data.\nI am constantly exploring new topics, and ways to improve upon past and current work.\nI graduated magna cum laude with a BS in Mathematics from Portland State University in 2022.\nI was born, raised, and am currently based out of Portland, Oregon.\n私は日本語を勉強しています。"
  },
  {
    "objectID": "01_blog/2022_11_21_Quartro/index.html",
    "href": "01_blog/2022_11_21_Quartro/index.html",
    "title": "Quarto Links",
    "section": "",
    "text": "Quarto LinksAccessibilityQuarto BlogsOther Blogs\n\n\nA Quarto tip a day\nCreating a blog with Quarto in 10 steps - Bea Milz\nCreate your Data Science portfolio with Quarto: Slides\nFrom R Markdown to Quarto\nHow to style your Quarto blog without knowing a lot of HTML/CSS\nInstalling and Configuring Python with RStudio\nManaging Execution\nQuarto: Creating a Blog (Documentation)\nQuarto/RMarkdown - What’s Different: Slides Repo\nquarto-trio (Using Python, R, and Apache Arrow)\n\n\na11Y - digital accessibility\nWAVE - Web Accessibility Evaluation Tool\n\n\nAbhirup Moitra\nAlbert Rapp\nBea Milz: Repo\nDaniel Tran: Repo\nDanielle Navarro - Notes from a data witch: Repo\nDeesha menghani: Repo\nJesse Adler\nMeghan Harris: Repo\nRober Mitchell: Repo\nSam Csik: Repo\nTed Landeras, PhD: repo\n\n\nAlicia Johnson (Made with blogdown))\nAllison Hill, PhD\nCrystal Lewis\nGreg Wilson\nDavid Neuzerling: Repo\nJenny Bryan (Made with Blogdown and Hugo): Repo\nJon Harmon\nJulia Silge (Made with Blogdown and Hugo): Repo\nMachine Learning Mastery!\nNan Xiao: Repo\nNicola Rennie: Repo\nTanner Heffner (Made with Svelte): Repo\nYanina Bellini Saibene\nYihui Xie"
  },
  {
    "objectID": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html",
    "href": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "",
    "text": "Scraping and cleaning NBA Salary data from ESPN - NBA Players Salaries, with some simple statistic and data visuals."
  },
  {
    "objectID": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html#change-salaray-from-character-to-numeric",
    "href": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html#change-salaray-from-character-to-numeric",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "4.1 Change Salaray from Character to Numeric",
    "text": "4.1 Change Salaray from Character to Numeric\nNotice that the SALARY column is a character value. This will not be helpful when trying to do math, or make graphs with this numerical data. To change this 3 things must be addressed:\n\nRemoving the dollar sign.\nRemoving the commas.\nChange character type to numeric.\n\n\nnba_salaries_2023$SALARY <- str_remove_all(nba_salaries_2023$SALARY,\n                    \"\\\\$\")\nnba_salaries_2023$SALARY <- str_remove_all(nba_salaries_2023$SALARY,\n                    \",\")\nnba_salaries_2023$SALARY <- as.numeric(nba_salaries_2023$SALARY)\n\nNow we are able to do math, make graphs, and arrange the data by salary."
  },
  {
    "objectID": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html#basic-statistics",
    "href": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html#basic-statistics",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "4.2 Basic Statistics",
    "text": "4.2 Basic Statistics\n\n\n\n\nHighest paid value : 48,070,014\nLowest paid value : 5,318\nMedian : 3,887,212\nMean : 8,432,334\nStandard Deviation : 10,203,700"
  },
  {
    "objectID": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html#box-plots",
    "href": "01_blog/2022_12_19_NBA-Salaries-Part-1-Web-Scraping/index.html#box-plots",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "4.3 Box Plots",
    "text": "4.3 Box Plots\n\n4.3.1 2022 - 2023 Yearly Salary by Postion\n\nggplot2::ggplot(data = nba_salaries_2023,\n                mapping = ggplot2::aes(x = SALARY,\n                                       y = POSITION)) + \n  ggplot2::geom_boxplot()\n\n\n\n\nFrom this visual we can see that Point Guards (PG) appear to be paid the most, while Guards (G) and Forwards (F) on average are paid the least.\n\n\n4.4 2022-2023 Yearly Salary by Team\n\nggplot2::ggplot(data = nba_salaries_2023,\n                mapping = ggplot2::aes(x = SALARY,\n                                       y = TEAM)) + \n  ggplot2::geom_boxplot()\n\n\n\n\nThis plot is not the easiest to read, and might be worth sub-setting the information further. However eye-balling this visual we can see most teams pay between $2,000,000 and $15,000,000 per player with a few outliers. These outliers of course being superstar players."
  },
  {
    "objectID": "01_blog/2023_01_09_Decision-Tree/index.html",
    "href": "01_blog/2023_01_09_Decision-Tree/index.html",
    "title": "Decision Trees - Shipwreak Data",
    "section": "",
    "text": "This is ….\n[picture]"
  },
  {
    "objectID": "01_blog/2023_01_09_Decision-Tree/index.html#load-packages",
    "href": "01_blog/2023_01_09_Decision-Tree/index.html#load-packages",
    "title": "Decision Trees - Shipwreak Data",
    "section": "Load Packages",
    "text": "Load Packages\n\nimport numpy as np\nimport pandas as pd\nimport regex\nimport xgboost as xgb\nimport seaborn as sns\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont"
  },
  {
    "objectID": "01_blog/2023_01_09_Decision-Tree/index.html#data",
    "href": "01_blog/2023_01_09_Decision-Tree/index.html#data",
    "title": "Decision Trees - Shipwreak Data",
    "section": "Data",
    "text": "Data\n\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import Series, DataFrame\n\n\ntitanic = pd.read_csv(\"../../00_data/titanic_data.csv\")\ntitanic.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\n\ntitanic.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S"
  },
  {
    "objectID": "01_blog/2023_01_09_Decision-Tree/index.html#split-data",
    "href": "01_blog/2023_01_09_Decision-Tree/index.html#split-data",
    "title": "Decision Trees - Shipwreak Data",
    "section": "split data",
    "text": "split data\n\ny=titanic.Survived\nx=titanic.drop('Survived', axis=1)\n\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\nx_train.head\n\n<bound method NDFrame.head of      PassengerId  Pclass                                  Name     Sex   Age  \\\n95            96       3           Shorney, Mr. Charles Joseph    male   NaN   \n123          124       2                   Webber, Miss. Susan  female  32.5   \n151          152       1     Pears, Mrs. Thomas (Edith Wearne)  female  22.0   \n162          163       3            Bengtsson, Mr. John Viktor    male  26.0   \n870          871       3                     Balkic, Mr. Cerin    male  26.0   \n..           ...     ...                                   ...     ...   ...   \n249          250       2         Carter, Rev. Ernest Courtenay    male  54.0   \n890          891       3                   Dooley, Mr. Patrick    male  32.0   \n490          491       3  Hagland, Mr. Konrad Mathias Reiersen    male   NaN   \n110          111       1        Porter, Mr. Walter Chamberlain    male  47.0   \n36            37       3                      Mamee, Mr. Hanna    male   NaN   \n\n     SibSp  Parch  Ticket     Fare Cabin Embarked  \n95       0      0  374910   8.0500   NaN        S  \n123      0      0   27267  13.0000  E101        S  \n151      1      0  113776  66.6000    C2        S  \n162      0      0  347068   7.7750   NaN        S  \n870      0      0  349248   7.8958   NaN        S  \n..     ...    ...     ...      ...   ...      ...  \n249      1      0  244252  26.0000   NaN        S  \n890      0      0  370376   7.7500   NaN        Q  \n490      1      0   65304  19.9667   NaN        S  \n110      0      0  110465  52.0000  C110        S  \n36       0      0    2677   7.2292   NaN        C  \n\n[712 rows x 11 columns]>"
  },
  {
    "objectID": "about.html#books-im-currently-reading",
    "href": "about.html#books-im-currently-reading",
    "title": "Randi Bolt",
    "section": "Books I’m Currently Reading",
    "text": "Books I’m Currently Reading"
  },
  {
    "objectID": "about.html#music-im-listening-to",
    "href": "about.html#music-im-listening-to",
    "title": "Randi Bolt",
    "section": "Music I’m Listening To",
    "text": "Music I’m Listening To\nFace Your Fears - Crazy Ex Girlfriend\nHer Morning Elegance\nNever Thought - Mel Bryant\nPudgy - Smino"
  },
  {
    "objectID": "about.html#movies-ive-watched-recently",
    "href": "about.html#movies-ive-watched-recently",
    "title": "About Randi",
    "section": "Movies I’ve Watched Recently",
    "text": "Movies I’ve Watched Recently\nThe Terminator (1,2,3)\nPearl\nX\nThe Father"
  },
  {
    "objectID": "about.html#movies-ive-recently-watched",
    "href": "about.html#movies-ive-recently-watched",
    "title": "Randi Bolt",
    "section": "Movies I’ve Recently Watched",
    "text": "Movies I’ve Recently Watched\nThe Terminator (1,2,3)\nPearl\nX\nThe Father"
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "",
    "text": "The problems on this page are from Probability & Statistics for Engineering & Sciences 9th Edition by Jay L. Devore - Duxbury Publisher, and the work is mine."
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#consider-the-strength-data-for-beams-given-in-example-1.2.",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#consider-the-strength-data-for-beams-given-in-example-1.2.",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.10 : Consider the strength data for beams given in Example 1.2.",
    "text": "1.10 : Consider the strength data for beams given in Example 1.2.\n\na. Construct a stem-and leaf display of the data. What appears to be a representative strength value? Do the observations appear to be highly concentrated about the representative value or rather spread out?\n\nbeam <- c(5.9, 7.2, 7.3, 6.3, 8.1, 6.8, 7.0, 7.6, 6.8, 6.5, 7.0, 6.3, 7.9, 9.0,\n         8.2, 8.7, 7.8, 9.7, 7.4, 7.7, 9.7, 7.8, 7.7, 11.6, 11.3, 11.8, 10.7)\nstem(beam)\n\n\n  The decimal point is at the |\n\n   5 | 9\n   6 | 33588\n   7 | 00234677889\n   8 | 127\n   9 | 077\n  10 | 7\n  11 | 368\n\n\nA representative strength value would be 7.7, as more observations are concentrated around this value than any other.\n\nDoes the display appear to be reasonably symmetric about a representative value, or would you describe its shape in some other way?\n\n\n\n\n\n\nNo, I would argue that the data has a slight positive skewed to the right. I say this because the higher frequency values (in blues) seem to be to the left of the representative value (indicated by the red line), which is to the left of the mean (indicated by the orange line).\n\nDo there appear to be any outlying strength values?\n\nYes there seems to be a small density of values outlying around 11.\n\nWhat proportion of strength observations in this sample exceed 10 MPa?\n\n\n\n[1] \"14.81 %\"\n\n\nAn unlikely 14.81% of values exceed 10 MPa."
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#consider-the-strength-data-for-beams-given-below.",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#consider-the-strength-data-for-beams-given-below.",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.10 : Consider the strength data for beams given below.",
    "text": "1.10 : Consider the strength data for beams given below.\n\na. Construct a stem-and leaf display of the data. What appears to be a representative strength value? Do the observations appear to be highly concentrated about the representative value or rather spread out?\n\nbeam <- c(5.9, 7.2, 7.3, 6.3, 8.1, 6.8, 7.0, 7.6, 6.8, 6.5, 7.0, 6.3, 7.9, 9.0,\n         8.2, 8.7, 7.8, 9.7, 7.4, 7.7, 9.7, 7.8, 7.7, 11.6, 11.3, 11.8, 10.7)\nstem(beam)\n\n\n  The decimal point is at the |\n\n   5 | 9\n   6 | 33588\n   7 | 00234677889\n   8 | 127\n   9 | 077\n  10 | 7\n  11 | 368\n\n\nA representative strength value would be 7.7, as more observations are concentrated around this value than any other.\n\nDoes the display appear to be reasonably symmetric about a representative value, or would you describe its shape in some other way?\n\n\n\n\n\n\nNo, I would argue that the data has a slight positive skewed to the right. I say this because the higher frequency values (in blues) seem to be to the left of the representative value (indicated by the red line), which is to the left of the mean (indicated by the orange line).\n\nDo there appear to be any outlying strength values?\n\nYes there seems to be a small density of values outlying around 11.\n\nWhat proportion of strength observations in this sample exceed 10 MPa?\n\n\n\n[1] \"14.81 %\"\n\n\nAn unlikely 14.81% of values exceed 10 MPa."
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#consider-the-strength-data-for-beams.",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#consider-the-strength-data-for-beams.",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.10 Consider the strength data for beams.",
    "text": "1.10 Consider the strength data for beams.\n\na. Construct a stem-and leaf display of the data. What appears to be a representative strength value? Do the observations appear to be highly concentrated about the representative value or rather spread out?\n\nbeam <- c(5.9, 7.2, 7.3, 6.3, 8.1, 6.8, 7.0, 7.6, 6.8, 6.5, 7.0, 6.3, 7.9, 9.0,\n         8.2, 8.7, 7.8, 9.7, 7.4, 7.7, 9.7, 7.8, 7.7, 11.6, 11.3, 11.8, 10.7)\nstem(beam)\n\n\n  The decimal point is at the |\n\n   5 | 9\n   6 | 33588\n   7 | 00234677889\n   8 | 127\n   9 | 077\n  10 | 7\n  11 | 368\n\n\nA representative strength value would be 7.7, as more observations are concentrated around this value than any other.\n\n\nb. Does the display appear to be reasonably symmetric about a representative value, or would you describe its shape in some other way?\n\n\n\n\n\nNo, I would argue that the data has a slight positive skewed to the right. I say this because the higher frequency values (in blues) seem to be to the left of the representative value (indicated by the red line), which is to the left of the mean (indicated by the orange line).\n\n\nc. Do there appear to be any outlying strength values?\nYes there seems to be a small density of values outlying around 11.\n\n\nd. What proportion of strength observations in this sample exceed 10 MPa?\n\n\n[1] \"14.81 %\"\n\n\nAn unlikely 14.81% of values exceed 10 MPa."
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#the-accompanying-specific-gravity-values-for-various-wood-types-used-in-construction-appeared-in-the-article-bolted-connection-design-values-based-on-european-yield-model-j.-of-structural-engr.-1993-2169-2186",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#the-accompanying-specific-gravity-values-for-various-wood-types-used-in-construction-appeared-in-the-article-bolted-connection-design-values-based-on-european-yield-model-j.-of-structural-engr.-1993-2169-2186",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.12 The accompanying specific gravity values for various wood types used in construction appeared in the article “Bolted Connection Design Values Based on European Yield Model” (J. of Structural Engr., 1993: 2169-2186):",
    "text": "1.12 The accompanying specific gravity values for various wood types used in construction appeared in the article “Bolted Connection Design Values Based on European Yield Model” (J. of Structural Engr., 1993: 2169-2186):\n\nwood.g <- c(.31, .35, .36, .36, .37, .38, .40, .40, .40,\n            .41, .41, .42, .42, .42, .42, .42, .43, .44,\n            .45, .46, .46, .47, .48, .48, .48, .51, .54,\n            .54, .55, .58, .62, .66, .66, .67, .68, .75)\n\n\nConstruct a stem-and-leaf display using repeated stems (see the previous exercise), and comment on any interesting features of the display.\n\n\n\n  The decimal point is at the |\n\n  0 | 344444444444444444\n  0 | 555555555566677778\n\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  3 | 156678\n  4 | 0001122222345667888\n  5 | 14458\n  6 | 26678\n  7 | 5\n\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  3 | 1\n  3 | 56678\n  4 | 000112222234\n  4 | 5667888\n  5 | 144\n  5 | 58\n  6 | 2\n  6 | 6678\n  7 | \n  7 | 5\n\n\nLooking at the three stem-and-leaf displays there are a few interesting features about the gravity for various wood types used in construction data that stands out. Dropping the last digit in the data, the first stem-and-leaf display shows us that there’s an even number of values that are >0.5 and 0.5that appear most dense around 0.4. The second and third stem-and-leaf displays show us that these values are most dense about 0.42, with a small blip around 6.6 and obvious outlier for value 0.75."
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#the-article-cited-in-example-1.2-also-gave-the-accompanying-strengths-observations-for-cylinders",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#the-article-cited-in-example-1.2-also-gave-the-accompanying-strengths-observations-for-cylinders",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.16 The article cited in Example 1.2 also gave the accompanying strengths observations for cylinders:",
    "text": "1.16 The article cited in Example 1.2 also gave the accompanying strengths observations for cylinders:\n\ncylinders <- c(6.1, 5.8, 7.8, 7.1, 7.2, 9.2, 6.6, 8.3, 7.0, 8.3,\n           7.8, 8.1, 7.4, 8.5, 8.9, 9.8, 9.7, 14.1, 12.6, 11.2)\n\n\na. Construct a comparative stem-and-leaf display (see the previous exercise) of the beam and cylinder data and then answer the questions in parts (b)-(d) of Exercise 10 for the observations on cylinders.\n\n\n\n  The decimal point is at the |\n\n   5 | 8\n   6 | 16\n   7 | 012488\n   8 | 13359\n   9 | 278\n  10 | \n  11 | 2\n  12 | 6\n  13 | \n  14 | 1\n\n\n\nDoes the display appear to be reasonably symmetric about a representative value, or would you describe its shape in some other way?\n\n\n\ncylinders\n 5.8  6.1  6.6    7  7.1  7.2  7.4  8.1  8.5  8.9  9.2  9.7  9.8 11.2 12.6 14.1 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 7.8  8.3 \n   2    2 \n\n\n[1] 8.575\n\n\n[1] 8.2\n\n\nThis data of cylinders also appears to have a slight positive skew, shown by mean (orange line) to the right of the median (red line) and mode (yellow line). However the mean is a lot closer to the median and mode in this than in the previous graph because there appear to bee less extreme observations.\n\nDo there appear to be any outlying strength values?\n\n14.1 appears to be an outlying strength value, as well as 11.2 and 12.6. Without these three values the mean and median are almost identical, shown below in cylinders.clean.\n\n\n [1] 6.1 5.8 7.8 7.1 7.2 9.2 6.6 8.3 7.0 8.3 7.8 8.1 7.4 8.5 8.9 9.8 9.7\n\n\n[1] 7.858824\n\n\n[1] 7.8\n\n\n\nWhat proportion of strength observations in this sample exceed 10 MPa?\n\n\n\n[1] \"15 %\"\n\n\nAbout 15% of the data observations exceed 10 Mpa.\n\n\nb. In what ways are the two sides of the display similar? Are there any obvious differences between the beam observations and the cylinder observations?\nBoth displays show relatively normal distributions with slight skews to the right. Similarly both the beam and cylinders have approximately 15% of the data outside the normal distribution. An obvious difference between the two is that the beam data was more skewed than the cylinder. A reason for this may be that the range of values for the beam is smaller than the range of values for cylinders, so the beam mean is more sensitive to outlying data."
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#exposure-to-microbial-products-especially-endotoxin-may-have-an-impact-on-vulnerability-to-allergic-diseases.-the-article-dust-sampling-methods-for-endotoxin-an-essential-but-underestimated-issue-indoor-air-2006-20-27-considered-various-issues-associated-with-determining-endotoxin-concentration.-the-following-data-on-concentration-eumg-in-settled-dust-for-one-sample-of-urban-homes-and-another-of-farm-homes-was-kindly-supplied-by-the-authors-of-the-cited-article.",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#exposure-to-microbial-products-especially-endotoxin-may-have-an-impact-on-vulnerability-to-allergic-diseases.-the-article-dust-sampling-methods-for-endotoxin-an-essential-but-underestimated-issue-indoor-air-2006-20-27-considered-various-issues-associated-with-determining-endotoxin-concentration.-the-following-data-on-concentration-eumg-in-settled-dust-for-one-sample-of-urban-homes-and-another-of-farm-homes-was-kindly-supplied-by-the-authors-of-the-cited-article.",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.34 Exposure to microbial products, especially endotoxin, may have an impact on vulnerability to allergic diseases. The article “Dust Sampling Methods for Endotoxin – An Essential, But Underestimated Issue” (Indoor Air, 2006: 20-27) considered various issues associated with determining endotoxin concentration. The following data on concentration (EU/mg) in settled dust for one sample of urban homes and another of farm homes was kindly supplied by the authors of the cited article.",
    "text": "1.34 Exposure to microbial products, especially endotoxin, may have an impact on vulnerability to allergic diseases. The article “Dust Sampling Methods for Endotoxin – An Essential, But Underestimated Issue” (Indoor Air, 2006: 20-27) considered various issues associated with determining endotoxin concentration. The following data on concentration (EU/mg) in settled dust for one sample of urban homes and another of farm homes was kindly supplied by the authors of the cited article.\n\nurban <- c(6.0, 5.0, 11.0, 33.0, 4.0, 5.0, 80.0, 18.0, 35.0, 17.0, 23.0)\nfarm <- c(4.0, 14.0, 11.0, 9.0, 9.0, 8.0, 4.0, 20.0, 5.0, 8.9, 21.0, 9.2, 3.0, 2.0, 0.3 )\n\n\na. Determine the sample mean for each sample. How do they compare?\n\nmean(urban)\n\n[1] 21.54545\n\nmean(farm)\n\n[1] 8.56\n\n\nThe mean endotoxin concentration is greater in urban homes than farm homes.\n\n\nb. Determine the sample median for each sample. How do they compare? Why is the median for the urban sample so different from the mean for that sample?\n\nmedian(urban)\n\n[1] 17\n\nmedian(farm)\n\n[1] 8.9\n\n\nThe median endotoxin concentration is greater in urban homes than farm homes.\nThe urban sample’s median is so different from the mean, because the mean is more sensitive to outliers in the data (such as 80 EU/mg) than the median is.\n\n\nc. Calculate the trimmed mean for each sample by deleting the smallest and largest observation. What are the corresponding trimming percentages? How do the values of these trimmed means compare to the corresponding means and medians?\nTo start I will sort the data to see what are the largest and smallest observations for both.\n\nsort(urban)\n\n [1]  4  5  5  6 11 17 18 23 33 35 80\n\nsort(farm)\n\n [1]  0.3  2.0  3.0  4.0  4.0  5.0  8.0  8.9  9.0  9.0  9.2 11.0 14.0 20.0 21.0\n\n\nFrom the sorted data I can see the lowest and highest values for urban are c(4,80), and for farm they are c(0.3,21). These values are removed from the urban.trim and farm.trim shown below.\n\nurban.trim <- c(6.0, 5.0, 11.0, 33.0, 5.0, 18.0, 35.0, 17.0, 23.0)\nfarm.trim <- c(4.0, 14.0, 11.0, 9.0, 9.0, 8.0, 4.0, 20.0, 5.0, 8.9, 9.2, 3.0, 2.0)\n\nTo find the corresponding trimming percentages I subtracted the sum of the subtracted values from the sums of the urban and farm data sets respectively, and then divide each by the length of each data set respectively.\n\n(sum(urban)-84)/length(urban)\n\n[1] 13.90909\n\n(sum(farm)-21.3)/length(farm)\n\n[1] 7.14\n\n\nThe two values removed from the urban dataset is about 14% of the data, slightly more then the 7.14% variation of the farm data set.\nNow to look at the trimmed means.\n\nmean(urban.trim)\n\n[1] 17\n\nmean(farm.trim)\n\n[1] 8.238462\n\n\nThe trimmed mean of the urban data is closer to the median of the urban data, whereas the trimmed mean for the farm data is farther away from the mean and median of the untrimmed data. After this analysis it would seem that trimming the first data set may be appropriate, whereas trimming the second may lead to misleading data. Looking at the stem-and-leaf displays are helpful in visualizing distributions and outliers.\n\nstem(urban, scale = 2)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 4556\n  1 | 178\n  2 | 3\n  3 | 35\n  4 | \n  5 | \n  6 | \n  7 | \n  8 | 0\n\nstem(farm, scale = .5)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 02344589999\n  1 | 14\n  2 | 01"
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#exercise-34-presented-the-following-data-on-endotoxin-concentration-in-settled-dust-both-for-a-sample-of-urban-homes-and-for-a-sample-of-farm-homes",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#exercise-34-presented-the-following-data-on-endotoxin-concentration-in-settled-dust-both-for-a-sample-of-urban-homes-and-for-a-sample-of-farm-homes",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.48 Exercise 34 presented the following data on endotoxin concentration in settled dust both for a sample of urban homes and for a sample of farm homes:",
    "text": "1.48 Exercise 34 presented the following data on endotoxin concentration in settled dust both for a sample of urban homes and for a sample of farm homes:\n\nurban\n\n [1]  6  5 11 33  4  5 80 18 35 17 23\n\nfarm\n\n [1]  4.0 14.0 11.0  9.0  9.0  8.0  4.0 20.0  5.0  8.9 21.0  9.2  3.0  2.0  0.3\n\n\n\na. Determine the value of the sample standard deviation for each sample, interpret these values, and then contrast variability in the two samples.\n\nsd(urban)\n\n[1] 22.29961\n\nsd(farm)\n\n[1] 6.087669\n\n\nAnother way we could find the standard deviation given the hint and the following equation given in class :\n[s^2=_{i=1}^{n}(x_i - x)^2] [=[_{i=1}{n}x_i2 - ]]\n\n# hint\nsigma_x_i.u <- 237.0\nsigma_x_i.f <- 128.4\nsigma_x_i_2.u <- 10079\nsigma_x_i_2.f <- 1617.94\nn.u <- length(urban)\nn.f <- length(farm)\nconstant.u <- 1/(n.u-1)\nconstant.f <- 1/(n.f-1)\nsv.u <- constant.u*(sigma_x_i_2.u - (sigma_x_i.u)^2/n.u)\nsv.f <- constant.f*(sigma_x_i_2.f - (sigma_x_i.f)^2/n.f)\nsqrt(sv.u)\n\n[1] 22.29961\n\nsqrt(sv.f)\n\n[1] 6.087669\n\n\n\n\nb. Compute the fourth spread for each sample and compare. Do the fourth spreads convey the same message about variability that the standard deviations do? Explain.\nThe quickest way to do this in r is with quantile().\n\nquantile(urban)\n\n  0%  25%  50%  75% 100% \n 4.0  5.5 17.0 28.0 80.0 \n\nquantile(farm)\n\n  0%  25%  50%  75% 100% \n 0.3  4.0  8.9 10.1 21.0 \n\n\nExplanation : coming soon\nAnother way to find the forth spread is by first computing the upper fourth and lower fourth . To do that I first sorted the data, and then split it in half. Notice that because n for both data sets is odd, I figure out the middle value that is included in both, before I create the new data sets.\n\n# sort \nsorted.u <- sort(urban)\nsorted.f <- sort(farm)\nsorted.u\n\n [1]  4  5  5  6 11 17 18 23 33 35 80\n\nsorted.f\n\n [1]  0.3  2.0  3.0  4.0  4.0  5.0  8.0  8.9  9.0  9.0  9.2 11.0 14.0 20.0 21.0\n\n# find included value \ninclude.u <- as.integer(n.u/2)+1\ninclude.f <- as.integer(n.f/2)+1\nsorted.u[include.u]\n\n[1] 17\n\nsorted.f[include.f]\n\n[1] 8.9\n\n# upper and lower forth \nurban.lower <- c(4,  5,  5,  6, 11, 17)\nurban.upper <- c(17, 18, 23, 33, 35, 80)\nfarm.lower <- c(0.3,  2.0,  3.0,  4.0,  4.0,  5.0,  8.0,  8.9)\nfarm.upper <- c(8.9,  9.0,  9.0,  9.2, 11.0, 14.0, 20.0, 21.0)\n\nNow that the data is sorted, the middle values of 17 and 8.9 have been found, and the data has been split into two chunks I can compute the minimum,lower forth median, median, upper forth median, and the largest value.\n\nmin(urban)\n\n[1] 4\n\nmedian(urban.lower)\n\n[1] 5.5\n\nmedian(urban)\n\n[1] 17\n\nmedian(urban.upper)\n\n[1] 28\n\nmax(urban)\n\n[1] 80\n\nmin(farm)\n\n[1] 0.3\n\nmedian(farm.lower)\n\n[1] 4\n\nmedian(farm)\n\n[1] 8.9\n\nmedian(farm.upper)\n\n[1] 10.1\n\nmax(farm)\n\n[1] 21\n\n\n\nThe authors of the cited article also provided endotoxin concentrations in dust bag dust:\n\n\nurban.bag <- c(34.0, 49.0, 13.0, 33.0, 24.0, 24.0, 35.0, 104.0, 34.0, 40.0, 38.0, 1.0)\nfarm.bag <- c(2.0, 64.0, 6.0, 17.0, 35.0, 11.0, 17.0, 13.0, 5.0, 27.0, 23.0,\n              28.0, 10.0, 13.0, 0.2)\nquantile(urban.bag)\n\n   0%   25%   50%   75%  100% \n  1.0  24.0  34.0  38.5 104.0 \n\nquantile(farm.bag)\n\n  0%  25%  50%  75% 100% \n 0.2  8.0 13.0 25.0 64.0 \n\n\nConstruct a comparative boxplot (as did the cited paper) and compare and contrast the four samples.\n\npar(mfrow = c(1,2))\nboxplot(urban)\nboxplot(urban.bag)\n\n\n\npar(mfrow = c(1,2))\nboxplot(farm)\nboxplot(farm.bag)"
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#the-article-a-thin-film-oxygen-uptake-test-for-the-evaluation-of-automotive-crankcase-lubricants-lubric.-engr.-1984-75-83-reported-the-following-data-on-oxidation-induction-time-min-for-various-commercial-oils",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#the-article-a-thin-film-oxygen-uptake-test-for-the-evaluation-of-automotive-crankcase-lubricants-lubric.-engr.-1984-75-83-reported-the-following-data-on-oxidation-induction-time-min-for-various-commercial-oils",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.51 The article “A Thin-Film Oxygen Uptake Test for the Evaluation of Automotive Crankcase Lubricants” (Lubric. Engr., 1984: 75-83) reported the following data on oxidation-induction time (min) for various commercial oils:",
    "text": "1.51 The article “A Thin-Film Oxygen Uptake Test for the Evaluation of Automotive Crankcase Lubricants” (Lubric. Engr., 1984: 75-83) reported the following data on oxidation-induction time (min) for various commercial oils:\n\noxi.induct.time.min <- c(87, 103, 130, 160, 180, 195, 132, 145, 211, 105, 145,\n                         153, 152, 138, 87, 99, 93, 119, 129)\n\n\na. Calculate the sample variance and the standard deviation.\n\noxi.var <- var(oxi.induct.time.min)\noxi.sd <- sd(oxi.induct.time.min)\noxi.var\n\n[1] 1264.766\n\noxi.sd\n\n[1] 35.56355\n\n\n\n\nb. If the observations were re-expressed in hours, what would be the resulting values of the sample variance and sample standard deviation? Answer without actually performing the re-expression.\nThe standard deviation has the same units as the data values (minutes) so in hours the standard deviation would be 35.56/60 (or a little over half an hour) whereas the variance is the standard deviation squared, so the values would be converted 1264.766/60^2.\n\noxi.var/60^2\n\n[1] 0.3513239\n\noxi.sd/60\n\n[1] 0.5927258\n\n# verification \noxi.induct.time.hour <- oxi.induct.time.min/60\nvar(oxi.induct.time.hour)\n\n[1] 0.3513239\n\nsd(oxi.induct.time.hour) \n\n[1] 0.5927258"
  },
  {
    "objectID": "01_blog/2021_06_28_Stat-451-Hw1/index.html#observations-on-burst-strength-lbin2-were-obtained-both-for-test-nozzle-welds-proper-procedures-are-the-key-to-welding-radioactive-waste-canisters-welding-j.-auud.-1997-61-67",
    "href": "01_blog/2021_06_28_Stat-451-Hw1/index.html#observations-on-burst-strength-lbin2-were-obtained-both-for-test-nozzle-welds-proper-procedures-are-the-key-to-welding-radioactive-waste-canisters-welding-j.-auud.-1997-61-67",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.60 Observations on burst strength (lb/in2) were obtained both for test nozzle welds (“Proper Procedures Are the Key to Welding Radioactive Waste Canisters,” Welding J., Auud. 1997: 61-67)",
    "text": "1.60 Observations on burst strength (lb/in2) were obtained both for test nozzle welds (“Proper Procedures Are the Key to Welding Radioactive Waste Canisters,” Welding J., Auud. 1997: 61-67)\n\nTest <-c(7200, 6100, 7300, 7300, 8000, 7400,\n         7300, 7300, 8000, 6700, 8300)         \nCannister <- c(5250, 5625, 5900, 5900, 5700, 6050,\n               5800, 6000, 5875, 6100, 5850, 6600)\n\n\na. Construct a comparative boxplot and comment on interesting features (the cited article did not include such a picture, but the authors commented that they had looked at one).\n\npar(mfrow = c(1,2))\nboxplot(Test, ylim = c(5000, 8500), main = \"Test\")\nboxplot(Cannister, ylim = c(5000, 8500), main = \"Cannister\")\n\n\n\n\n\nmean(Test)-mean(Cannister)\n\n[1] 1467.045\n\nmean(Test)\n\n[1] 7354.545\n\nmean(Cannister)\n\n[1] 5887.5\n\nsd(Test)\n\n[1] 613.7811\n\nsd(Cannister)\n\n[1] 317.9301"
  },
  {
    "objectID": "00_data/about_data/movies.html",
    "href": "00_data/about_data/movies.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Movie Titles - Generes - Rating - Review\n\ntitles <- c(\"The Terminator 1\",\n            \"Terminator 2: Judgement Day\",\n            \"Terminator 3: Rise of the Machines\",\n          \"Peal\",\n          \"X\",\n          \"The Father\",\n          \"Our Friend\",\n          \"The Lighthouse\",\n          \"Men\")\n\n\ngenres <- c(\"Action\",\n           \"Horror / Thriller\",\n           \"Horror / Slasher\",\n           \"Drama\")\n\n\nstar_rating <- function(rating, max_rating = 5){\n  \n  star_icon <- function(status) {\n    \n    if (status == \"Half\") {\n      tagAppendAttributes(shiny::icon(\"star-half-alt\"),\n      style = paste(\"color: orange\"),\n      \"aria-hidden\" = \"true\")\n    } else {\n          tagAppendAttributes(shiny::icon(\"star\"),\n      style = paste(\"color:\", if (status == \"Empty\") \"#edf0f2\" else \"orange\"),\n      \"aria-hidden\" = \"true\"\n    )\n    }\n    \n  }\n  \n  rounded_rating <- floor(rating)\n  last_rating <- rating - rounded_rating\n  last_rating <- case_when(last_rating < .25 ~ \"Empty\",\n                           last_rating > .75 ~ \"Full\",\n                           TRUE ~ \"Half\"\n                           )\n  \n  stars <- lapply(seq_len(max_rating), function(i) {\n    if (i <= rounded_rating) { star_icon(status = \"Full\")} \n    else {\n      if (i == (rounded_rating+1)) star_icon(status = last_rating)\n      else {\n      star_icon(status = \"Empty\")\n    }\n    }\n    \n    })\n  \n  label <- sprintf(\"%s out of %s\", rating, max_rating)\n  div(title = label, \"aria-label\" = label, role = \"img\", stars)\n\n}\n\n# Example\nstar_rating(4.5)\n\n\n\n\n\n\n\n\n\n\n\nwatched <- c(\"12/22\",\n             \"12/22\",\n             \"12/22\",\n             \"12/22\")"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html",
    "title": "R and SQLite",
    "section": "",
    "text": "In this post I use fake hospital data to answer questions using both R and SQL (Structured Query Language) via a R package called RSQLite."
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#packages",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#packages",
    "title": "R and SQLite",
    "section": "0.1 Packages",
    "text": "0.1 Packages\nFor all my data quering and manipulation in R I will be using the tidyverse.\n\nlibrary(tidyverse)\n\nTo run create a SQL database, and run SQL queries in R chunks I will be using a package called RSQLite.\n\nlibrary(RSQLite)"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#data",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#data",
    "title": "R and SQLite",
    "section": "0.2 Data",
    "text": "0.2 Data\nThis post will use two data sets that I copied from Learn SQL:\n\npatients: Which includes patient_id, first_name, last_name, gender, birth_date, city, province_id, allergies, height, and weight. Note I only copied the first 1000 entries.\n\n\npatients <- utils::read.csv('../../00_data/patients.csv')\n\n\nprovince_names: Which includes province_id, and province_name.\n\n\nprovince_names <- utils::read.csv(\"../../00_data/province_names.csv\")"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#database",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#database",
    "title": "R and SQLite",
    "section": "0.3 Database",
    "text": "0.3 Database\nTo start I am using the dbConnect(), and SQLite() functions to create a hospital\nSince SQL lets the user access and manipulate data from a database, it is important to create this data base.\nCreate a Hospital database to store the patients data.\n\nhosp <- RSQLite::dbConnect(RSQLite::SQLite(),\n                           \"../../00_data/Hospital.db\")\n\n\nRSQLite::dbWriteTable(hosp,\n                      \"patients\",\n                      patients)\nRSQLite::dbWriteTable(hosp,\n                      \"province_names\",\n                      province_names)\n\n\nRSQLite::dbListTables(hosp)\n\n[1] \"patients\"       \"province_names\""
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\nutils::head(patients, 10)\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp, \n                    \"SELECT * \n                     FROM patients \n                     LIMIT 10\"\n                    )\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-1",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-1",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\nbase::data.frame(\"total_admissions\" = base::nrow(patients))\n\n  total_admissions\n1             1000"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-1",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-1",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT COUNT(*) AS total_admissions \n                     FROM patients\"\n                    )\n\n  total_admissions\n1             1000"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-2",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-2",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\nutils::head(\n  base::data.frame(\n    full_name = base::paste0(patients$first_name, \n                             \" \", \n                             patients$last_name)), \n  10)\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-2",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-2",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp,\n  \"SELECT first_name || ' ' || last_name AS full_name \n   FROM patients \n   LIMIT 10\"\n   )\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-3",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-3",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\npatients %>% \n  filter(province_id == \"NS\") %>%\n  summarise(unique_cities = unique(city)) \n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-3",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-3",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\ndbGetQuery(hosp,\n           \"SELECT DISTINCT(city) AS unique_cities \n            FROM patients \n            WHERE province_id IS 'NS'\"\n           )\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-4",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-4",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\nbase::data.frame(\n  male_count = base::length(base::which(patients$gender == 'M')),\n  female_count = base::length(base::which(patients$gender == 'F'))\n  )\n\n  male_count female_count\n1        543          457"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-4",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-4",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\ndbGetQuery(hosp,\n           \"SELECT \n           (SELECT COUNT(*) FROM patients WHERE gender = 'M') AS male_count, \n           (SELECT COUNT(*) FROM patients WHERE gender = 'F') AS female_count\")\n\n  male_count female_count\n1        543          457"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-5",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-5",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\npatients %>% \n  filter(allergies != \"NULL\") %>%\n  group_by(allergies) %>%\n  summarise(total_diagnosis = n()) %>%\n  arrange(desc(total_diagnosis)) %>%\n  head(10)\n\n# A tibble: 10 × 2\n   allergies   total_diagnosis\n   <chr>                 <int>\n 1 Penicillin              230\n 2 Codeine                  58\n 3 Sulfa                    35\n 4 ASA                      16\n 5 Sulfa Drugs              13\n 6 Tylenol                  11\n 7 Wheat                    11\n 8 Peanuts                  10\n 9 Bee Stings                9\n10 Iodine                    9"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-5",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-5",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\ndbGetQuery(hosp,\n           \"SELECT allergies,\n            COUNT(*) AS total_diagnosis\n            FROM patients\n            WHERE allergies IS NOT 'NULL'\n            GROUP BY allergies\n            ORDER BY total_diagnosis DESC\n            LIMIT 10\"\n           )\n\n     allergies total_diagnosis\n1   Penicillin             230\n2      Codeine              58\n3        Sulfa              35\n4          ASA              16\n5  Sulfa Drugs              13\n6        Wheat              11\n7      Tylenol              11\n8      Peanuts              10\n9       Iodine               9\n10  Bee Stings               9"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-6",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-6",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\nbase::merge(province_names, patients, by = \"province_id\") %>%\n  group_by(province_name) %>%\n  summarise(patient_count = n()) %>%\n  arrange(desc(patient_count))\n\n# A tibble: 8 × 2\n  province_name             patient_count\n  <chr>                             <int>\n1 Ontario                             954\n2 Alberta                              14\n3 British Columbia                     11\n4 Nova Scotia                           9\n5 Manitoba                              7\n6 Newfoundland and Labrador             2\n7 Quebec                                2\n8 Saskatchewan                          1"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-6",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-6",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT province_name,\n                     COUNT(*) as patient_count\n                     FROM patients pa\n                     join province_names pr on pr.province_id = pa.province_id\n                     group by pr.province_id\n                     order by patient_count desc\")\n\n              province_name patient_count\n1                   Ontario           954\n2                   Alberta            14\n3          British Columbia            11\n4               Nova Scotia             9\n5                  Manitoba             7\n6                    Quebec             2\n7 Newfoundland and Labrador             2\n8              Saskatchewan             1"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-7",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-7",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\nbase::merge(province_names, patients, by = \"province_id\") %>%\n  dplyr::group_by(province_name) %>%\n  dplyr::count(gender == \"M\", gender == \"F\") %>%\n  dplyr::slice(base::which.max(n)) %>%\n  dplyr::summarise(province_name = province_name)\n\n# A tibble: 8 × 1\n  province_name            \n  <chr>                    \n1 Alberta                  \n2 British Columbia         \n3 Manitoba                 \n4 Newfoundland and Labrador\n5 Nova Scotia              \n6 Ontario                  \n7 Quebec                   \n8 Saskatchewan"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-7",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-7",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT pr.province_name\n                     FROM patients AS pa\n                     JOIN province_names AS pr ON pa.province_id = pr.province_id\n                     GROUP BY pr.province_id\n                     HAVING\n                     COUNT( CASE WHEN gender = 'M' THEN 1 END) >\n                     COUNT( CASE WHEN gender = 'F' THEN 1 END)\")\n\n              province_name\n1                   Alberta\n2          British Columbia\n3                  Manitoba\n4 Newfoundland and Labrador\n5               Nova Scotia\n6                   Ontario\n7                    Quebec\n8              Saskatchewan"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-8",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-8",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\n\n\n\nfor(i in 1:length(patients$patient_id))\n  {\n  if(patients$patient_id[[i]]%%2==1){\n     patients$has_insurance[[i]] <- \"Yes\"\n  }else{\n    patients$has_insurance[[i]] <- \"No\"\n  }\n}\n\n#. patients"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-8",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-8",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT \n                        CASE WHEN patient_id % 2 = 0 Then 'Yes'\n                        ELSE 'No' \n                        END as has_insurance,\n                     SUM(\n                        CASE WHEN patient_id % 2 = 0 Then 10\n                        ELSE 50 \n                        END\n                         ) as cost_after_insurance\n                     FROM patients \n                     GROUP BY has_insurance;\")\n\n  has_insurance cost_after_insurance\n1            No                25000\n2           Yes                 5000"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-9",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#r-9",
    "title": "R and SQLite",
    "section": "R",
    "text": "R\n\npatients %>%\n  filter(base::grepl(\"^.{2}[b]\", first_name),\n         gender == \"F\",\n         weight > 50 && weight < 80,\n         patient_id %%2==1,\n         city == \"Burlington\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight has_insurance\n1 Penicillin    160     51           Yes"
  },
  {
    "objectID": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-9",
    "href": "01_blog/2023_01_23_Data-Tool-Box-P1/index.html#sql-9",
    "title": "R and SQLite",
    "section": "SQL",
    "text": "SQL\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT *\n                     FROM patients\n                     WHERE\n                      first_name LIKE '__b%'\n                      AND gender = 'F'\n                      AND weight BETWEEN 50 AND 70\n                      AND patient_id % 2 = 1\n                      AND city = 'Burlington';\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51"
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html",
    "href": "01_blog/2021_07_19_Quiz-1/index.html",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "",
    "text": "This is a “presentation” style proof for Math-338: Modern College Geometry, and looks at congruence of two verticle angles."
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#suppose-two-angles-are-verticle.",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#suppose-two-angles-are-verticle.",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "Suppose two angles are verticle.",
    "text": "Suppose two angles are verticle.\nProve that they are congruent."
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#vertical-angles",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#vertical-angles",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "1.1 Vertical Angles",
    "text": "1.1 Vertical Angles\nIn class we defined vertical angles as being “across from each other”.\n\n\n\n\n\nFig. 1:  Vertical angles are shown in pink.\n\n\n\n\nWe can see a simple example of this in Fig. 1 where a pair of pink angles \\(\\angle AOB\\) and \\(\\angle COD\\) are vertical to each other.\nNote that \\(\\angle AOC\\) and \\(\\angle BOD\\) are vertical angles as well."
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#congruence",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#congruence",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "1.2 Congruence",
    "text": "1.2 Congruence\n\\(x\\cong y\\) if there is an \\(\\underline{\\text{isometry}}\\) that superimposes x onto y.\n\nIsometry is a map that preserves distance and angles\n\ntranslation (move without turning)\nrotation (moving about a fixed point)\nreflection (mirror)\ncombination\n\n\n\n\n\n\n\nFig. 2:  Two congruent triangles\n\n\n\n\nIn Fig. 2 we see the two triangles are congruent, and would only need a translation isometry or two to map \\(\\triangle ABC\\) onto \\(\\triangle A_1B_1C_1\\)."
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#supplementary-angles",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#supplementary-angles",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "1.3 Supplementary Angles",
    "text": "1.3 Supplementary Angles\nWe defined supplementary angles as angles whose measurement adds up to \\(180^\\circ\\).\n\n\n\n\n\nFig. 3: Supplementary angles are shown in pink and orange.\n\n\n\n\nIn Fig.3 we can clearly see that \\(m\\angle AOC\\) shown in pink and \\(m\\angle AOB\\) in orange adds to a straight line or \\(180^\\circ\\). We can also see three other pairs of supplementary angles:\n\n\\(m\\angle AOB+m\\angle BOD=180^\\circ\\)\n\\(m\\angle BOD+m\\angle COD=180^\\circ\\)\n\\(m\\angle COD+m\\angle AOC=180^\\circ\\)"
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#axioms-of-angle-measure",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#axioms-of-angle-measure",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "2.1 Axioms of Angle Measure",
    "text": "2.1 Axioms of Angle Measure\n\nRight angle measures \\(90^\\circ\\)\n\\(m\\angle ABC=m\\angle CBA\\)\nIf D is the interior of \\(\\angle ABC\\), then \\(m\\angle ABC=m\\angle ABD+m\\angle BDC\\)\nThere exists a unique ray that is the angle bisector of \\(\\angle ABC\\)."
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#congruence-and-angle-measure-theorem",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#congruence-and-angle-measure-theorem",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "2.2 Congruence and Angle Measure theorem",
    "text": "2.2 Congruence and Angle Measure theorem\n\\(\\angle ABC\\cong \\angle DEF \\Leftrightarrow m\\angle ABC=m\\angle DEF\\)\n\nIf two angles are congruent then the measure of those two angles is the same.\nIf the measure of two angles is the same, then those two angles are congruent.\n\n(Note that the measure for angles that will be used on the proof will be in degrees. )"
  },
  {
    "objectID": "01_blog/2021_07_19_Quiz-1/index.html#supplementary-interior-angle-theorem",
    "href": "01_blog/2021_07_19_Quiz-1/index.html#supplementary-interior-angle-theorem",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "2.3 Supplementary Interior Angle Theorem",
    "text": "2.3 Supplementary Interior Angle Theorem\nIf two lines are parallel then the supplementary interior angles add to \\(180^\\circ\\).\n\n\n\n\n\nFig. 4:  Supplementary interior angles are shown in red.\n\n\n\n\nIn Fig. 4 the supplementary interior angles \\(\\angle AEF\\) and \\(\\angle CFE\\) are shown in red and add up to \\(180^\\circ\\)."
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html",
    "href": "01_blog/2021_07_26_Quiz-2/index.html",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "",
    "text": "This is a “presentation” style proof for Math-338: Modern College Geometry, and looks at the symmetries of a non-square rectangle."
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#suppose-r-is-a-non-square-rhombus.",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#suppose-r-is-a-non-square-rhombus.",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "Suppose R is a non-square rhombus.",
    "text": "Suppose R is a non-square rhombus.\n\nList the symmetries of R. Write as a composition of translations, reflections, and rotations.\nProve that the set of symmetries of R is a group under composition."
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#parallelogram",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#parallelogram",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.1 Parallelogram",
    "text": "1.1 Parallelogram\nBoth pairs of opposite sides are parallel.\n\n\n\n\n\nFig. 1:  Parallelogram\n\n\n\n\n\n\\(AB\\parallel DC\\)\n\\(CB\\parallel DA\\)"
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#rhombus",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#rhombus",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.2 Rhombus",
    "text": "1.2 Rhombus\nParallelogram whose sides are all the same length.\nNotation: R\n\n\n\n\n\nFig. 2:  Rhombus\n\n\n\n\n\n\\(\\overline{AB}=\\overline{BC}=\\overline{CD}=\\overline{DA}\\)"
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#isometry",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#isometry",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.3 Isometry",
    "text": "1.3 Isometry\nPreserves distance, length, and angle measure through “rigid motion” of rotations, reflections, translations, and compositions.\n\n1.3.1 Translations\nMove all the points across a vector \\(\\vec{v}\\).\nNotation: \\(\\tau_{\\text{start point,end point}}\\)\n\n\n\n\n\nFig. 3:  Translation of line AB along vector v.\n\n\n\n\n\n\n1.3.2 Rotations\nPick a center O (origin), \\(\\theta\\) (angle), takes P to P’ on a circle on a circle with center O and radius \\(\\overline{OP}\\) with \\(\\angle POP'=\\theta\\).\nNotation: \\(R_{O,\\theta}\\)\n\n\n\n\n\nFig. 4:  Rotation of line OP.\n\n\n\n\n\n\n1.3.3 Reflections\nMirror of a shape across a line.\nPick line b, shown in red in Fig. 5.\n\nPoints on B don’t move.\nPoints not on b, such as P, go to P’ where b is perpendicular to bisector of \\(\\overline{PP'}\\).\nMidpoint m of PP’ on b make right angle \\(\\overline{PP'}\\)\n\nNotation: \\(r_{b}\\)\n\n\n\n\n\nFig. 5:  Triangle QPR reflected across line b.\n\n\n\n\n\n\n1.3.4 Compositions\nCombinations of rotations, reflections, and translations."
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#symmetry",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#symmetry",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.4 Symmetry",
    "text": "1.4 Symmetry\nAn isometry that sends a geometric figure to itself."
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#group",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#group",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.5 Group",
    "text": "1.5 Group\n\n1.5.0 Closure\n\nOrder doesn’t matter.\n\\(ab=ba\\)\n\n\n\n1.5.1 Associativity\n\nParentheses don’t matter.\n\\((ab)c=a(bc)\\)\n\n\n\n1.5.2 Identity\n\nAnything combined with identity equals itself.\n\\(ea=ae=a\\)\n\n\n\n1.5.3 Inverses\n\nUndoes isometry.\n\\(f^{-1}(f(a))=a\\)"
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#symmetries-of-r-1",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#symmetries-of-r-1",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "2.1 4 Symmetries of R",
    "text": "2.1 4 Symmetries of R\n(Using Boyce’s Notation)\n\n2.1.1 \\(e\\)\n\n\n\n\n\nIdentity\n\n\n\n\n\n\n2.1.2 \\(R_{O,180^\\circ}\\)\n\n\n\n\n\nRotation about center (O) by \\(180^o\\).\n\n\n\n\n\n\n2.1.3 \\(r_{m_1}\\)\n\n\n\n\n\nReflection over \\(m_1\\)\n\n\n\n\n\n\n2.1.4 \\(r_{m_2}\\)\n\n\n\n\n\nReflection over \\(m_2\\)\n\n\n\n\n\n\n2.1.5 Verify\nTo verify this we can imagine that one point only has two places to choose from, then the a different point only has two places to choose from, and then the remaining two points only have one place to choose i.e. \\(2\\cdot 2\\cdot 1\\cdot 1=4\\). Therefore there are only four possible symmetries of R."
  },
  {
    "objectID": "01_blog/2021_07_26_Quiz-2/index.html#claim-mathscrs-is-a-group-under-composition.",
    "href": "01_blog/2021_07_26_Quiz-2/index.html#claim-mathscrs-is-a-group-under-composition.",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "3.1 Claim: \\(\\mathscr{S}\\) is a group under composition.",
    "text": "3.1 Claim: \\(\\mathscr{S}\\) is a group under composition.\n\n3.1.0 Closure\nWe want to show that composing two symmetries equals a symmetry.\nLet \\(\\square 1234\\) be a non-square rhombus, and suppose F and G are in \\(\\mathscr{S}\\).\n\\[F\\circ G(\\square 1234)=F(G(\\square 1234))=F(\\square 1234)=\\square 1234\\]\nFor example:\nLet \\(F=r_{m_1}\\) and \\(G=r_{m_2}\\). Then \\(F\\circ G(\\square 1234)=R_{O,180^\\circ}\\).\n\n\n3.1.1 Associative\n\\(F\\circ(G\\circ H)=(F\\circ G)\\circ H\\)\n\\[\\begin{equation}\\label{D14,1}\n\\begin{split}\nF\\circ(G\\circ H) &= F\\circ (G\\circ H)(\\square 1234)\\\\\n&= F(G\\circ H (\\square 1234))\\\\\n&= F(G(H(\\square 1234)))\\\\\n&= (F\\circ G)\\circ H(\\square 1234)\n\\end{split}\n\\end{equation}\\]\n\n\n3.1.2 Identity\ne is one of the symmetries.\nExample: \\(R_{O,180^\\circ}\\circ e=R_{O,180^\\circ}\\)\n\n\n3.1.3 Inverses\nEvery symmetry of a rhombus undoes itself.\n\\(e\\circ e=e\\)\n\\(R_{O,180^\\circ}\\circ R_{O,180^\\circ}=e\\)\n\\(r_{m_1}\\circ r_{m_1}=e\\)\n\\(r_{m_2}\\circ r_{m_2}=e\\)"
  },
  {
    "objectID": "01_blog/2023_01_23_R-and-SQLite/index.html",
    "href": "01_blog/2023_01_23_R-and-SQLite/index.html",
    "title": "R and SQLite",
    "section": "",
    "text": "Naruto with SQL logo, and Sauske with the R Studio logo, standing back to back.\n\n\n\n0. Introduction\nThere are a handful of programming languages that data scientists use when querying, analyzing, and manipulating data. What I have found is that while R and Python have a lot more power in what they are capable of producing, SQL is used by a wider variety of roles to access and query data. So to get more practice using both SQL and R I pulled 10 questions and some data off a website called Learn SQL, and will be answering the following questions in both languages.\n\n0.0 Set-Up0.1 Packages0.2 Data0.3 Database\n\n\nContents:\n\n0.1 Packages\n0.2 Data\n0.3 Data base\n\n\n\nFor all my data queries and manipulation in R I will be using base R, dplyr, and magrittr.\n\nlibrary(dplyr)\nlibrary(magrittr)\n\nTo create a SQL database, and run SQL queries in R chunks I will be using a package called RSQLite.\n\nlibrary(RSQLite)\n\n\n\nThis post will use three data sets that I copied from Learn SQL:\n\npatients: Which includes patient_id, first_name, last_name, gender, birth_date, city, province_id, allergies, height, and weight. Note I only copied the first 1000 entries.\n\n\npatients <- utils::read.csv('data/patients.csv')\n\n\nprovince_names: Which includes province_id, and province_name.\n\n\nprovince_names <- utils::read.csv(\"data/province_names.csv\")\n\n\nadmissions: which includes patient_id, admission_date, discharge_date, diagnosis, attending_doctor_id\n\n\nadmissions <- utils::read.csv(\"data/admissions.csv\")\n\n\n\nTo create a database use:\n\ndbConnect() to connect to a SQL data base called Hospital.db in the 00_data folder.\nSQLite() to connect to a SQLite database file.\n\n\nhosp <- RSQLite::dbConnect(RSQLite::SQLite(),\n                           \"data/Hospital.db\")\n\nTo define data within the database use:\n\ndbWriteTable() to create a data set within the hospital database first call the data base (hosp), define a name, and then define the data.\n\n\nRSQLite::dbWriteTable(hosp,\n                      \"patients\",\n                      patients)\nRSQLite::dbWriteTable(hosp,\n                      \"province_names\",\n                      province_names)\nRSQLite::dbWriteTable(hosp,\n                      \"admissions\",\n                      admissions)\n\nVerify the three data sets are in the database using:\n\ndbListTables() to list the tables within the hosp database.\n\n\nRSQLite::dbListTables(hosp)\n\n[1] \"admissions\"     \"patients\"       \"province_names\"\n\n\n\n\n\n\n\n1. Show the first ten rows of patients data.\n\n1.01.1 R1.2 SQL\n\n\nContents\n\n1.1 Solution in R\n1.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to view the first 10 rows of the patients data.\n\n\nutils::head(patients, 10)\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\nIn R use:\n\ndbGetQuery() to run SQL commands from a given data base.\n\nIn SQL use:\n\nSELECT to select.\n* to include all columns.\nFROM to define the patients data for select to include all columns from.\nLIMIT to only show the top ten rows.\n\n\nRSQLite::dbGetQuery(hosp, \n                    \"SELECT * \n                     FROM patients \n                     LIMIT 10\"\n                    )\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\n\n\n\n2. Show total patients admitted.\n\n2.02.1 R2.2 SQL\n\n\nContents\n\n2.1 Solution in R\n2.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to define a column for total_admissions.\nnrow() to count the rows in the patients data which will equal the total_admissions.\n\n\nbase::data.frame(\"total_admissions\" = base::nrow(patients))\n\n  total_admissions\n1             1000\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count the total number of rows.\nAS to define that count as a new variable, total_admissions.\nFROM to define the patients data for select to count the total numbers of rows for, and define as total_admissions.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT COUNT(*) AS total_admissions \n                     FROM patients\"\n                    )\n\n  total_admissions\n1             1000\n\n\n\n\n\n\n\n3. Show first and last name as full_name.\n\n3.03.1 R3.2 SQL\n\n\nContents\n\n3.1 Solution in R\n3.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to show the first 10 rows of data.\ndata.frame() to define a data frame that includes full_name.\npaste0() to paste together the first_name, a space, and the last_name. This will equal the full_name.\n\n\nutils::head(\n  base::data.frame(\n    full_name = base::paste0(patients$first_name, \n                             \" \", \n                             patients$last_name)), \n  10)\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\nIn SQL use:\n\nSELECT to select.\n|| to concatenate first_name, space, and last name.\nAS to define the concatination as full_name.\nFROM to define the patients data for select to concatenate data from.\nLIMIT to show the first 10 rows of data.\n\n\nRSQLite::dbGetQuery(hosp,\n  \"SELECT first_name || ' ' || last_name AS full_name \n   FROM patients \n   LIMIT 10\"\n   )\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\n\n\n\n4. Show unique cities that are in province_id ‘NS’?\n\n4.04.1 R4.2 SQL\n\n\nContents\n\n4.1 Solution in R\n4.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%>% to pipe data.\nfilter() to filter for all province_id that is equal to “NS”.\nsummerise() to define unique_cites.\nunique() to remove duplicate elements of the city column which will be defined as unique_cites.\n\n\npatients %>% \n  filter(province_id == \"NS\") %>%\n  summarise(unique_cities = unique(city)) \n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\nIn SQL:\n\nSELECT to select.\nDISTINCT() to define city as the column to remove duplicates from.\nAS to define those cities as unique_cites.\nFROM to define the patients data for select to get unique_cites from.\nWHERE specifies a condition.\nIS is the condition that ‘NS’ is equal to province_id.\n\n\ndbGetQuery(hosp,\n           \"SELECT DISTINCT(city) AS unique_cities \n            FROM patients \n            WHERE province_id IS 'NS'\"\n           )\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\n\n\n\n5. Show the total number of male patients and the total number of female patients.\nDisplay the two results in the same row.\n\n5.05.1 R5.2 SQL\n\n\nContents\n\n5.1 Solution in R\n5.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to create a data frame with two columns: male_count, and female_count.\nlength() to count the length of the input.\nwhich() to indicate which gender equals “M” or “F”, and counts that length accordingly.\n\n\nbase::data.frame(\n  male_count = base::length(base::which(patients$gender == 'M')),\n  female_count = base::length(base::which(patients$gender == 'F'))\n  )\n\n  male_count female_count\n1        543          457\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count all input values.\nFROM to define the patients data for select to count on.\nWHERE is a condition defined as gender = “M” or “F”.\nAS is defining the count as male_count, or female_count respectively.\n\n\ndbGetQuery(hosp,\n           \"SELECT \n           (SELECT COUNT(*) FROM patients WHERE gender = 'M') AS male_count, \n           (SELECT COUNT(*) FROM patients WHERE gender = 'F') AS female_count\")\n\n  male_count female_count\n1        543          457\n\n\n\n\n\n\n\n6. Show all allergies ordered by popularity. Remove NULL values from the query.\n\n6.06.1 R6.2 SQL\n\n\nContents\n\n6.1 Solution in R\n6.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%>% pipe the data.\nfilter() to subset data to all allergies that aren’t “NULL”.\ngroup_by() to convert the table into one that is grouped by allergies.\nsummarise() to define total_diagnosis.\nn() to count the size of each group.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by total_diagnosis.\n\n\npatients %>% \n  dplyr::filter(allergies != \"NULL\") %>%\n  dplyr::group_by(allergies) %>%\n  dplyr::summarise(total_diagnosis = dplyr::n()) %>%\n  dplyr::arrange(dplyr::desc(total_diagnosis)) %>%\n  utils::head(10)\n\n# A tibble: 10 × 2\n   allergies   total_diagnosis\n   <chr>                 <int>\n 1 Penicillin              230\n 2 Codeine                  58\n 3 Sulfa                    35\n 4 ASA                      16\n 5 Sulfa Drugs              13\n 6 Tylenol                  11\n 7 Wheat                    11\n 8 Peanuts                  10\n 9 Bee Stings                9\n10 Iodine                    9\n\n\n\n\nIn SQL:\n\nSELECT the allergies column.\nCOUNT(*) count all input.\nAS to define the count as total_diagnosis.\nFROM to define the patients data to select the allergies column from.\nWHERE to define a condition.\nIS NOT is the condition that says allergies cannot be ‘NULL’.\nGROUP BY groups the data by allergies.\nOrder BY to define how the data order is output.\nDESC is the given condition that the data is ordered in descending order by total_diagnosis.\nLIMIT to limit output to the first 10 rows.\n\n\ndbGetQuery(hosp,\n           \"SELECT allergies,\n            COUNT(*) AS total_diagnosis\n            FROM patients\n            WHERE allergies IS NOT 'NULL'\n            GROUP BY allergies\n            ORDER BY total_diagnosis DESC\n            LIMIT 10\"\n           )\n\n     allergies total_diagnosis\n1   Penicillin             230\n2      Codeine              58\n3        Sulfa              35\n4          ASA              16\n5  Sulfa Drugs              13\n6        Wheat              11\n7      Tylenol              11\n8      Peanuts              10\n9       Iodine               9\n10  Bee Stings               9\n\n\n\n\n\n\n\n7. Display the total number of patients for each province. Order by descending.\n\n7.07.1 R7.1 SQL\n\n\nContents\n\n7.1 Solution in R\n7.2 Solution in SQL\n\n\n\nIn R use:\n\nmerge() to join the data sets province_names, and patients by “province_id”.\n%>% to pipe the data.\ngroup_by() to group by province name.\nsummarise() to define the patient count.\nn() to count the number of patients in each province. This will equal patient_count.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by patient_count.\n\n\nbase::merge(province_names, patients, by = \"province_id\") %>%\n  dplyr::group_by(province_name) %>%\n  dplyr::summarise(patient_count = dplyr::n()) %>%\n  dplyr::arrange(dplyr::desc(patient_count))\n\n# A tibble: 8 × 2\n  province_name             patient_count\n  <chr>                             <int>\n1 Ontario                             954\n2 Alberta                              14\n3 British Columbia                     11\n4 Nova Scotia                           9\n5 Manitoba                              7\n6 Newfoundland and Labrador             2\n7 Quebec                                2\n8 Saskatchewan                          1\n\n\n\n\nIn SQL use:\n\nSELECT select the province_name column.\nCOUNT(*) to count all input.\nAS to define count as patient_count.\nFROM to define the patients data AS pa to select province_name to count the number of patients from.\nJOIN to join the province_names data AS pr.\nON is the clause to join data based on pr.province_id = pa.province_id.\nGROUP BY to group the data by pr.province_id.\nORDER BY to define the order of the data output.\nDESC defines that the output data be arranged in order of descending patient_count.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT province_name,\n                     COUNT(*) AS patient_count\n                     FROM patients pa\n                     JOIN province_names pr ON pr.province_id = pa.province_id\n                     GROUP BY pr.province_id\n                     ORDER BY patient_count DESC\")\n\n              province_name patient_count\n1                   Ontario           954\n2                   Alberta            14\n3          British Columbia            11\n4               Nova Scotia             9\n5                  Manitoba             7\n6                    Quebec             2\n7 Newfoundland and Labrador             2\n8              Saskatchewan             1\n\n\n\n\n\n\n\n8. Show the provinces that have more patients identified as ‘M’ than ‘F’. Must only show full province_name.\n\n8.08.1 R8.2 SQL\n\n\nContents\n\n8.1 Solution in R\n8.2 Solution in SQL\n\n\n\nIn R use,\n\nmerge() to join the data sets province_names, and patients by “province_id”.\ngroup_by() to group by province name.\ncount() to count the number of “M” and “F” patients for each province.\nslice() to remove certain rows based on a given criteria.\nwhich.max() to determine which province has a greater number of male patients.\nsummarise() to define only province_name in the output.\n\n\nbase::merge(province_names, patients, by = \"province_id\") %>%\n  dplyr::group_by(province_name) %>%\n  dplyr::count(gender == \"M\", gender == \"F\") %>%\n  dplyr::slice(base::which.max(n)) %>%\n  dplyr::summarise(province_name = province_name)\n\n# A tibble: 8 × 1\n  province_name            \n  <chr>                    \n1 Alberta                  \n2 British Columbia         \n3 Manitoba                 \n4 Newfoundland and Labrador\n5 Nova Scotia              \n6 Ontario                  \n7 Quebec                   \n8 Saskatchewan             \n\n\n\n\nIn SQL use,\n\nSELECT to select pr.province_name column.\nFROM to define the patients data to select pr.province_name from.\nAS to define patients data as pa.\nJOIN to join pa.province data with pr.province_name data.\nAS to define province_name data as pr.\nON to join pa.patients and pr.province_names data by province_id.\nGROUP BY to group data by province_id.\nHAVING to define a clause to filter the data where the number of “M” patients is greater than “F” patients.\nCOUNT() to count the given input.\nCASE to go through the condition of gender = “M” or when gender = “F”\nWHEN to preform the count when the condition is true.\nTHEN 1 END to add a 1 to the count when the case is met.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT pr.province_name\n                     FROM patients AS pa\n                     JOIN province_names AS pr ON pa.province_id = pr.province_id\n                     GROUP BY pr.province_id\n                     HAVING\n                     COUNT( CASE WHEN gender = 'M' THEN 1 END) >\n                     COUNT( CASE WHEN gender = 'F' THEN 1 END)\")\n\n              province_name\n1                   Alberta\n2          British Columbia\n3                  Manitoba\n4 Newfoundland and Labrador\n5               Nova Scotia\n6                   Ontario\n7                    Quebec\n8              Saskatchewan\n\n\n\n\n\n\n\n9. Each admission costs $50 for patients without insurance, and $10 for patients with insurance. All patients with an even patient_id have insurance.\nGive each patient a ‘Yes’ if they have insurance, and a ‘No’ if they don’t have insurance. Add up the admission_total cost for each has_insurance group.\n\n9.09.1 R9.2 SQL\n\n\nContents\n\n9.1 Solution in R\n9.2 Solution in SQL\n\n\n\nIn R use,\n\n%>% to pipe the data.\nmutate() to mutate the data to include has_insurance, and cost_after_insurance information.\ncase_when() to define a case where if the patient id is odd then they don’t have insurance, and if they are even they do have insurance.\ngroup_by() to group the data by has_insurance.\nsummarize() to define cost_after_insurance.\nsum() to add up the cost for all those with and without insurance.\n\n\npatients %>%\n  dplyr::mutate(\n    has_insurance = dplyr::case_when(\n    patient_id %%2==1 ~ \"Yes\",\n    patient_id %%2!=1 ~ \"No\"\n  ),cost_after_insurance = dplyr::case_when(\n    has_insurance == \"Yes\" ~ 10,\n    has_insurance == \"No\" ~ 50\n  )) %>%\n  group_by(has_insurance)  %>%\n  summarise(cost_after_insurance = base::sum(cost_after_insurance))\n\n# A tibble: 2 × 2\n  has_insurance cost_after_insurance\n  <chr>                        <dbl>\n1 No                           25000\n2 Yes                           5000\n\n\n\n\nIn SQL use,\n\nSELECT to select.\nCASE to define a case.\nWHEN to select when a patient_id is even.\nTHEN to set insurance to “Yes” if patient_id is even.\nELSE to set has_insurance to “No” when patient_id is not even.\nEND to end the case.\nAS to define the first case values as has_insurance.\nSUM() to add up input.\nCASE to define another case.\nWHEN to select when a patient has an even id.\nTHEN to set cost_after_insurance to 10 if the patient id is even.\nELSE to set the cost_after_insurance to 50 if the patient id is not even.\nEND to end the case.\nAS to define the second case values as cost_after_insurance.\nFROM to define the patients data to select and figure out insurance costs for.\nGROUP BY to group data by has_insurance.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT \n                        CASE WHEN patient_id % 2 = 0 Then 'Yes'\n                        ELSE 'No' \n                        END as has_insurance,\n                     SUM(\n                        CASE WHEN patient_id % 2 = 0 Then 10\n                        ELSE 50 \n                        END\n                         ) AS cost_after_insurance\n                     FROM patients \n                     GROUP BY has_insurance;\")\n\n  has_insurance cost_after_insurance\n1            No                25000\n2           Yes                 5000\n\n\n\n\n\n\n\n10. We are looking for a specific patient. Pull all columns for the patient who matches the following criteria:\n\nFirst_name contains a ‘b’ after the first two letters.\nIdentifies their gender as ‘F’\nTheir weight would be between 50 kg and 70 kg\nTheir patient_id is an odd number\nThey are from the city ‘Burlington’\n\n\n10.010.1 R10.2 SQL\n\n\nContents\n\n10.1 Solution in R\n10.2 Solution in SQL\n\n\n\nIn R define the patients data then use,\n\n%>% to pipe the data.\nfilter to filter the database on first name, gender, weight, patient_id, and city.\ngrepl to select first_names where the 3rd letter is b.\n\n\npatients %>%\n  dplyr::filter(base::grepl(\"^.{2}[b]\", first_name),\n         gender == \"F\",\n         weight > 50 && weight < 80,\n         patient_id %%2==1,\n         city == \"Burlington\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51\n\n\n\n\nIn SQL use:\n\nSELECT to select all * columns.\nFROM to select all columns from the patients data.\nWhere defines multiple conditions.\nLIKE is a condition where first_name has the third letter equal to a lower case b.\nAND to define multiple conditions such as gender, patient_id, and city.\nBETWEEN to define weight is greater than 50, but less than 70.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT *\n                     FROM patients\n                     WHERE\n                      first_name LIKE '__b%'\n                      AND gender = 'F'\n                      AND weight BETWEEN 50 AND 70\n                      AND patient_id % 2 = 1\n                      AND city = 'Burlington';\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51"
  },
  {
    "objectID": "01_qmd-files/index.html",
    "href": "01_qmd-files/index.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "I post bimonthly about a range of topics that are related to data science, statistics, mathematics, or machine learning. Check out my journey, and feel free to leave my posts comments. I’d love to hear from you!\nThanks for stopping by, and please enjoy!\n😘✨"
  },
  {
    "objectID": "01_qmd-files/about.html",
    "href": "01_qmd-files/about.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "About\nI am a data analyst / scientist based out of Portland, Oregon.\nI have been writing about mathematics, statistics, and data science in my blog since 2021.\nI am constantly exploring new topics, and ways to improve upon past and current work.\nI graduated magna cum laude with a Bachelors of Science in Mathematics from Portland State University in 2022."
  },
  {
    "objectID": "02_blog/2023_01_23_R-and-SQLite/index.html",
    "href": "02_blog/2023_01_23_R-and-SQLite/index.html",
    "title": "R and SQLite",
    "section": "",
    "text": "Naruto with SQL logo, and Sauske with the R Studio logo, standing back to back.\n\n\n\n0. Introduction\nThere are a handful of programming languages that data scientists use when querying, analyzing, and manipulating data. What I have found is that while R and Python have a lot more power in what they are capable of producing, SQL is used by a wider variety of roles to access and query data. So to get more practice using both SQL and R I pulled 10 questions and some data off a website called Learn SQL, and will be answering the following questions in both languages.\n\n0.0 Set-Up0.1 Packages0.2 Data0.3 Database\n\n\nContents:\n\n0.1 Packages\n0.2 Data\n0.3 Data base\n\n\n\nFor all my data queries and manipulation in R I will be using base R, dplyr, and magrittr.\n\nlibrary(dplyr)\nlibrary(magrittr)\n\nTo create a SQL database, and run SQL queries in R chunks I will be using a package called RSQLite.\n\nlibrary(RSQLite)\n\n\n\nThis post will use two data sets that I copied from Learn SQL:\n\npatients: Which includes patient_id, first_name, last_name, gender, birth_date, city, province_id, allergies, height, and weight. Note I only copied the first 1000 entries.\n\n\npatients <- utils::read.csv('../../00_data/patients.csv')\n\n\nprovince_names: Which includes province_id, and province_name.\n\n\nprovince_names <- utils::read.csv(\"../../00_data/province_names.csv\")\n\n\n\nTo create a database use:\n\ndbConnect() to connect to a SQL data base called Hospital.db in the 00_data folder.\nSQLite() to connect to a SQLite database file.\n\n\nhosp <- RSQLite::dbConnect(RSQLite::SQLite(),\n                           \"../../00_data/Hospital.db\")\n\nTo define data within the database use:\n\ndbWriteTable() to create a data set within the hospital database called patients and province_names with the respective data.\n\n\nRSQLite::dbWriteTable(hosp,\n                      \"patients\",\n                      patients)\nRSQLite::dbWriteTable(hosp,\n                      \"province_names\",\n                      province_names)\n\nVerify the two data sets are in the database using:\n\ndbListTables() to list the tables within the hosp database.\n\n\nRSQLite::dbListTables(hosp)\n\n[1] \"patients\"       \"province_names\"\n\n\n\n\n\n\n\n1. Show the first ten rows of patients data.\n\n1.01.1 R1.2 SQL\n\n\nContents\n\n1.1 Solution in R\n1.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to view the first 10 rows of the patients data.\n\n\nutils::head(patients, 10)\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\nIn R use:\n\ndbGetQuery() to run SQL commands from a given data base.\n\nIn SQL use:\n\nSELECT to select.\n(*) to include all columns.\nFROM to define the patients data for select to include all columns from.\nLIMIT to only show the top ten rows.\n\n\nRSQLite::dbGetQuery(hosp, \n                    \"SELECT * \n                     FROM patients \n                     LIMIT 10\"\n                    )\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\n\n\n\n2. Show total patients admitted.\n\n2.02.1 R2.2 SQL\n\n\nContents\n\n2.1 Solution in R\n2.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to define a column for total_admissions.\nnrow() to count the rows in the patients data which will equal the total_admissions.\n\n\nbase::data.frame(\"total_admissions\" = base::nrow(patients))\n\n  total_admissions\n1             1000\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count the total number of rows.\nAS to define that count as a new variable, total_admissions.\nFROM to define the patients data for select to count the total numbers of rows for, and define as total_admissions.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT COUNT(*) AS total_admissions \n                     FROM patients\"\n                    )\n\n  total_admissions\n1             1000\n\n\n\n\n\n\n\n3. Show first and last name as full_name.\n\n3.03.1 R3.2 SQL\n\n\nContents\n\n3.1 Solution in R\n3.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to show the first 10 rows of data.\ndata.frame() to define a data frame that includes full_name.\npaste0() to paste together the first_name, a space, and the last_name. This will equal the full_name.\n\n\nutils::head(\n  base::data.frame(\n    full_name = base::paste0(patients$first_name, \n                             \" \", \n                             patients$last_name)), \n  10)\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\nIn SQL use:\n\nSELECT to select.\n|| to concatenate first_name, space, and last name.\nAS to define the concatination as full_name.\nFROM to define the patients data for select to concatenate data from.\nLIMIT to show the first 10 rows of data.\n\n\nRSQLite::dbGetQuery(hosp,\n  \"SELECT first_name || ' ' || last_name AS full_name \n   FROM patients \n   LIMIT 10\"\n   )\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\n\n\n\n4. Show unique cities that are in province_id ‘NS’?\n\n4.04.1 R4.2 SQL\n\n\nContents\n\n4.1 Solution in R\n4.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%>% to pipe data.\nfilter() to filter for all province_id that is equal to “NS”.\nsummerise() to define unique_cites.\nunique() to remove duplicate elements of the city column which will be defined as unique_cites.\n\n\npatients %>% \n  filter(province_id == \"NS\") %>%\n  summarise(unique_cities = unique(city)) \n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\nIn SQL:\n\nSELECT to select.\nDISTINCT() to define city as the column to remove duplicates from.\nAS to define those cities as unique_cites.\nFROM to define the patients data for select to get unique_cites from.\nWHERE specifies a condition.\nIS is the condition that ‘NS’ is equal to province_id.\n\n\ndbGetQuery(hosp,\n           \"SELECT DISTINCT(city) AS unique_cities \n            FROM patients \n            WHERE province_id IS 'NS'\"\n           )\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\n\n\n\n5. Show the total number of male patients and the total number of female patients.\nDisplay the two results in the same row.\n\n5.05.1 R5.2 SQL\n\n\nContents\n\n5.1 Solution in R\n5.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to create a data frame with two columns: male_count, and female_count.\nlength() to count the length of the input.\nwhich() to indicate which gender equals “M” or “F”, and counts that length accordingly.\n\n\nbase::data.frame(\n  male_count = base::length(base::which(patients$gender == 'M')),\n  female_count = base::length(base::which(patients$gender == 'F'))\n  )\n\n  male_count female_count\n1        543          457\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count all input values.\nFROM to define the patients data for select to count on.\nWHERE is a condition defined as gender = “M” or “F”.\nAS is defining the count as male_count, or female_count respectively.\n\n\ndbGetQuery(hosp,\n           \"SELECT \n           (SELECT COUNT(*) FROM patients WHERE gender = 'M') AS male_count, \n           (SELECT COUNT(*) FROM patients WHERE gender = 'F') AS female_count\")\n\n  male_count female_count\n1        543          457\n\n\n\n\n\n\n\n6. Show all allergies ordered by popularity. Remove NULL values from the query.\n\n6.06.1 R6.2 SQL\n\n\nContents\n\n6.1 Solution in R\n6.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%>% pipe the data.\nfilter() to subset data to all allergies that aren’t “NULL”.\ngroup_by() to convert the table into one that is grouped by allergies.\nsummarise() to define total_diagnosis.\nn() to count the size of each group.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by total_diagnosis.\n\n\npatients %>% \n  dplyr::filter(allergies != \"NULL\") %>%\n  dplyr::group_by(allergies) %>%\n  dplyr::summarise(total_diagnosis = dplyr::n()) %>%\n  dplyr::arrange(dplyr::desc(total_diagnosis)) %>%\n  utils::head(10)\n\n# A tibble: 10 × 2\n   allergies   total_diagnosis\n   <chr>                 <int>\n 1 Penicillin              230\n 2 Codeine                  58\n 3 Sulfa                    35\n 4 ASA                      16\n 5 Sulfa Drugs              13\n 6 Tylenol                  11\n 7 Wheat                    11\n 8 Peanuts                  10\n 9 Bee Stings                9\n10 Iodine                    9\n\n\n\n\nIn SQL:\n\nSELECT the allergies column.\nCOUNT(*) count all input.\nAS to define the count as total_diagnosis.\nFROM to define the patients data to select the allergies column from.\nWHERE to define a condition.\nIS NOT is the condition that says allergies cannot be ‘NULL’.\nGROUP BY groups the data by allergies.\nOrder BY to define how the data order is output.\nDESC is the given condition that the data is ordered in descending order by total_diagnosis.\nLIMIT to limit output to the first 10 rows.\n\n\ndbGetQuery(hosp,\n           \"SELECT allergies,\n            COUNT(*) AS total_diagnosis\n            FROM patients\n            WHERE allergies IS NOT 'NULL'\n            GROUP BY allergies\n            ORDER BY total_diagnosis DESC\n            LIMIT 10\"\n           )\n\n     allergies total_diagnosis\n1   Penicillin             230\n2      Codeine              58\n3        Sulfa              35\n4          ASA              16\n5  Sulfa Drugs              13\n6        Wheat              11\n7      Tylenol              11\n8      Peanuts              10\n9       Iodine               9\n10  Bee Stings               9\n\n\n\n\n\n\n\n7. Display the total number of patients for each province. Order by descending.\n\n7.07.1 R7.1 SQL\n\n\nContents\n\n7.1 Solution in R\n7.2 Solution in SQL\n\n\n\nIn R use:\n\nmerge() to join the data sets province_names, and patients by “province_id”.\n%>% to pipe the data.\ngroup_by() to group by province name.\nsummarise() to define the patient count.\nn() to count the number of patients in each province. This will equal patient_count.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by patient_count.\n\n\nbase::merge(province_names, patients, by = \"province_id\") %>%\n  dplyr::group_by(province_name) %>%\n  dplyr::summarise(patient_count = dplyr::n()) %>%\n  dplyr::arrange(dplyr::desc(patient_count))\n\n# A tibble: 8 × 2\n  province_name             patient_count\n  <chr>                             <int>\n1 Ontario                             954\n2 Alberta                              14\n3 British Columbia                     11\n4 Nova Scotia                           9\n5 Manitoba                              7\n6 Newfoundland and Labrador             2\n7 Quebec                                2\n8 Saskatchewan                          1\n\n\n\n\nIn SQL use:\n\nSELECT select the province_name column.\nCOUNT(*) to count all input.\nAS to define count as patient_count.\nFROM to define the patients data AS pa to select province_name to count the number of patients from.\nJOIN to join the province_names data AS pr.\nON is the clause to join data based on pr.province_id = pa.province_id.\nGROUP BY to group the data by pr.province_id.\nORDER BY to define the order of the data output.\nDESC defines that the output data be arranged in order of descending patient_count.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT province_name,\n                     COUNT(*) AS patient_count\n                     FROM patients pa\n                     JOIN province_names pr ON pr.province_id = pa.province_id\n                     GROUP BY pr.province_id\n                     ORDER BY patient_count DESC\")\n\n              province_name patient_count\n1                   Ontario           954\n2                   Alberta            14\n3          British Columbia            11\n4               Nova Scotia             9\n5                  Manitoba             7\n6                    Quebec             2\n7 Newfoundland and Labrador             2\n8              Saskatchewan             1\n\n\n\n\n\n\n\n8. Show the provinces that have more patients identified as ‘M’ than ‘F’. Must only show full province_name.\n\n8.08.1 R8.2 SQL\n\n\nContents\n\n8.1 Solution in R\n8.2 Solution in SQL\n\n\n\nIn R use,\n\nmerge() to join the data sets province_names, and patients by “province_id”.\ngroup_by() to group by province name.\ncount() to count the number of “M” and “F” patients for each province.\nslice() to remove certain rows based on a given criteria.\nwhich.max() to determine which province has a greater number of male patients.\nsummarise() to define only province_name in the output.\n\n\nbase::merge(province_names, patients, by = \"province_id\") %>%\n  dplyr::group_by(province_name) %>%\n  dplyr::count(gender == \"M\", gender == \"F\") %>%\n  dplyr::slice(base::which.max(n)) %>%\n  dplyr::summarise(province_name = province_name)\n\n# A tibble: 8 × 1\n  province_name            \n  <chr>                    \n1 Alberta                  \n2 British Columbia         \n3 Manitoba                 \n4 Newfoundland and Labrador\n5 Nova Scotia              \n6 Ontario                  \n7 Quebec                   \n8 Saskatchewan             \n\n\n\n\nIn SQL use,\n\nSELECT to select pr.province_name column.\nFROM to define the patients data to select pr.province_name from.\nAS to define patients data as pa.\nJOIN to join pa.province data with pr.province_name data.\nAS to define province_name data as pr.\nON to join pa.patients and pr.province_names data by province_id.\nGROUP BY to group data by province_id.\nHAVING to define a clause to filter the data where the number of “M” patients is greater than “F” patients.\nCOUNT() to count the given input.\nCASE to go through the condition of gender = “M” or when gender = “F”\nWHEN to preform the count when the condition is true.\nTHEN 1 END to add a 1 to the count when the case is met.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT pr.province_name\n                     FROM patients AS pa\n                     JOIN province_names AS pr ON pa.province_id = pr.province_id\n                     GROUP BY pr.province_id\n                     HAVING\n                     COUNT( CASE WHEN gender = 'M' THEN 1 END) >\n                     COUNT( CASE WHEN gender = 'F' THEN 1 END)\")\n\n              province_name\n1                   Alberta\n2          British Columbia\n3                  Manitoba\n4 Newfoundland and Labrador\n5               Nova Scotia\n6                   Ontario\n7                    Quebec\n8              Saskatchewan\n\n\n\n\n\n\n\n9. Each admission costs $50 for patients without insurance, and $10 for patients with insurance. All patients with an even patient_id have insurance.\nGive each patient a ‘Yes’ if they have insurance, and a ‘No’ if they don’t have insurance. Add up the admission_total cost for each has_insurance group.\n\n9.09.1 R9.2 SQL\n\n\nContents\n\n9.1 Solution in R\n9.2 Solution in SQL\n\n\n\nIn R use,\n\n%>% to pipe the data.\nmutate() to mutate the data to include has_insurance, and cost_after_insurance information.\ncase_when() to define a case where if the patient id is odd then they don’t have insurance, and if they are even they do have insurance.\ngroup_by() to group the data by has_insurance.\nsummarize() to define cost_after_insurance.\nsum() to add up the cost for all those with and without insurance.\n\n\npatients %>%\n  dplyr::mutate(\n    has_insurance = dplyr::case_when(\n    patient_id %%2==1 ~ \"Yes\",\n    patient_id %%2!=1 ~ \"No\"\n  ),cost_after_insurance = dplyr::case_when(\n    has_insurance == \"Yes\" ~ 10,\n    has_insurance == \"No\" ~ 50\n  )) %>%\n  group_by(has_insurance)  %>%\n  summarise(cost_after_insurance = base::sum(cost_after_insurance))\n\n# A tibble: 2 × 2\n  has_insurance cost_after_insurance\n  <chr>                        <dbl>\n1 No                           25000\n2 Yes                           5000\n\n\n\n\nIn SQL use,\n\nSELECT to select.\nCASE to define a case.\nWHEN to select when a patient_id is even.\nTHEN to set insurance to “Yes” if patient_id is even.\nELSE to set has_insurance to “No” when patient_id is not even.\nEND to end the case.\nAS to define the first case values as has_insurance.\nSUM() to add up input.\nCASE to define another case.\nWHEN to select when a patient has an even id.\nTHEN to set cost_after_insurance to 10 if the patient id is even.\nELSE to set the cost_after_insurance to 50 if the patient id is not even.\nEND to end the case.\nAS to define the second case values as cost_after_insurance.\nFROM to define the patients data to select and figure out insurance costs for.\nGROUP BY to group data by has_insurance.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT \n                        CASE WHEN patient_id % 2 = 0 Then 'Yes'\n                        ELSE 'No' \n                        END as has_insurance,\n                     SUM(\n                        CASE WHEN patient_id % 2 = 0 Then 10\n                        ELSE 50 \n                        END\n                         ) AS cost_after_insurance\n                     FROM patients \n                     GROUP BY has_insurance;\")\n\n  has_insurance cost_after_insurance\n1            No                25000\n2           Yes                 5000\n\n\n\n\n\n\n\n10. We are looking for a specific patient. Pull all columns for the patient who matches the following criteria:\n\nFirst_name contains a ‘b’ after the first two letters.\nIdentifies their gender as ‘F’\nTheir weight would be between 50 kg and 70 kg\nTheir patient_id is an odd number\nThey are from the city ‘Burlington’\n\n\n10.010.1 R10.2 SQL\n\n\nContents\n\n10.1 Solution in R\n10.2 Solution in SQL\n\n\n\nIn R define the patients data then use,\n\n%>% to pipe the data.\nfilter to filter the database on first name, gender, weight, patient_id, and city.\ngrepl to select first_names where the 3rd letter is b.\n\n\npatients %>%\n  dplyr::filter(base::grepl(\"^.{2}[b]\", first_name),\n         gender == \"F\",\n         weight > 50 && weight < 80,\n         patient_id %%2==1,\n         city == \"Burlington\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51\n\n\n\n\nIn SQL use:\n\nSELECT to select all * columns.\nFROM to select all columns from the patients data.\nWhere defines multiple conditions.\nLIKE is a condition where first_name has the third letter equal to a lower case b.\nAND to define multiple conditions such as gender, patient_id, and city.\nBETWEEN to define weight is greater than 50, but less than 70.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT *\n                     FROM patients\n                     WHERE\n                      first_name LIKE '__b%'\n                      AND gender = 'F'\n                      AND weight BETWEEN 50 AND 70\n                      AND patient_id % 2 = 1\n                      AND city = 'Burlington';\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51"
  },
  {
    "objectID": "01_qmd-files/blog.html",
    "href": "01_qmd-files/blog.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "01_blog/2023_01_02_SportsObserveR-package/index.html",
    "href": "01_blog/2023_01_02_SportsObserveR-package/index.html",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "",
    "text": "In this tutorial I will be taking the functions I created in my previous post, SportsObserveR - Part 1: Scraping Functions, and using them to create my own package, SportsObserveR."
  },
  {
    "objectID": "01_blog/2023_01_02_SportsObserveR-package/index.html#technologies",
    "href": "01_blog/2023_01_02_SportsObserveR-package/index.html#technologies",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "0.1 Technologies",
    "text": "0.1 Technologies\nTo build a package in R you need three things:\n\nR installed on your computer.\nA coding editor such as R Studio, or Sublime.\nA bash terminal."
  },
  {
    "objectID": "01_blog/2023_01_02_SportsObserveR-package/index.html#creating-the-function",
    "href": "01_blog/2023_01_02_SportsObserveR-package/index.html#creating-the-function",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "8.1 Creating the Function",
    "text": "8.1 Creating the Function\nFrom the R Studio Console, create the function file, and load it into your enviroment.\n\nuse_r(\"scrape_nba_player_stats\")\n\n\nload_all()\n\nIn the R file named “scrape_nba_player_stats.R” copy the code below:\n\n#' Scrapes NBA player stats tables off basketball-reference.com.\n#'\n#'@import rvest \n#'@import magrittr\n#'\n#'@param name is a char string that corresponds to the players name.\n#'@param stats_tb is a char string that corresponds to the statistics table such as #per_game, #totals, #per_36_minutes, and #advanced.\n#'\n#'@return a data frame of statistics for a specific NBA player. \n#'@export\n#'\n#'@examples\n#'scrape_nba_player_stats(\"Allen Iverson\", \"#per_game\")\nscrape_nba_player_stats <- function(name, stats_tb){\n  # make name lower case\n  lower_case_name <- base::tolower(name)\n\n  # split name \n  split_name <- base::strsplit(lower_case_name, \" +\")[[1]]\n\n  # define first and last name\n  first_name <- split_name[[1]]\n  last_name <- split_name[[2]]\n  \n  # first letter of last name\n  letter <- base::substr(last_name, 1,1)\n  \n  # first five letters of last name \n  last_5 <- base::substr(last_name, 1, 5)\n  \n  # first two letters of first name\n  first_2 <- base::substr(first_name, 1,2)\n  \n  # define team page URL\n  url <- base::paste0(\"https://www.basketball-reference.com/players/\",letter ,\"/\",last_5,first_2,\"01.html\")\n  \n  # Read stats table\n  stats_tb <- url %>%\n  read_html %>%\n  html_node(stats_tb) %>% \n  html_table()\n  \n  # Rename Column 2 to Name \n  names(stats_tb)[2] <- \"Name\"\n  \n  # Replace NA values with 0 (for stat functions)\n  stats_tb[base::is.na(stats_tb)] <- 0\n  \n  # make list a dataframe\n  df <- base::data.frame(stats_tb)\n  \n  base::return(df)\n  }\n\nNow save this file, and in the R Studio Console type document() to create documentation for this function.\n\ndocument()\n\nCheck that the documentation works:\n\n?scrape_nba_player_stats()\n\nUse the check() function to look at any potential errors.\n\ncheck()\n\nIf everything looks good, from the Bash Terminal push this code to github.\n\ngit status\n\n\ngit add --all\n\n\ngit status\n\n\ngit commit -m\"Created second function\"\n\n\ngit push origin main"
  },
  {
    "objectID": "01_blog/2023_01_02_SportsObserveR-package/index.html#creating-tests-1",
    "href": "01_blog/2023_01_02_SportsObserveR-package/index.html#creating-tests-1",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "8.2 Creating Tests",
    "text": "8.2 Creating Tests\nFrom the R Studio Console use use_test() again to create a test file for scrape_nba_player_stats().\n\nuse_test(\"scrape_nba_player_stats\")\n\nNext create a similar code to what was previously done for scrape_nba_team_stats().\n\ntest_that(\"Returns that typeof is list.\", {\n  expect_equal(typeof(scrape_nba_player_stats(\"Kareem Abdul-Jabbar\", \"#totals\")),\n               \"list\")\n})\n\nCheck for errors.\n\ncheck()\n\nIf everything looks green, from the Bash Terminal push to Github.\n\ngit status\n\n\ngit add --all\n\n\ngit status\n\n\ngit commit -m\"Created test for scrape_nba_player_stats\"\n\n\ngit push origin main"
  },
  {
    "objectID": "01_blog/2023_01_02_SportsObserveR-package/index.html#update-readme",
    "href": "01_blog/2023_01_02_SportsObserveR-package/index.html#update-readme",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "8.3 Update README",
    "text": "8.3 Update README\nAdd another example to the readme.rmd file using the scrape_nba_player_stats() function, and then update the .md file.\n\nbuild_readme()\n\nFrom the Bash Terminal push this update to Github.\n\ngit status\n\n\ngit add --all\n\n\ngit status\n\n\ngit commit -m\"added scrape_nba_player_stats example to readme\"\n\n\ngit push origin main"
  },
  {
    "objectID": "dictionary.html",
    "href": "dictionary.html",
    "title": "Terminology",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "02_terminology/Statistics/index.html",
    "href": "02_terminology/Statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "COMING SOON!!"
  },
  {
    "objectID": "terminology.html",
    "href": "terminology.html",
    "title": "terminology",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "02_terminology/Machine-Learning/index.html",
    "href": "02_terminology/Machine-Learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "COMING SOON!!"
  },
  {
    "objectID": "02_terms/Statistics/index.html",
    "href": "02_terms/Statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "COMING SOON!!"
  },
  {
    "objectID": "02_terms/Machine-Learning/index.html",
    "href": "02_terms/Machine-Learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "COMING SOON!!"
  },
  {
    "objectID": "01_blog/2021_06_21_Ballet/index.html",
    "href": "01_blog/2021_06_21_Ballet/index.html",
    "title": "Ballet Data",
    "section": "",
    "text": "Image from the ballet Firebird.\n\n\n\n0. Introduction\nBallet has been a popular form of dance and culture since the fifteen hundreds1. It has changed and evolved a lot since then, but for this analysis I thought it would be fun to investigate some of the most popular ballets.\n\n0.1 Set Up0.2 Collect Data0.3 Create Data Frame\n\n\nI will use the following packages:\n\ndplyr to manipulate the data.\nlubridate to convert the date character in a date variable.\npurr to use map function for links.\ngt to make the table.\nggplot2 to make the graph.\nplotly to make the graph interactive.\n\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(gt) \nlibrary(ggplot2)\nlibrary(plotly)\n\n\n\nTo build this data from scratch, I started with 13 different popular ballets.\n\nBallet <- c(\"Swan Lake\", \n            \"The Nutcracker\", \n            \"Giselle\", \n            \"Romeo and Juliet\",  \n            \"Don Quixote\", \n            \"Cinderella \", \n            \"La Bayadere\", \n            \"Coppelia\", \n            \"The Sleeping Beauty \", \n            \"La Sylphide \", \n            \"Spartacus\", \n            \"Alice's Adventures in Wonder Land\",\n            \"Firebird\")\n\nUsing Wikipedia I am including:\n\nComposers\n\n\nComposer <- c(\"Pyotr Ilyich Tchaikovsky\", \n              \"Pyotr Ilyich Tchaikovsky\",\n              \"Adolph Adam\", \n              \"Sergei Prokofiev\",\n              \"Ludwig Minkus\",\n              \"Sergei Prokofiev\",\n              \"Ludwig Minkus\",\n              \"Leo Delibes\",\n              \"Pyotr Ilyich Tchaikovsky\",\n              \"Filippo Talioni\",\n              \"Leonid Yakobson\",\n              \"Joby Talbo\",\n              \"Igor Stravinsky \")\n\n\nChoreographers\n\nNote ballets with only one choreographer will show the second choreographer as blank.\n\nchoreographer_1 <- c(\"Julius Reisinger\",\n                     \"Marius Petipa\", \n                     \"Jean Coralli\",\n                     \"Ivo Vana-Psota\",\n                     \"Marius Petipa\", \n                     \"Rosstislav Zakharov\",\n                     \"Marius Petipa\",\n                     \"Arthur Saint-Leon\",\n                     \"Marius Petipa\",\n                     \"Jean-Madeline Schnitzhoeffer\",\n                     \"Aram Khachaturian\",\n                     \"Christopoher Wheeldon\",\n                     \"Michel Fokine\")\nchoreographer_2 <- c(\"\",\n                     \"Lev Ivanov\",\n                     \"Jules Perrot\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\",\n                     \"\")\n\n\nNumber of Acts\n\n\nActs <- c(4, 2, 2, 4, 3, 3, 4, 2, 3, 2, 3, 3, 1)\n\n\nDate Premiered\n\n\nmonth_premiered <- c( 3, 12, 6, 12, 12, 11, 2, 5, 1, 3, 12, 2, 6)\nday_premiered <- c( 4, 18, 28, 30, 26, 21, 11, 25, 5, 12, 27, 28, 9)\nyear_premiered <- c(1877, 1892, 1841, 1938, 1869, 1945, 1877, 1870, 1890, 1832, 1956, 2011, 1910)\n\n\nLinks to Youtube Videos of each Ballet\n\n\nLinks <- c(\"https://www.youtube.com/watch?v=LbUatYSm8ME\", \n            \"https://www.youtube.com/watch?v=tR_Z1LUDQuQ\", \n            \"https://www.youtube.com/watch?v=VroMXEDLTq8\", \n            \"https://www.youtube.com/watch?v=7AnpPu7j6Dg\",  \n            \"https://www.youtube.com/watch?v=pazAS4cNi7w\", \n            \"https://www.youtube.com/watch?v=LPbKZXNfJ-Y\", \n            \"https://www.youtube.com/watch?v=zTR4Oco_0Bc\", \n            \"https://www.youtube.com/watch?v=uE2fjFMag7E\", \n            \"https://www.youtube.com/watch?v=EDFlRq5RnbQ\", \n            \"https://www.youtube.com/watch?v=R_RFxSLar2A\", \n            \"https://www.youtube.com/watch?v=Fha6rYtaLMk\", \n            \"https://www.youtube.com/watch?v=c2wWq25p5Sk\",\n            \"https://www.youtube.com/watch?v=Yo9L9H--t3k\")\n\n\n\nFor the ballet dataframe, ballet_df, I will use:\n\nmake_date() to make the character variables a date.\narrange() to arrange the df alphabetically by Ballet.\n\nI also did not include choreographer_2, because it only provided two data points which did not contribute a lot to this short analysis.\n\nballet_df <- base::data.frame(\n  Ballet,\n  Composer,\n  choreographer_1,\n  Acts,\n  date_premiered = lubridate::make_date(year_premiered,\n                                        month_premiered,\n                                        day_premiered)) %>%\n  dplyr::arrange(Ballet)\n\n\n\n\n\n\n1. Code\n\n1.1 Code for Table1.2 Code for Interactive Plot\n\n\nTo add the hyper-linked text I used:\n\nmutate() to overwrite exsiting variables.\nmap() to transform the input into an object of the same length.\na() to create an R object that represents an HTML hyperlink tag.\n.x stands for the vector that is being input.\nhtml() so the HTML tag will come through as HTML when rendered.\nas.character() to convert the input to a character.\n\nTo create the table I used:\n\ngt() to create the table.\n\n\nballet_table <- ballet_df %>%\n    dplyr::mutate(\n      Links = map(Links, ~ htmltools::a(href = .x, \"Youtube\")),\n      Links = map(Links, ~ gt::html(as.character(.x)))) %>%\n    gt::gt()\n\n\n\nTo create a ggplot I used:\n\nggplot() to create the plot, and define the mapping.\ngeom_point() to create a scatterplot.\nlabs() to define the labels.\nggtitle to define the title.\n\n\nballet_plot <- ballet_df %>%\n  ggplot2::ggplot(mapping = aes(x = date_premiered, \n                                y = Ballet, \n                                color = factor(Acts))) +\n  ggplot2::geom_point() +\n  ggplot2::labs(x = \"Date Premiered\", \n                y = \"Ballet\", \n                color = \"Number of Acts\") +\n  ggplot2::ggtitle(\"Ballets by Date Premiered\")\n\n\n\n\n\n\n2. Visuals\n\n2.1 Table2.2 Interactive Plot\n\n\n\n\n\n\n\n\n  \n  \n    \n      Ballet\n      Composer\n      choreographer_1\n      Acts\n      date_premiered\n      Links\n    \n  \n  \n    Alice's Adventures in Wonder Land\nJoby Talbo\nChristopoher Wheeldon\n3\n2011-02-28\nYoutube\n    Cinderella \nSergei Prokofiev\nRosstislav Zakharov\n3\n1945-11-21\nYoutube\n    Coppelia\nLeo Delibes\nArthur Saint-Leon\n2\n1870-05-25\nYoutube\n    Don Quixote\nLudwig Minkus\nMarius Petipa\n3\n1869-12-26\nYoutube\n    Firebird\nIgor Stravinsky \nMichel Fokine\n1\n1910-06-09\nYoutube\n    Giselle\nAdolph Adam\nJean Coralli\n2\n1841-06-28\nYoutube\n    La Bayadere\nLudwig Minkus\nMarius Petipa\n4\n1877-02-11\nYoutube\n    La Sylphide \nFilippo Talioni\nJean-Madeline Schnitzhoeffer\n2\n1832-03-12\nYoutube\n    Romeo and Juliet\nSergei Prokofiev\nIvo Vana-Psota\n4\n1938-12-30\nYoutube\n    Spartacus\nLeonid Yakobson\nAram Khachaturian\n3\n1956-12-27\nYoutube\n    Swan Lake\nPyotr Ilyich Tchaikovsky\nJulius Reisinger\n4\n1877-03-04\nYoutube\n    The Nutcracker\nPyotr Ilyich Tchaikovsky\nMarius Petipa\n2\n1892-12-18\nYoutube\n    The Sleeping Beauty \nPyotr Ilyich Tchaikovsky\nMarius Petipa\n3\n1890-01-05\nYoutube\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nA Breif History of Ballet↩︎"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "<iframe width=“800” height=“800 src=”Resume/Randi-Bolt-Resume.pdf”"
  },
  {
    "objectID": "01_blog/2022_12_12_Data-Science-Interview-Questions-P1/index.html",
    "href": "01_blog/2022_12_12_Data-Science-Interview-Questions-P1/index.html",
    "title": "Data Science Interview Questions - Part 1",
    "section": "",
    "text": "1. What is the difference between supervised and unsupervised learning?\nSupervised Learning: Uses labeled data for prediction. (Logistical Regression, Linear Regression, Decision Tree)\nUnsupervised Learning: Uses unlabeled data for analysis such as identifying hidden patterns in clustering, association, and anomalies or errors. (K-means clustering, hierarchical clustering)\n\n\n2. What is the difference between univariate, bivariate, and multivariate analysis?\nUnivariate: Looks at only one variable at a time.\nBivariate: Compares two variables.\nMultivariate: Compares more than two variables.\n\n\n3. What is the difference between wide format data and long format data?\nWide Format Data: Has a column for each variable.\nLong Format Data: Has a column for possible variable types, and a value for each of those variables.\n\n\n4. What is the difference between normalization and stadardization?\nNormalization: Rescales the values into a range of [0,1].\nStandardization: Rescales data to have a mean of 0 and a standard deviation of 1.\n\n\n5. What is variance?\nVariance: \\(s^2=\\frac{\\sum(x_i-\\bar{x})}{n-1}\\) is a measure of spread within the data.\n\n\n6. What is a normal distribution?\nNormal distribution: (Gaussian distribution or bell curve) is a probability distribution that is symmetrical about the mean.\n\n\n7. What is the law of large numbers?\nLaw of Large Numbers: if an experiment is repeated independently a large number of times then the average results of the obtained results should be close to the expected value.\n\n\n8. What is the goal of A/B testing?\nTo determine which experiment A or B preformed better.\n\n\n9. You are given a dataset consisting of variables with more than 30 percent missing values. How do you deal with them?\n\nDelete rows or columns with missing values. This can be problematic if it means loosing valuable data.\nFill in the missing values with an approximation of the average of the other values in the column.\n\n\n\n10. For the given points how do you calculate the Euclidean distance in Python?\npoint 1 = (2,3) and point 2 = (1,1)\n\nimport numpy as np\n \n# initializing points in numpy arrays\npoint1 = np.array((1, 2, 3))\npoint2 = np.array((1, 1, 1))\n \n# calculating Euclidean distance using linalg.norm()\ndist = np.linalg.norm(point1 - point2)\n \n# printing Euclidean distance\nprint(dist)\n\n2.23606797749979\n\n\nNote: Check out the distance package I created to solve this same problem using R.\n\n\n11. How do you find RMSE and MSE in a linear regression model?\nRoot Mean Square Error (RMSE): \\(\\text{RMSE}=\\sqrt{\\text{MSE}}=\\sqrt\\frac{\\sum{(y_i-\\hat{y_i})^2}}{n}\\)\nMean Square Error (MSE): \\(\\text{MSE}=\\frac{\\sum{(\\text{observed}-\\text{predicted})^2}}{n}=\\frac{\\sum{(y_i-\\hat{y_i})^2}}{n}\\)\nNote: The values are squared is to prevent negative values, and to increase the impact of of larger errors.\n\n\n12. What is the significance of p-value?\nThe p value is the probability that the null hypothesis is true. Meaning there is no statistical significance that exists between variables.\nWhen p is \\(\\leq\\) 0.05 then we can reject the null hypothesis.\nWhen p is > 0.05 then we fail to reject the null hypothesis aka accept it."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html",
    "title": "Final Prep. Group Theory",
    "section": "",
    "text": "Notes consist of order, cyclic groups, counting cosets, homomorphisms, normal subgroups, quotient groups, and the Fundamental Homomorphism Theorem."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems",
    "title": "Final Prep. Group Theory",
    "section": "Theorems",
    "text": "Theorems\nTheorem 3: Let G be a group and let \\(a\\in G\\) have order \\(n<\\infty\\). Then every power of a is equal to exactly one \\(e,a,a^2,a^3,...,a^{n-1}\\)\nCorollary: Let ord(a)=\\(n<\\infty\\). Then the cyclic group generated by a has n elements. \\(\\langle a\\rangle=\\{...,a^{-2},a^{-1},e,a,a^2,a^3\\}=\\{e,a,a^2,...,a^{n-1}\\}\\).\nTheorem 4: Let ord(a)=\\(\\infty\\). Then for all \\(i,j\\in\\mathbb{Z}\\), if \\(i\\in j\\) then \\(a^i\\ne a^j\\).\n(No power of a gives the identity, and no power of a is the same.)\nCorollary: If ord(a)=\\(\\infty\\), the cyclic group generated by a has infintely many elements: \\(\\langle a\\rangle =\\{...,a^{-2},a^{-1},e,a,a^{2},...\\}\\)\nTheorem 5: Let \\(a\\in G\\) have order \\(n<\\infty\\). Then \\(a^m=e\\) if and only if m is a multiple of n."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#facts-about-cyclic-groups",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#facts-about-cyclic-groups",
    "title": "Final Prep. Group Theory",
    "section": "Facts about Cyclic Groups",
    "text": "Facts about Cyclic Groups\nLet \\(G=\\langle a\\rangle=\\{e,a,a^2,a^3,...,a^{n-1}\\}\\) be a cyclic group of size n. \n\nG is abelian (Since \\(a^ia^j=a^{i+j}=a^{j}a^{i}\\))\nLet \\(k\\geq 1\\) be a positive divisor of \\(|G|=n\\). Then G has exactly one subgroup of size k.\nThe subgroup of G of size 1 is \\(\\langle e\\rangle =\\{e\\}\\). If \\(k>1\\) is a divisor of n, the subgorup of size k is \\(\\langle a^{n/k}\\rangle\\)"
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#definitions",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#definitions",
    "title": "Final Prep. Group Theory",
    "section": "Definitions",
    "text": "Definitions\ncoset: Let G be a group and H a subgroup of G. For any \\(a\\in G\\), the set \\(Ha=\\{ha|h\\in H\\}.\\) This is a (right) \\(\\underline{\\text{coset}}\\) of H in G.\nindex: The number of cosets of H in G, n, is called the \\(\\underline{\\text{index}}\\) of H in G."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#facts-about-cosets",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#facts-about-cosets",
    "title": "Final Prep. Group Theory",
    "section": "Facts about Cosets",
    "text": "Facts about Cosets\n\nAll cosets have the same size as H.\nDistinct cosets don’t overlap.\nEvery element of G is in some coset of H.\nHow should we choose a so that we don’t get repeated cosets? Choose a to be an element of G that hasn’t yet appeared in a coset.\nHow do we know when we’ve listed all cosets? We’re done when every element of G has appeared in a coset."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-1",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-1",
    "title": "Final Prep. Group Theory",
    "section": "Theorems",
    "text": "Theorems\nLemma: Distinct cosets don’t overlap.\nCorollary: If \\(a\\in Hb\\) then \\(Ha=Hb\\).\nTheorem 1: The set \\(\\{Ha|a\\in G\\}\\) of all cosets of H is a partiion of G.\nCorollary: Define a relation \\(\\sim\\) on G by \\(a\\sim b\\) iff a and b are in the same coset of H.\nTheorem 2: Every coset of H in G has the same size as H.\nLagrange’s Theorem: Let G be a finite group and H be a subgroup of G. Then the size of H divides the size of G.\nTheorem 4: Let \\(p\\in\\mathbb{Z}\\) be prime. If \\(|G|=p\\), then \\(G\\cong\\mathbb{Z}_p\\).\nTheorem 5: Let G be a finite group and \\(a\\in G\\). Then ord(a) divides \\(|G|\\)."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#definitions-1",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#definitions-1",
    "title": "Final Prep. Group Theory",
    "section": "Definitions",
    "text": "Definitions\nHomomorphism: Let \\(G_1\\) and \\(G_2\\) be groups. A \\(\\underline{\\text{homomorphism}}\\) is a function \\(f:G_1\\rightarrow G_2\\) such that for all \\(a,b\\in G_1\\), \\(f(ab)=f(a)f(b)\\).\nHomomorphic Image: If \\(f:G_1\\rightarrow G_2\\) is an onto homomorphism, we call \\(G_2\\) a \\(\\underline{\\text{homomorphic image}}\\) of \\(G_1\\).\nKernel: Let \\(f:G_1\\rightarrow G_2\\) be a homomorphism. The \\(\\underline{\\text{kernel}}\\) of f is the set of all elements in \\(G_1\\) that get sent to \\(e_2\\in G_2\\): \\(\\text{ker}(f)=\\{x\\in G_1|f(x)=e_2\\}.\\)\nConjugate: Let G be a group and \\(g_1x\\in G\\). The element \\(gxg^{-1}\\) is called the \\(\\underline{\\text{conjugate}}\\) of x by g. Going from x to \\(gxg^{-1}\\) is called \\(\\underline{\\text{conjugation by g}}\\).\nNormal: Let H be a subgroup of G. We call H a \\(\\underline{\\text{normal}}\\) subgroup of G if for all \\(g\\in G\\) and \\(h\\in H\\), the conjugate \\(ghg^{-1}\\) is in H. Notation: \\(H \\unlhd G\\)\nNormal Subgroup: A subgroup H of G is called a \\(\\underline{\\text{normal subgroup}}\\) if for all \\(g\\in G\\) and \\(h\\in H\\). \\(ghg^{-1}\\in H\\)."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-2",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-2",
    "title": "Final Prep. Group Theory",
    "section": "Theorems",
    "text": "Theorems\nTheorem 1: Let \\(f:G_1\\rightarrow G_2\\) be a homomorphism. Then\n\n\\(f(e_1)=e_2\\)\n\\(f(a^{-1})=[f(a)]^{-1}\\forall a\\in G_1\\)\n\nTheorem: Let \\(f:G_1\\rightarrow G_2\\) be a homomorphism. Then f is one-to-one if and only if ker(f)=\\(\\{e_1\\}\\).\nTheorem: If G is abelian, every subgroup of G is normal.\nTheorem: Let H be a subgroup of G. Then H is normal in G if and only if \\(aH=Ha\\) for all \\(a\\in G\\).\nTheorem: A subgroup H of G is normal in G iff \\(aH=Ha\\forall a\\in G\\).\nTheorem 2: Let \\(f:G_1\\rightarrow G_2\\) be a homomorphism.\n\nran(f)=\\(\\{f(x)|x\\in G_1\\}\\) is a subgroup of \\(G_2\\)\nker(f) is a normal subgroup of \\(G_1\\)."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-3",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-3",
    "title": "Final Prep. Group Theory",
    "section": "Theorems",
    "text": "Theorems\nTheorem 2: Let H be a normal subgroup of G. If \\(Ha=Hc\\) and \\(Hb=Hd\\), then \\(H(ab)=H(cd)\\).\nTheorem 3: Let H be a normal subgroup of G. The set \\(G/H=\\{Ha|a\\in G\\}\\)\nTheorem 4: Let H be a normal subgroup of G. The function \\(f:G\\rightarrow G/H\\) define by \\[f(a)=Ha\\] is a surjective homomorphsim with kernel H.\nTheorem 5: Let H be a subgroup of G.\n\n\\(Ha=Hb\\) if and only if \\(ab^{-1}\\in H\\).\n\\(Ha=H\\) if and only if \\(a\\in H\\)."
  },
  {
    "objectID": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-4",
    "href": "01_blog/2022_05_23_Final-Prep-Group-Theory/index.html#theorems-4",
    "title": "Final Prep. Group Theory",
    "section": "Theorems",
    "text": "Theorems\nTheorem 1: Let \\(f:G\\rightarrow H\\) be a homomorphism with kernel K. Then \\(f(a)=f(b) \\Leftrightarrow Ka=Kb\\).\nTheorem 2: Let \\(f:G\\rightarrow H\\) be a surjective (onto) homomorphism with kernel K. Then \\(G/K\\cong H\\)."
  },
  {
    "objectID": "01_blog/2021_09_20_APIs-and-tidycensus/index.html",
    "href": "01_blog/2021_09_20_APIs-and-tidycensus/index.html",
    "title": "API’s and tidycensus",
    "section": "",
    "text": "1. Set-up\nTo start Request a Key to get an API key.\nThen create an .Renviron file to your projects main directory with “CENSUS_API_KEY=XXXXXXXXXXX”, where all the X’s represent you key.\nNote:\n\nThis key will not work with spaces on either side of the equal sign.\ntidycensus already has this utility worked into it (read ?census_api_key). They call their api key CENSUS_API_KEY (it is common for this key to be in all caps), so that is what I also called mine. This will be especially helpful in not mixing up API keys if I use other API keys in the future.\n\nNow load the tidycensus package and use readRenviron() to access the API key.\n\nlibrary(tidycensus)\nbase::readRenviron(\"../../.Renviron\")\n\nNote:\n\nThe first time you access your API key you may want to reload your environment so you don’t have to restart R.\n../ tells your machine to go one folder outside the folder it is in.\nUse Sys.getenv(\"CENSUS_API_KEY\") to check your key is accesible and correct.\n\n\n\n2. Using tidycensus\nUse load_variables(year, dataset, chache=T/F) for various data sets. Read ?load_variables() for more information.\nNote:\n\nlabel shows the estimates by total, and then sex and age range.\nconcept is by sex, then race, origins, and ancestry.\n\n\na <- tidycensus::load_variables(2019, \"acs1\")\nutils::head(a, 10)\n\n# A tibble: 10 × 3\n   name       label                                    concept   \n   <chr>      <chr>                                    <chr>     \n 1 B01001_001 Estimate!!Total:                         SEX BY AGE\n 2 B01001_002 Estimate!!Total:!!Male:                  SEX BY AGE\n 3 B01001_003 Estimate!!Total:!!Male:!!Under 5 years   SEX BY AGE\n 4 B01001_004 Estimate!!Total:!!Male:!!5 to 9 years    SEX BY AGE\n 5 B01001_005 Estimate!!Total:!!Male:!!10 to 14 years  SEX BY AGE\n 6 B01001_006 Estimate!!Total:!!Male:!!15 to 17 years  SEX BY AGE\n 7 B01001_007 Estimate!!Total:!!Male:!!18 and 19 years SEX BY AGE\n 8 B01001_008 Estimate!!Total:!!Male:!!20 years        SEX BY AGE\n 9 B01001_009 Estimate!!Total:!!Male:!!21 years        SEX BY AGE\n10 B01001_010 Estimate!!Total:!!Male:!!22 to 24 years  SEX BY AGE\n\n\nLet’s only focus on the first line for now, “B01001_001” which should be the total estimates. Then we can use get_acs() to get data population data by state from the American Community Survey.\n\nb <- tidycensus::get_acs(geography = \"state\", year = 2019, variable = \"B01001_001\")\nutils::head(b, 10)\n\n# A tibble: 10 × 5\n   GEOID NAME                 variable   estimate   moe\n   <chr> <chr>                <chr>         <dbl> <dbl>\n 1 01    Alabama              B01001_001  4876250    NA\n 2 02    Alaska               B01001_001   737068    NA\n 3 04    Arizona              B01001_001  7050299    NA\n 4 05    Arkansas             B01001_001  2999370    NA\n 5 06    California           B01001_001 39283497    NA\n 6 08    Colorado             B01001_001  5610349    NA\n 7 09    Connecticut          B01001_001  3575074    NA\n 8 10    Delaware             B01001_001   957248    NA\n 9 11    District of Columbia B01001_001   692683    NA\n10 12    Florida              B01001_001 20901636    NA\n\n\nWe can get similar population estimates setting the variable = c(“POP), with get_estimates. As well as”DENSITY”; for housing unit estimates, c(“HUEST”); and for components of change estimates, c(“BIRTHS”, “DEATHS”, “DOMESTICMIG”, “INTERNATIONALMIG”, “NATURALINC”, “NETMIG”, “RBIRTH”, “RDEATH”, “RDOMESTICMIG”, “RINTERNATIONALMIG”, “RNATURALINC”, “RNETMIG”).\n\nc <- tidycensus::get_estimates(geography = \"state\", year = 2019, variable = c(\"POP\"))\nutils::head(c, 10)\n\n# A tibble: 10 × 4\n   NAME                 GEOID variable    value\n   <chr>                <chr> <chr>       <dbl>\n 1 Alabama              01    POP       4903185\n 2 Alaska               02    POP        731545\n 3 Arizona              04    POP       7278717\n 4 Arkansas             05    POP       3017804\n 5 California           06    POP      39512223\n 6 Colorado             08    POP       5758736\n 7 Delaware             10    POP        973764\n 8 District of Columbia 11    POP        705749\n 9 Connecticut          09    POP       3565287\n10 Florida              12    POP      21477737\n\n\n\nd <- tidycensus::get_estimates(geography = \"county\", state = \"OR\", year = 2019, variable = c(\"POP\"))\nutils::head(d, 10)\n\n# A tibble: 10 × 4\n   NAME                      GEOID variable  value\n   <chr>                     <chr> <chr>     <dbl>\n 1 Lane County, Oregon       41039 POP      382067\n 2 Washington County, Oregon 41067 POP      601592\n 3 Clatsop County, Oregon    41007 POP       40224\n 4 Jackson County, Oregon    41029 POP      220944\n 5 Clackamas County, Oregon  41005 POP      418187\n 6 Tillamook County, Oregon  41057 POP       27036\n 7 Josephine County, Oregon  41033 POP       87487\n 8 Umatilla County, Oregon   41059 POP       77950\n 9 Columbia County, Oregon   41009 POP       52354\n10 Wasco County, Oregon      41065 POP       26682"
  },
  {
    "objectID": "01_blog/2022_02_21_UO-ggplot2/index.html",
    "href": "01_blog/2022_02_21_UO-ggplot2/index.html",
    "title": "UO: ggplot2 Part 2",
    "section": "",
    "text": "1. Set Up\nFor this post we used the following packages:\n\nggplot2: to create nice looking plots.\nmagrittr: to pipe %>%.\ndplyr: to use filter().\nflametree: to make art.\nozmaps: to make Australian Maps.\nrmapshaper: to use ms_simplyfy to simplify polygons.\nplotly: to create interactive plots.\n\n\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(flametree)\nlibrary(ozmaps)\nlibrary(rmapshaper)\nlibrary(plotly)\n\nAnd the following data sets:\n\ncars\nBOD(Biochemical Oxygen Demand)\n\n\ndata(\"cars\")\ndata (\"BOD\")\n\n\n\n2. ggplot2 Review\nTo start we made a simple point plot using the cars data set.\nNote: ggplot(data= <DATA>, mapping = aes(<MAPPING>))+ <GEOM FUNCTION>()\n\nggplot2::ggplot(data = mpg, mapping = ggplot2::aes(x = displ, y = hwy)) + \n  ggplot2::geom_point()\n\n\n\n\nWe can compare this to a simple plot in base R.\nNote : the $ is how we id the specific variable we are wanting to work with.\n\nbase::plot(mpg$displ, mpg$hwy)\n\n\n\n\n\n\n3. Line Graph\nStarting with base R:\nNote: help(pressure) is the same as ?pressure\n\nbase::plot(pressure$temperature, pressure$pressure, type = \"l\")\n# add points\ngraphics::points(pressure$temperature, pressure$pressure)\n# add lines (and points)\ngraphics::lines(pressure$temperature, pressure$pressure/2, col = \"red\")\ngraphics::points(pressure$temperature, pressure$pressure/2, col = \"red\")\n\n\n\n\nggplot:\n\nggplot2::ggplot(pressure, ggplot2::aes(x = temperature, y = pressure)) + \n  ggplot2::geom_line() + \n  ggplot2::geom_point() + \n  ggplot2::geom_line(ggplot2::aes(x = temperature, y = pressure/2), color = \"red\") + \n  ggplot2::geom_point(ggplot2::aes(x = temperature, y = pressure/2), color = \"red\") \n\n\n\n\n\n\n4. Bar Graphs\nBase R:\n\ngraphics::barplot(BOD$demand, names.arg = BOD$Time)\n\n\n\n\n\ngraphics::barplot(base::table(mtcars$cyl))\n\n\n\n\nggplot2:\n\nggplot2::ggplot(BOD, ggplot2::aes(x = base::factor(Time), y = demand)) + \n  ggplot2::geom_col()\n\n\n\n\nNotice that the 6 isn’t there because of factor().\nNote : geom_bar does counts, but column has the height of the bar based on the data.\n\nggplot2::ggplot(mtcars, aes(x=cyl)) +\n  ggplot2::geom_bar()\n\n\n\n\n\n\n5. Histogram\nBase R:\n\ngraphics::hist(mtcars$mpg, breaks = 4)\n\n\n\n\nggplot2:\n\nggplot2::ggplot(mtcars, ggplot2::aes(x=mpg)) +\n  ggplot2::geom_histogram(binwidth = 4)\n\n\n\n\n\n\n6. Boxplot\nBase R:\n\nbase::plot(ToothGrowth$supp, ToothGrowth$len)\n\n\n\n\nBase R: Formula Syntax\n\nbase::plot(len ~ supp, data = ToothGrowth)\nbase::plot(len ~ supp + dose, data = ToothGrowth)\n\n\n\n\n\n\n\nggplot2:\n\nggplot2::ggplot(ToothGrowth, ggplot2::aes(x= supp, y = len)) +\n  ggplot2::geom_boxplot()\n\n\n\n\n\n\n7. Time Series\nggplot2 will automatically recognize the variable as a date as long as the variable is imported as a date.\nTo start create some dummy data:\n\ndata <- base::data.frame(\n  day = base::as.Date(\"2017-06-14\")-0:364,\n  value = stats::runif(365)\n)\nutils::head(data)\n\n         day     value\n1 2017-06-14 0.2255421\n2 2017-06-13 0.2302054\n3 2017-06-12 0.7440591\n4 2017-06-11 0.9867589\n5 2017-06-10 0.1083051\n6 2017-06-09 0.2689003\n\n\nThen plot it with ggplot2:\n\nggplot2::ggplot(data, ggplot2::aes(x = day, y = value)) +\n  ggplot2::geom_line()\n\n\n\n\nNow to make a plot with the economics data set which is included in ggplot2.\n\nggplot2::ggplot(data = economics, ggplot2::aes(x = date, y = pop)) +\n  ggplot2::geom_line()\n\n\n\n\nNext create a subset of the data from 2006 and beyond:\n\nsubset <- ggplot2::economics %>%\n  dplyr::filter(date>base::as.Date(\"2006-1-1\"))\n\nNow to create a different line graph of the subset data over time where the size of the line is based on the value of unemployment (which is the number of unemployment in thousands).\n\nggplot2::ggplot(economics, ggplot2::aes(x = date, y = pop)) +\n  ggplot2::geom_line(ggplot2::aes(size = unemploy), color = \"red\")\n\n\n\n\n\n\n8. Maps\nUsing map_data() get lat and long data for counties in Oregon:\n\nor_counties <- ggplot2::map_data(\"county\", \"oregon\") %>%\n  dplyr::select(lon = long, lat, group, id = subregion)\nutils::head(or_counties)\n\n        lon      lat group    id\n1 -117.2042 44.30683     1 baker\n2 -117.4907 44.30683     1 baker\n3 -117.4907 44.38704     1 baker\n4 -117.5366 44.42142     1 baker\n5 -117.5709 44.42142     1 baker\n6 -117.5996 44.43861     1 baker\n\n\nUsing or_counties data create a ggplot2 map:\n\nggplot2::ggplot(or_counties, ggplot2::aes(lon, lat, group = group))+\n  ggplot2::geom_polygon(fill = \"white\", color = \"grey\") +\n  ggplot2::coord_quickmap()\n\n\n\n\nUsing ozmap_states get the names of the different states in Australia.\n\noz_stats <- ozmaps::ozmap_states\noz_stats\n\nSimple feature collection with 9 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 105.5507 ymin: -43.63203 xmax: 167.9969 ymax: -9.229287\nGeodetic CRS:  GDA94\n# A tibble: 9 × 2\n  NAME                                                                  geometry\n* <chr>                                                       <MULTIPOLYGON [°]>\n1 New South Wales              (((150.7016 -35.12286, 150.6611 -35.11782, 150.6…\n2 Victoria                     (((146.6196 -38.70196, 146.6721 -38.70259, 146.6…\n3 Queensland                   (((148.8473 -20.3457, 148.8722 -20.37575, 148.85…\n4 South Australia              (((137.3481 -34.48242, 137.3749 -34.46885, 137.3…\n5 Western Australia            (((126.3868 -14.01168, 126.3625 -13.98264, 126.3…\n6 Tasmania                     (((147.8397 -40.29844, 147.8902 -40.30258, 147.8…\n7 Northern Territory           (((136.3669 -13.84237, 136.3339 -13.83922, 136.3…\n8 Australian Capital Territory (((149.2317 -35.222, 149.2346 -35.24047, 149.271…\n9 Other Territories            (((167.9333 -29.05421, 167.9188 -29.0344, 167.93…\n\n\nThen create a ggplot2 map of Australia:\n\nggplot2::ggplot(oz_stats)+\n  ggplot2::geom_sf() +\n  ggplot2::coord_sf()\n\n\n\n\nNext we remove the “Other territories”, and create a multi-polygon data set of Australian Bureau of Statistics.\n\noz_stats <- ozmaps::ozmap_states %>% \n  dplyr::filter(NAME != \"Other Territories\")\noz_votes <- rmapshaper::ms_simplify(ozmaps::abs_ced)\n\nThen create another map of Australian territories:\n\nggplot2::ggplot()+\n  ggplot2::geom_sf(data = oz_stats, mapping = ggplot2::aes(fill = NAME)) +\n  ggplot2::geom_sf(data = oz_votes, fill = NA) +\n  ggplot2::coord_sf()\n\n\n\n\n\n\n9. Plotly\nAn interactive graph of the iris data:\n\nfig <- plotly::plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length)\nfig\n\n\n\n\n\nAn interactive plot of the cars data set:\n\nmpg %>% plotly::plot_ly(x = ~displ, y = ~mpg, color = ~class)\n\n\n\n\n\nNote: you can double click on the legend to see a subset of the data.\nAnother plot with ggplot2:\n\nplot <- ggplot2::ggplot(mpg, ggplot2::aes(x = displ, y = hwy)) + \n  ggplot2::geom_point(mapping = aes(color = class)) +\n  ggplot2::geom_smooth()\nplot\n\n\n\n\nUse ggplotly to make it interactive:\n\nplotly::ggplotly(plot)\n\n\n\n\n\n\n\n10. Art\n\nshades <- c(\"blue\", \"green\", \"red\", \"orange\")\ndata <- flametree::flametree_grow(time = 10, trees = 10)\ndata %>% flametree::flametree_plot(\n  background = \"white\",\n  palette = shades,\n  style = \"nativeflora\"\n)\n\n\n\n\nPackage by Danielle Navarro. Check out her art."
  },
  {
    "objectID": "01_blog/2021_08_23_Emojis/index.html",
    "href": "01_blog/2021_08_23_Emojis/index.html",
    "title": "Enable Emojis in Quarto",
    "section": "",
    "text": "1. Visual Editor\nOne way to add emojis to a quarto blog posts is from the ‘Visual’ editor in R Studio, select the Insert tab \\(\\rightarrow\\) ‘Special Characters’ \\(\\rightarrow\\) ‘Insert Emoji …’ and then selecting you desired emoji from a large list.\n\n\n2. Include 'from: markdown+emoji' in Header\nThe second way to add emoji’s into a quarto blog is to include ‘from: markdown+emoji’ in the .qmd header, and then type the name of the emoji you want to include encased in colons as shown below:\n:grinning: \\(\\rightarrow\\) 😀\n:smile: \\(\\rightarrow\\) 😄\n:heart: \\(\\rightarrow\\) ❤️\n:thumbsup: \\(\\rightarrow\\) 👍\n:call_me_hand: \\(\\rightarrow\\) 🤙\n\n\n\n\n\nFootnotes\n\n\nQuarto Documentation - Content Editing↩︎"
  },
  {
    "objectID": "01_blog/2022_06_20_Final-Prep-Statistics/index.html",
    "href": "01_blog/2022_06_20_Final-Prep-Statistics/index.html",
    "title": "Final Prep. Statistics",
    "section": "",
    "text": "1. Method of Distribution Function (CDF)\n\nGiven the density function \\(f(y)\\) the distribution function \\[F_Y(y)=\\int_{\\text{lower}}^{y}f(t)dt=F_Y(t)|_{t=\\text{lower}}^{t=y}=F_Y(y)\\]\nFor \\(U=\\color{red}\\text{FUNCTION WITH Y}\\) , when y = lower, u = \\(\\color{yellow}\\text{MIN}\\) and when y = upper, u = \\(\\color{orange}\\text{MAX}\\).\nBy definition of the CDF, the CDF of U is,\n\n\\(\\begin{equation}\\label{a}\\begin{split}F_U(u) &= P(U\\leq u)\\\\&=P(\\color{red}\\text{FUNCTION WITH Y}\\color{black}\\leq u)\\\\&=P(Y\\leq\\color{blue}\\text{FUNCTION WITH u}\\color{black})\\\\&= F_Y(\\color{blue}\\text{FUNCTION WITH u}\\color{black})\\\\&=\\color{purple}\\text{NEW FUNCTION WITH u}\\end{split}\\end{equation}\\)\n\\(F_U(u)=\\begin{cases}0 & u<\\text{MIN}\\\\ \\color{purple}\\text{NEW FUNCTION WITH u}\\color{black} & \\color{yellow}\\text{MIN}\\color{black}\\leq u\\leq \\color{orange}\\text{MAX}\\color{black}\\\\1 & \\color{orange}\\text{MAX}\\color{black}\\leq u\\end{cases}\\)\n\nSince \\(f_U(u)=F'_U(u)\\),\n\n\\(f_U(u)=\\frac{d}{du}(F_U(u))=\\color{green}\\text{DERIVATIVE OF NEW FUNCTION WITH u}\\)\nThe complete pdf of U is,\n\\[f_U(u)=\\begin{cases}\\color{green}\\text{DERIVATIVE OF NEW FUNCTION WITH u}\\color{black} & \\color{yellow}\\text{MIN}\\color{black}<u<\\color{orange}\\text{MAX}\\color{black}\\\\0 & \\text{otherwise}\\end{cases}\\]\n\n\n2. Jacobian (Univariate)\n\nGraph U (Y,u) to verify g is monotone and increasing.\n\\(U=g(Y)\\) where \\(g(y)=U\\). Since g is a monotinc increasing function we can apply the Transformation Method.\nThe inverse of g is \\(h(u)=g^{-1}(y)=\\)🥚 for \\(\\color{yellow}\\text{MIN}\\) \\(\\leq u\\leq\\) \\(\\color{orange}\\text{MAX}\\), and \\(h'(u)=\\) 🐤.\nThe pdf of U is then, \\(f_U(u)=f_Y(h(u))\\cdot |h'(u)|=f_Y(\\)🥚\\()\\cdot|\\)🐤\\(|=\\color{white}\\text{A Solution}\\)\n\nAnd the complete pdf is,\n\\[f_U(u)=\\begin{cases}\\color{white}\\text{A Solution}\\color{black} & \\color{yellow}\\text{MIN}\\color{black}<u<\\color{orange}\\text{MAX}\\color{black}\\\\0 & \\text{otherwise}\\end{cases}\\]\nNote:\n\n\\(y^2\\) is monotonic on \\(0\\leq y\\)\n\n\n\n3. MGF\n\n\nThe mgf of Y is \\(M_Y(t)=\\color{white}\\text{Moment Generating Function of Y}\\color{black}=E_Y(t)=E(e^{ty})\\).\nBy definition of the mgf, the mgf of U is,\n\n\\(\\begin{equation}\\label{b}\\begin{split}M_U(t) &= E(e^{tU})\\\\&=\\text{something with the form of }E(e^{ty})\\\\&=\\color{white}\\text{something similar to known distribution}\\end{split}\\end{equation}\\)\n\nTherefore, mgf U has the form of \\(\\underline{\\text{a known distribution}}\\) with parameters \\(\\color{yellow}\\text{Parameter}\\) and \\(\\color{orange}\\text{Parameter}\\).\n\nNote: (For Bivariate)\n\nLet \\(Y_i\\) be the ith \\(\\underline{\\quad\\quad\\quad}\\) for \\(i=1,2\\).\n\n\n\n4. Jacobian (bivariate)\n\nThe pdf of \\(Y_1\\) is \\(f(y_1)=\\)🌗 and the pdf of \\(Y_2\\) is \\(f(y_2)=\\)🌓.\nSince \\(Y_1\\) and \\(Y_2\\) are independent, their joint pdf is \\(f_{Y_1,Y_2}(y_1,y_2)=f(y_1)f(y_2)=\\)🌗🌓=🌝\nLet \\(U_1=\\)🏃\\(=h_1(y_1,y_2)\\) and \\(U_2=\\)🌎\\(=h_2(y_1,y_2)\\). Then, \\(y_1=...=\\)⛵\\(=h_1^{-1}(u_1,u_2)\\) and \\(y_2=...=\\)🌊\\(=h_2^{-1}(u_1,u_2)\\).\nThen the Jacobian is, \\(J=\\text{det}\\begin{bmatrix}\\frac{\\partial y_1}{\\partial u_1} & \\frac{\\partial y_1}{\\partial u_2}\\\\ \\frac{\\partial y_2}{\\partial u_1} & \\frac{\\partial y_2}{\\partial u_2}\\end{bmatrix}=\\) ⚓\nRecall: det\\(|\\begin{smallmatrix}a & b \\\\ c & d\\end{smallmatrix}|=(ad)-(bc)\\)\nTherefore the joint pdf of \\(U_1\\) and \\(U_2\\) is \\(f_{U_1,U_2}=(u_1,u_2)=f_{Y_1,Y_2}(h_1^{-1}(u_1,u_2),h_2^{-1}(u_1,u_2))\\times|J|=\\)🌜⛵🌊🌛\\(\\times\\)|⚓|\n\n\n\n5. Normal Sample\nFind the probability that the sample of \\(n=\\)🐝 will be within X=🍯 of the population mean. Given \\(\\mu=\\)🌻 and \\(\\sigma^2=\\)🌿\n\nLet \\(\\overline{Y}\\) be the mean of \\(\\underline{\\quad\\text{     }\\quad}\\) of 🐝 \\(\\underline{\\quad\\text{     }\\quad}\\).\nWe want to find \\(P(|\\overline{Y}-\\mu|\\leq X)=P(|\\overline{Y}-\\)🌻\\(|\\leq\\)🍯\\()=P(-\\)🍯\\(\\leq \\overline{Y}-\\)🌻\\(\\leq\\)🍯\\()\\).\nSince the population is normally distributed with mean \\(=\\mu\\) and variance \\(\\frac{\\sigma^2}{\\sqrt{n}}=\\)🌿/\\(\\sqrt{}\\)🐝=🌾.\n\nThen, \\(P(-Z\\leq\\overline{Y}-\\mu\\leq Z)=P(\\)-🍯/🌾\\(\\leq [\\overline{Y}\\)-🌻]/🌾\\(\\leq\\)🍯/🌾)=P(🍄\\(\\leq Z\\leq\\) 🌸)\n\nOn Ti Calculator normalcdf(🍄,🌸,0,1)\n\nNote:\n\nVariance = 4 \\(\\rightarrow\\sqrt{4}=\\sigma^2\\)\nStandard Deviation = 4 \\(\\rightarrow 4=\\sigma^2\\)\n\\(Z=\\frac{\\overline{Y}-\\mu}{\\frac{\\sigma^2}{\\sqrt{n}}}=\\frac{\\sqrt{n}(\\overline{Y}-\\mu)}{\\sigma^2}\\)"
  },
  {
    "objectID": "01_blog/2022_07_25_Tidyverse-P1/index.html",
    "href": "01_blog/2022_07_25_Tidyverse-P1/index.html",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "",
    "text": "This is part 1 of my notes on the tidyverse style guide."
  },
  {
    "objectID": "01_blog/2022_07_25_Tidyverse-P1/index.html#files",
    "href": "01_blog/2022_07_25_Tidyverse-P1/index.html#files",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.1 Files",
    "text": "2.1 Files\n\nnames are meaningful and use letters, numbers, _, and/or -.\navoid special characters in files names.\nif more than 10 files then left pad with a zero (i.e. 00, 01, 02, 03,…)\nrename files instead of attempting 02a, 02b, and so on.\npay attention to capitalization.\nload all packages at once in the beginning of the file.\nuse - and = to break up file into readable chunks.\n\n\n# Load data ---------------------------\n\n# Plot data ---------------------------"
  },
  {
    "objectID": "01_blog/2022_07_25_Tidyverse-P1/index.html#syntax",
    "href": "01_blog/2022_07_25_Tidyverse-P1/index.html#syntax",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.2 Syntax",
    "text": "2.2 Syntax\n\n2.2.1 Object Names\n\nVariables and function names should use lowercase letters, numbers and _ (no camel case).\nBase R uses dots in function and class names (data.frame).\nVariable names should be names and nouns.\nFunction names should be verbs.\nNames should be concise and meaningful.\nAvoid re-using names of common function and variables.\n\n\n\n2.2.2 Spacing\n\nPut a space after a common, never before.\nDo not put spaces inside or outside parentheses for regular functions : mean(x, na.rm = TRUE).\nPut a space before and after () when using if, for, or while.\nPlace a space after () used for function arguments : function(x) {}.\n\n\n# if (debug) {\n#   show(x)\n# }\n\n\nThe embracing operator, {{ }}, should always have inner spaces to help emphasize its special behavior.\nMost infix operators (==, +, -, <-, etc.) should always be surrounded by spaces.\nExceptions being:\n\nThe operators with high precedence: ::, :::, $, @, [, [[, ^, unary -, unary +, and :.\nSingle-sided formulas when the right-hand side is a single identifier: call(!!xyz).\nWhen used in tidy evaluation !! (bang-bang) and !!! (bang-bang-bang) (because have precedence equivalent to unary -/+).\nThe helper operator: package?stats.\n\nadding extra spaces is ok if it improves alignment of = or <-.\n\n\n\n2.2.3 Function Calls\n\nWhen you call a function omit the names of data arguemnts (such as x= because it is used so commonly).\nIf you override the default value of an argument, use the full name.\nAvoid assignments in function calls.\n\nException: function that capture side-effects: output <- capture.output(x <- f()).\n\n\n\n\n2.2.4 Control Flow\n\nFor {} brackets:\n\n{ should be the last character on the line.\nContents should be indented by two spaces.\n} should be the first character on the line.\nElse should be on the same line as } (if used).\n\n{} define the most important hierarchy of R code.\nAlways use && and || inside an if clause and never & and |.\nIf you want to rewrite a simple but lengthy if block:\n\n\nif (x > 10) {\n  message <- \"big\"\n} else {\n  message <- \"small\"\n}\n\nJust write it all on one line:\n\nmessage <- if (x > 10) \"big\" else \"small\"\n\n\nit is okay to drop curly braces for very simple statements that fit on one line as long as they don’t have side-effects.\nFunction calls that affect control flow (like return(), stop() or continue) should always go in their own {} block.\nAvoid implicit type coercion (e.g. from numeric to logical) in if statements:\n\n\n# Good\nif (length(x) > 0) {\n  # do something\n}\n\nNULL\n\n# Bad\nif (length(x)) {\n  # do something\n}\n\nNULL\n\n\n\nAvoid position-based switch() statements.\nEach element should go on its own line.\nProvided a fall-through arrow, unless you have previously validate the input.\n\n\nswitch(x, \n  a = 0,\n  b = 1, \n  c = 2,\n  stop(\"Unknown `x`\", call. = FALSE)\n)\n\n[1] 0\n\n\n\n\n2.2.5 Long Lines\n\nLimit code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font.\n\n\n# # Good\n# do_something_very_complicated(\n#   something = \"that\",\n#   requires = many,\n#   arguments = \"some of which may be long\"\n# )\n# \n# # Bad\n# do_something_very_complicated(\"that\", requires, many, arguments,\n#                               \"some of which may be long\"\n#                               )\n\n\nYou may also place several arguments on the same line if they are closely related to each other, e.g., strings in calls to paste() or stop().\n\n\n\n2.2.6 Semicolons\n\nDon’t put ; at the end of a line, and don’t use ; to put multiple commands on one line.\n\n\n\n2.2.7 Assignment\n\nUse <-, not =, for assignment.\n\n\n\n2.2.8 Data\n\nUse \", not ', for quoting text. The only exception is when the text already contains double quotes and no single quotes.\nPrefer TRUE and FALSE over T and F.\n\n\n\n2.2.9 Comments\n\nEach line of a comment should begin with the comment symbol and a single space.\nIn data analysis code, use comments to record important findings and analysis decisions.\nIf you need comments to explain what your code is doing, consider rewriting your code to be clearer.\nIf you discover that you have more comments than code, consider switching to R Markdown."
  },
  {
    "objectID": "01_blog/2022_07_25_Tidyverse-P1/index.html#functions",
    "href": "01_blog/2022_07_25_Tidyverse-P1/index.html#functions",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.3 Functions",
    "text": "2.3 Functions\n\n2.3.1 Naming\n\nStrive to use verbs for function names.\n\n\n\n2.3.2 Long Lines\n\nPrefer function-indent style to double-indent style when it fits.\n\n\n# Function-indent --------------------------------------\nlong_function_name <- function(a = \"a long argument\",\n                               b = \"another argument\",\n                               c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\n# Double-indent ----------------------------------------\nlong_function_name <- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\n\nIf a function argument can’t fit on a single line, this is a sign you should rework the argument to keep it short and sweet.\n\n\n\n2.3.3 return()\n\nOnly use return() for early returns. Otherwise, rely on R to return the result of the last evaluated expression.\n\n\n# Good\nfind_abs <- function(x) {\n  if (x > 0) {\n    return(x)\n  }\n  x * -1\n}\nadd_two <- function(x, y) {\n  x + y\n}\n\n# Bad\nadd_two <- function(x, y) {\n  return(x + y)\n}\n\n\nReturn statements should always be on their own line because they have important effects on the control flow.\nIf your function is called primarily for its side-effects (like printing, plotting, or saving to disk), it should return the first argument invisibly. This makes it possible to use the function as part of a pipe.\n\n\n\n2.3.4 Comments\n\nIn code, use comments to explain the “why” not the “what” or “how”.\nComments should be in sentence case, and only end with a full stop if they contain at least two sentences.\n\n\n# Good -----------------------------------------\n\n# Objects like data frames are treated as leaves\n\n# Do not use `is.list()`. Objects like data frames \n# must be treated as leaves.\n\n# Bad -----------------------------------------\n\n# objects like data frames are treated as leaves\n\n# Objects like data frames are treated as leaves."
  },
  {
    "objectID": "01_blog/2022_07_25_Tidyverse-P1/index.html#pipes",
    "href": "01_blog/2022_07_25_Tidyverse-P1/index.html#pipes",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.4 Pipes",
    "text": "2.4 Pipes\n\n2.4.1 Introduction\n\nReserve pipes for a sequence of steps applied to one primary object.\nAvoid using pipes when there are meaningful intermediate objects that could be given informative names.\n\n\n\n2.4.2 Withespace\n\n%>% should always have a space before it, and should usually be followed by a new line. After the first step, each line should be indented by two spaces.\n\n\n\n2.4.3 Long Lines\n\nIf the arguments to a function don’t all fit on one line, put each argument on its own line and indent.\n\n\n\n2.4.4 Short Pipes\n\nA one-step pipe can stay on one line, but unless you plan to expand it later on, you should consider rewriting it to a regular function call.\nSometimes it’s useful to include a short pipe as an argument to a function in a longer pipe. Carefully consider whether the code is more readable with a short inline pipe (which doesn’t require a lookup elsewhere) or if it’s better to move the code outside the pipe and give it an evocative name.\n\n\n# Good\n# x %>%\n#   select(a, b, w) %>%\n#   left_join(y %>% select(a, b, v), by = c(\"a\", \"b\"))\n# \n# # Better\n# x_join <- x %>% select(a, b, w)\n# y_join <- y %>% select(a, b, v)\n# left_join(x_join, y_join, by = c(\"a\", \"b\"))\n\n\n\n2.4.5 No Arguments\n\nmagrittr allows you to omit () on functions that don’t have arguments. Avoid this feature.\n\n\n\n2.4.6 Assignment\nThere are three acceptable forms of assignment:\n\nVariable name and assignment on separate lines:\n\n\niris_long <-\n  iris %>%\n  gather(measure, value, -Species) %>%\n  arrange(-value)\n\n\nVariable name and assignment on the same line:\n\n\niris_long <- iris %>%\n  gather(measure, value, -Species) %>%\n  arrange(-value)\n\n\nAssignment at the end of the pipe with ->:\n\n\niris %>%\n  gather(measure, value, -Species) %>%\n  arrange(-value) ->\n  iris_long\n\n\nThe magrittr package provides the %<>% operator as a shortcut for modifying an object in place. Avoid this operator."
  },
  {
    "objectID": "01_blog/2022_07_25_Tidyverse-P1/index.html#ggplot2",
    "href": "01_blog/2022_07_25_Tidyverse-P1/index.html#ggplot2",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.5 ggplot2",
    "text": "2.5 ggplot2\n\n2.5.1 Introduction\n\nStyling suggestions for + used to separate ggplot2 layers are very similar to those for %>% in pipelines.\n\n\n\n2.5.2 Whitespace\n\n+ should always have a space before it, and should be followed by a new line.\nIf you are creating a ggplot off of a dplyr pipeline, there should only be one level of indentation.\n\n\n\n2.5.3 Long lines\n\nIf the arguments to a ggplot2 layer don’t all fit on one line, put each argument on its own line and indent.\nggplot2 allows you to do data manipulation, such as filtering or slicing, within the data argument. Avoid this, and instead do the data manipulation in a pipeline before starting plotting.\n\n\nTo be continued …"
  },
  {
    "objectID": "01_blog/2022_06_27_Predicting-Stock-Prices/index.html",
    "href": "01_blog/2022_06_27_Predicting-Stock-Prices/index.html",
    "title": "Predicting Stock Prices",
    "section": "",
    "text": "1. Background\nDuring the summer of 2021 I took a business fiance class, and learned a lot about stocks and the stock market. Most of my projects centered around MicroVision which is a research and development company creating the newest Lidar technology for autonomous vehicles.\n\n\n2. Set Up\nThis analysis will use three packages:\n\ntidyverse: to clean the data\nquantmod: to get data from yahoo finace\nprophet: to make predictions\n\n\n# Load Libraries \nlibrary(quantmod)\nlibrary(tidyverse)  \nlibrary(prophet) \n\nThere are a few places that quantmod can pull data from, but the default which I will be using is Yahoo Finace. You can specify this with src=\"yahoo\". Use ?getSymbols for more information on this functions parameters.\nNote: If you are having issues with quantmod try re-installing it.\n\nquantmod::getSymbols(\"MVIS\", src=\"yahoo\")\n\n[1] \"MVIS\"\n\n\n\n\n3. quantmod Functions\nRight away we quantmod functions like chartSeries() can be used to look at various plots of MicroVision stocks. Below are a few examples showing various subsets of data from 2019 - 2022. As well as different types such as bar, line, candlesticks, and auto. The last graph also includes addBBands(), addMomentum() and addROC(). Use ? with any of these functions to find out more details on their parameters.\n\nchartSeries(MVIS, type = \"candlesticks\", subset = '2019-09-01::2019-12-31', theme= chartTheme('white'))\n\n\n\nchartSeries(MVIS, type = \"bar\", subset = '2020', theme= chartTheme('white'))\n\n\n\nchartSeries(MVIS, type = \"line\", subset = '2021', theme= chartTheme('white'))\n\n\n\nchartSeries(MVIS, type = \"auto\", subset = '2022', theme= chartTheme('white'))\n\n\n\nchartSeries(MVIS, type = \"auto\", subset = '2022-06', theme= chartTheme('white'))\n\n\n\naddBBands(n=20,sd=2)\n\n\n\naddMomentum(n=1)\n\n\n\naddROC(n=7)\n\n\n\n\n\n\n4. Tidy Data\n\nhead(MVIS)\n\n           MVIS.Open MVIS.High MVIS.Low MVIS.Close MVIS.Volume MVIS.Adjusted\n2007-01-03     25.68     26.40    24.16      24.56      106750         24.56\n2007-01-04     25.76     30.48    25.44      29.92      703538         29.92\n2007-01-05     30.00     31.44    28.16      29.60      333425         29.60\n2007-01-08     31.92     32.64    30.24      31.44      341050         31.44\n2007-01-09     30.96     31.20    29.20      29.36      242725         29.36\n2007-01-10     29.68     30.80    27.76      29.68      169038         29.68\n\n\nYou might notice that the date column doesn’t have a column name, and that is because it is being read as the names of the rows. To change this I will first need to change the data into a data frame, and then change the row names into a column with the function rownames_to_column(). It is then important to assign the date series column the name ds. That is how the prophet package will recognize it, so this will save us the step of renaming this later on.\n\nMVIS <- base::data.frame(MVIS)\nMVIS <- tibble::rownames_to_column(MVIS, \"ds\")\nutils::head(MVIS)\n\n          ds MVIS.Open MVIS.High MVIS.Low MVIS.Close MVIS.Volume MVIS.Adjusted\n1 2007-01-03     25.68     26.40    24.16      24.56      106750         24.56\n2 2007-01-04     25.76     30.48    25.44      29.92      703538         29.92\n3 2007-01-05     30.00     31.44    28.16      29.60      333425         29.60\n4 2007-01-08     31.92     32.64    30.24      31.44      341050         31.44\n5 2007-01-09     30.96     31.20    29.20      29.36      242725         29.36\n6 2007-01-10     29.68     30.80    27.76      29.68      169038         29.68\n\n\nIt will also be important that the ds column is read as date values instead of character value.\n\nMVIS <- MVIS %>% \n  dplyr::mutate(ds = as.Date(ds))\nutils::head(MVIS)\n\n          ds MVIS.Open MVIS.High MVIS.Low MVIS.Close MVIS.Volume MVIS.Adjusted\n1 2007-01-03     25.68     26.40    24.16      24.56      106750         24.56\n2 2007-01-04     25.76     30.48    25.44      29.92      703538         29.92\n3 2007-01-05     30.00     31.44    28.16      29.60      333425         29.60\n4 2007-01-08     31.92     32.64    30.24      31.44      341050         31.44\n5 2007-01-09     30.96     31.20    29.20      29.36      242725         29.36\n6 2007-01-10     29.68     30.80    27.76      29.68      169038         29.68\n\n\nLastly to create a data frame for the prophet package with just the date and closing costs. It is important to rename the closing cost column to y.\n\n# CLosing Data \nMVIS_close <- base::data.frame(ds = MVIS$ds, y = MVIS$MVIS.Close)\nutils::head(MVIS_close)\n\n          ds     y\n1 2007-01-03 24.56\n2 2007-01-04 29.92\n3 2007-01-05 29.60\n4 2007-01-08 31.44\n5 2007-01-09 29.36\n6 2007-01-10 29.68\n\n\n\n\n5. Prophet Functions\nUsing the prophet() function I can create a model of the data. Then I can use make_future_dataframe() to make a predicted model of the next three years.\n\n# call prophet function to fit the model \nModel1 <- prophet::prophet(MVIS_close,\n                           daily.seasonality=TRUE)\nFuture1 <- prophet::make_future_dataframe(Model1,\n                                          periods = 365*3)\nutils::tail(Future1)\n\n             ds\n5134 2026-01-19\n5135 2026-01-20\n5136 2026-01-21\n5137 2026-01-22\n5138 2026-01-23\n5139 2026-01-24\n\n\n\n\n6. Predict Function\nThe predict() function is a stats function that uses various model fitting functions to predict future results.\n\n# Forecast Proper \nForecast1 <- stats::predict(Model1, Future1)\n# Forecast Values \npredict_date <- Forecast1$ds[length(Forecast1$ds)]\npredict_value <- Forecast1$yhat[length(Forecast1$yhat)]\npredict_lower <- Forecast1$yhat_lower[length(Forecast1$yhat_lower)]\npredict_upper <- Forecast1$yhat_upper[length(Forecast1$yhat_upper)]\n\nThis model predicts that on 2026-01-24, the value of MicroVision stock will be about 9.535406. This value is in a range between -11.1586938 and 30.7971739. Note that this range is so large because of the long time period on which it is making the prediction.\n\n\n7. Plot Model\nBelow is an interactive plot that shows the actual values in black, and the predicted values in blue. The grey graph underneath can be adjusted to look at a specific window of time.\n\nprophet::dyplot.prophet(Model1, Forecast1)\n\n\n\n\n\n\n\n8. Plot Componets\nLastly using the prophet_plot_components() function can be used to see yearly, weekly, seasonally, and daily trends.\n\nprophet::prophet_plot_components(Model1, Forecast1)\n\n\n\n\nLooking at daily trends it appears that MicroVision was on the decline after 2010 for some time, but has been trading up since about 2019.\nLooking at the weekly trends, it obvious MicroVision is popular during the Monday - Friday trading week, however looking at the time trends the stock is most popular at the beginning and end of the day.\nThe third graph shows us that MicroVision does not seem to preform well in the first quarter of the year, but picks up around May until it starts to drop off again around November.\n\n\n9. Sources\nEasily Import Financial Data to R with Quantmod\nTechnical Analysis with R - Ch.7 Quantmod\nForecasting Bitcoin Prices Using Prophet in R"
  },
  {
    "objectID": "01_blog/2021_12_20_Transformations-and-Weighting-to-Correct-Model-Inadequacies/index.html",
    "href": "01_blog/2021_12_20_Transformations-and-Weighting-to-Correct-Model-Inadequacies/index.html",
    "title": "Transformations and Weighting to Correct Model Inadequacies",
    "section": "",
    "text": "1. Set Up\nFor this analysis I will be using four packages:\n\nmagrittr: for piping (%>%)\ndplyr: to arrange the data\nMASS: to use the boxcox function\nlatex2exp: to put latex on graphs\n\n\nlibrary(latex2exp) \nlibrary(magrittr) \nlibrary(dplyr) \nlibrary(MASS) \n\n\n\n2. Example 5.1: The Electric Utility Data\nAn electric utility company is interested in developing a model relating peak-hour demand \\((y)\\) to total energy usage during the month \\((x)\\).\nTo start lets look at the data.\n\nex51 <- utils::read.csv(\"data/ex5-1.csv\")\nex51\n\n    X Customer x_.kWh. y_.kW.\n1   1        1     679   0.79\n2   2        2     292   0.44\n3   3        3    1012   0.56\n4   4        4     493   0.79\n5   5        5     582   2.70\n6   6        6    1156   3.64\n7   7        7     997   4.73\n8   8        8    2189   9.50\n9   9        9    1097   5.34\n10 10       10    2078   6.85\n11 11       11    1818   5.84\n12 12       12    1700   5.21\n13 13       13     747   3.25\n14 14       14    2030   4.43\n15 15       15    1643   3.16\n16 16       16     414   0.50\n17 17       17     354   0.17\n18 18       18    1276   1.88\n19 19       19     745   0.77\n20 20       20     435   1.39\n21 21       21     540   0.56\n22 22       22     874   1.56\n23 23       23    1543   5.28\n24 24       24    1029   0.64\n25 25       25     710   4.00\n26 26       26    1434   0.31\n27 27       27     837   4.20\n28 28       28    1748   4.88\n29 29       29    1381   3.48\n30 30       30    1428   7.58\n31 31       31    1255   2.63\n32 32       32    1777   4.99\n33 33       33     370   0.59\n34 34       34    2316   8.19\n35 35       35    1130   4.79\n36 36       36     463   0.51\n37 37       37     770   1.74\n38 38       38     724   4.10\n39 39       39     808   3.94\n40 40       40     790   0.96\n41 41       41     783   3.29\n42 42       42     406   0.44\n43 43       43    1242   3.24\n44 44       44     658   2.14\n45 45       45    1746   5.71\n46 46       46     468   0.64\n47 47       47    1114   1.90\n48 48       48     413   0.51\n49 49       49    1787   8.33\n50 50       50    3560  14.94\n51 51       51    1495   5.11\n52 52       52    2221   3.85\n53 53       53    1526   3.93\n\n\nRight away we can see that for each customer there is a value x_.kWH for Kilowatt hour which corresponds to energy usage during the month, and y_.kW for kilowatt which would then be peak-hour demand. The plot of this is shown below.\n\nbase::plot(ex51$x_.kWh., \n           ex51$y_.kW.,\n           xlab = \"Usage\",\n           ylab = \"Demand\")\n\n\n\n\nScatter diagram of the energy demand (kW) versus energy usage (kWh)\n\n\n\n\nAs a starting point a simple linear regression model is assumed. Lets look at the summary to get an equation for the least-squares fit, and analyze variability.\n\nlm51 <- stats::lm(ex51$y_.kW. ~ ex51$x_.kWh., data = ex51)\nsummary(lm51)\n\n\nCall:\nstats::lm(formula = ex51$y_.kW. ~ ex51$x_.kWh., data = ex51)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1399 -0.8275 -0.1934  1.2376  3.1522 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.8313037  0.4416121  -1.882   0.0655 .  \nex51$x_.kWh.  0.0036828  0.0003339  11.030 4.11e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.577 on 51 degrees of freedom\nMultiple R-squared:  0.7046,    Adjusted R-squared:  0.6988 \nF-statistic: 121.7 on 1 and 51 DF,  p-value: 4.106e-15\n\n\nFrom our summary our we can extrapolate our least-squares fit is: \\(\\hat y=-0.83130+0.00368x\\)\nFor this model \\(R^2=0.7046\\); that is about 70% of the variability in demand is accounted for by the straight-line fit to energy usage. The summary statistics do not reveal any obvious problems with this model.\nBelow this model is plotted with a red line.\n\nbase::plot(ex51$x_.kWh.,\n           ex51$y_.kW.,\n           xlab = \"Usage\",\n           ylab = \"Demand\")\ngraphics::abline(lm51, col = \"red\")\n\n\n\n\nScatter diagram of the energy demand (kW) versus energy usage (kWh) with Simple Linear Model\n\n\n\n\nFrom visual inspection we can see the points on the far left side of the graph are much closer to the best fit line than those in the middle and right side of the graph. We might want to apply a transformation to this model, so lets look at the Studentized Residual also known as r student.\n\nbase::plot(stats::fitted(lm51),\n           stats::rstudent(lm51),\n           ylab=latex2exp::TeX(r'($t_i$)'),\n           xlab=latex2exp::TeX(r'($\\hat{y}_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\nPlot of R-Student vs. fitted values\n\n\n\n\nFrom this graph we can see that the residuals form an outward-opening funnel, indicating that the error variance is increasing as energy consumption increases. A transformation may be helpful in correcting this model inadequacy. To select the form of the transformation, note that the response variable y may be viewed as a “count” of the number of kilowatts used by a customer during a particular hour. The simplest probabilistic model for count data is the Poisson distribution. This suggests regressing \\(y^*=\\sqrt{y}\\) on x as a variance-stabilizing transformation.\n\nex51$ystar <- base::sqrt(ex51$y_.kW.)\nlm51T <- stats::lm(ex51$ystar ~ ex51$x_.kWh., data = ex51)\nbase::summary(lm51T)\n\n\nCall:\nstats::lm(formula = ex51$ystar ~ ex51$x_.kWh., data = ex51)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.39185 -0.30576 -0.03875  0.25378  0.81027 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.822e-01  1.299e-01   4.481 4.22e-05 ***\nex51$x_.kWh. 9.529e-04  9.824e-05   9.699 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.464 on 51 degrees of freedom\nMultiple R-squared:  0.6485,    Adjusted R-squared:  0.6416 \nF-statistic: 94.08 on 1 and 51 DF,  p-value: 3.614e-13\n\n\nThe resulting least-squares fit is: \\(\\hat y^*=0.5822+0.0009529x\\)\n\nbase::plot(stats::fitted(lm51T),\n           stats::rstudent(lm51T),\n           ylab=latex2exp::TeX(r'($t_i$)'),\n           xlab=latex2exp::TeX(r'($\\hat{y}^*_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\nThe impression from examining this plot is that the variance is stable; consequently, we conclude that the transformed model is adequate.\nNote that there is one suspiciously large residual (customer 26) and one customer whose energy usage is somewhat large (customer 50). The effect of these two points on the fit should be studied further before the model is released for use.\n\n\n3. Example 5.2: The Windmill Data\nA research engineer is investigating the use of a windmill to generate electricity. He has collected data on the DC Output from his windmill and the corresponding wind velocity.\n\nex52 <- utils::read.csv(\"data/ex5-2.csv\")\nutils::head(ex52)\n\n  X ObservationNumber_i WindVelocity_xi_mph DCOutput_yi\n1 1                   1                 5.0       1.582\n2 2                   2                 6.0       1.822\n3 3                   3                 3.4       1.057\n4 4                   4                 2.7       0.500\n5 5                   5                10.0       2.236\n6 6                   6                 9.7       2.386\n\n\nThe data is plotted below.\n\nbase::plot(ex52$WindVelocity_xi_mph,\n           ex52$DCOutput_yi,\n           xlab = \"Wind Velocity, X\",\n           ylab = \"DC Output, Y\")\n\n\n\n\nInspection of the scatter diagram indicates that the relationship between DC output \\((y)\\) and wind velocity \\((x)\\) may be nonlinear. However, we initially fit a straight-line model to the data, and look at the summary statistics.\n\nlm52 <- stats::lm(ex52$DCOutput_yi ~ ex52$WindVelocity_xi_mph, data = ex52)\nbase::summary(lm52)\n\n\nCall:\nstats::lm(formula = ex52$DCOutput_yi ~ ex52$WindVelocity_xi_mph, \n    data = ex52)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59869 -0.14099  0.06059  0.17262  0.32184 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               0.13088    0.12599   1.039     0.31    \nex52$WindVelocity_xi_mph  0.24115    0.01905  12.659 7.55e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2361 on 23 degrees of freedom\nMultiple R-squared:  0.8745,    Adjusted R-squared:  0.869 \nF-statistic: 160.3 on 1 and 23 DF,  p-value: 7.546e-12\n\n\nThe summary statistics for this model are \\(R^2=0.8745\\), and \\(F_0=160.26\\) (the P-value is <0.0001), and he regression model is: \\(\\hat y=0.1309+0.2411x\\), shown in red below.\n\nbase::plot(ex52$WindVelocity_xi_mph,\n           ex52$DCOutput_yi,\n           xlab = \"Wind Velocity, x\",\n           ylab = \"DC Output, Y\")\ngraphics::abline(lm52, col = \"red\")\n\n\n\n\nBelow we can extract the fitted and residual values from our linear model, and then arrange them in order of increasing wind speed.\n\nex52$fitted <- stats::fitted(lm52)\nex52$resid <- stats::resid(lm52)\nex52 %>% dplyr::arrange(-dplyr::desc(ex52$WindVelocity_xi_mph)) \n\n    X ObservationNumber_i WindVelocity_xi_mph DCOutput_yi    fitted       resid\n1  25                  25                2.45       0.123 0.7216899 -0.59868986\n2   4                   4                2.70       0.500 0.7819771 -0.28197708\n3  11                  11                2.90       0.653 0.8302069 -0.17720685\n4   8                   8                3.05       0.558 0.8663792 -0.30837918\n5   3                   3                3.40       1.057 0.9507813  0.10621871\n6  16                  16                3.60       1.137 0.9990111  0.13798894\n7  24                  24                3.95       1.144 1.0834132  0.06058683\n8  23                  23                4.10       1.194 1.1195855  0.07441450\n9  13                  13                4.60       1.562 1.2401599  0.32184007\n10  1                   1                5.00       1.582 1.3366195  0.24538052\n11 20                  20                5.45       1.501 1.4451365  0.05586353\n12 14                  14                5.80       1.737 1.5295386  0.20746142\n13  2                   2                6.00       1.822 1.5777683  0.24423165\n14 10                  10                6.20       1.866 1.6259981  0.24000188\n15 12                  12                6.35       1.930 1.6621705  0.26782955\n16 19                  19                7.00       1.800 1.8189172 -0.01891722\n17 15                  15                7.40       2.088 1.9153768  0.17262323\n18 17                  17                7.85       2.179 2.0238938  0.15510624\n19  9                   9                8.15       2.166 2.0962384  0.06976158\n20 18                  18                8.80       2.112 2.2529852 -0.14098518\n21 21                  21                9.10       2.303 2.3253298 -0.02232985\n22  7                   7                9.55       2.294 2.4338468 -0.13984684\n23  6                   6                9.70       2.386 2.4700192 -0.08401917\n24  5                   5               10.00       2.236 2.5423638 -0.30636383\n25 22                  22               10.20       2.310 2.5905936 -0.28059360\n\n\nThe residuals show a distinct pattern, that is, they move systematically from negative to positive and back to negative again as wind speed increases.\n\nbase::plot(stats::fitted(lm52),\n           stats::resid(lm52),\n           ylab=TeX(r'($e_i$)'),\n           xlab=TeX(r'($\\hat{y}_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\nThis residual plot indicates model inadequacy and implies that the linear relationship has not captured all of the information in the wind speed variable. Note that the curvature was apparent in the earlier scatter diagram, but is greatly amplified in the residual plot\nClearly some other model form must be considered. We might initially consider using a quadratic model such as: \\(y=\\beta_0+\\beta_1x+\\beta_2x^2+\\epsilon\\) to account for the curvature. However since the quadratic model will eventually bend downward as wind speed increases, it would not be appropriate for these data. A more reasonable model for windmill data that incorporates an upper asymptote would be: \\(y=\\beta_0+\\beta_1(\\frac{1}{x})+\\epsilon\\).\n\nex52$xstar <- 1/ex52$WindVelocity_xi_mph\nlm52T <- stats::lm(ex52$DCOutput_yi ~ ex52$xstar, data = ex52)\nbase::summary(lm52T)\n\n\nCall:\nstats::lm(formula = ex52$DCOutput_yi ~ ex52$xstar, data = ex52)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20547 -0.04940  0.01100  0.08352  0.12204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.9789     0.0449   66.34   <2e-16 ***\nex52$xstar   -6.9345     0.2064  -33.59   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09417 on 23 degrees of freedom\nMultiple R-squared:   0.98, Adjusted R-squared:  0.9792 \nF-statistic:  1128 on 1 and 23 DF,  p-value: < 2.2e-16\n\n\nThe fitted regression model is \\(\\hat y=2.9789-6.9345x'\\)\nThe summary statistics for this model are \\(R^2=0.98\\), and \\(F_0=1128\\) (the p value is <0.0001).\n\nbase::plot(stats::fitted(lm52T),\n           stats::rstudent(lm52T),\n           ylab=TeX(r'($t_i$)'),\n           xlab=TeX(r'($\\hat{y}_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\nThis plot does not reveal any serious problems.\n\n\n4. Example 5.3: The Electic Utility Data\nWe use the Box-Cox procedure to select a variance-stabilizing transformation. The values of \\(SS_{Res}(\\lambda)\\) for various values are shown in the table.\n\nboxcoxResult = MASS::boxcox(ex51$y_.kW. ~ ex51$x_.kWh., data = ex51, lambda = seq(-2,2,0.125))\n\n\n\n\nThe Box-Cox graph shows most of the data is below the 95% confidence interval.\n\nlambda <- boxcoxResult$x[which.max(boxcoxResult$y)]\nlambda\n\n[1] 0.5454545\n\n\nWhere \\(\\lambda\\approx\\) 0.5454545 could be used as an appropriate exponent to use to transform the data into a “normal shape.”"
  },
  {
    "objectID": "01_blog/2022_10_31_P-logically-equivalent-neg-neg-P/index.html",
    "href": "01_blog/2022_10_31_P-logically-equivalent-neg-neg-P/index.html",
    "title": "Prove P is Logically Equivalent to the Negation of the Negation of P",
    "section": "",
    "text": "Solution 1\nConsider the truth table for P, \\(\\neg P\\), and \\(\\neg (\\neg P)\\), as shown below in Figure 1:\n\n\nFigure 1: Truth Table\n\n\nP\n\\(\\neg P\\)\n\\(\\neg (\\neg P)\\)\n\n\n\n\nT\nF\nT\n\n\nF\nT\nF\n\n\n\n\nSince the truth values for P and \\(\\neg (\\neg P)\\) are the same then P and \\(\\neg (\\neg P)\\) are logically equivalent.\n\n\nSolution 2\nSuppose by way of contradiction (BWOC) that P and \\(\\neg (\\neg P)\\) are not logically equivalent.\nLet P be true and then \\(\\neg (\\neg P)\\) would be false.\nIf P is true then \\(\\neg P\\) would be false, but \\(\\neg P\\) and \\(\\neg (\\neg P)\\) cannot both be false. Therefore BWOC \\(\\neg (\\neg P)\\equiv P\\).\n\\(\\square\\)"
  },
  {
    "objectID": "01_blog/2021_08_30_Ch.3-R-for-ds/index.html",
    "href": "01_blog/2021_08_30_Ch.3-R-for-ds/index.html",
    "title": "R for Data Science - Ch.3: Data Visualisations",
    "section": "",
    "text": "1. Set Up\nThis first chunk will remove warning messages from all chunks in this file. To hide this chunk use include=FALSE within the {} brackets.\n\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \n\nThis second chunk calls two packages:\n\ntidyverse: to tidy data and create visuals with ggplot2.\ngridExtra: to arrange data in a grid\n\n\nlibrary(tidyverse)\nlibrary(gridExtra)\n\nThis chapter analyzes the mpg data so I’m using the head() function from utils to view the first five rows in the mpg data set.\n\nutils::head(mpg, 5)\n\n# A tibble: 5 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n\n\n\n\n2. Visuals\n\\(\\underline{\\text{Question 1}}\\): Do cars with big engines use more fuel than cars with small engines?\nTo answer this question I will focus on two columns:\ndispl : a cars engine size in litres\nhwy : a car’s fuel efficiency on the highway in mpg.\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy))\n\n\n\n\nNote:\n\nThere is a negative relation between engine size and fuel efficiency.\nThe mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes.\n\n\n\n3. 3.2.4 Exercises\n\nThe following code chunk creates an empty square.\n\n\nggplot2::ggplot(data = mpg)\n\n\n\n\n\nThe mpg data set has 234 rows and 11 columns.\nThe drv variable is the type of drive the car has such as f = front wheel, r = rear wheel, and 4 = 4 wheel drive.\nThe following plot shows hwy vs. cyl.\n\n\nggplot2::ggplot(mpg) +\n  ggplot2::geom_point(ggplot2::aes(x = cyl, y = hwy))\n\n\n\n\nNote: This isn’t very useful because it is obvious that as the number of cylinders increases the miles per gallon decreases.\n\nThe following plot shows class vs. drv.\n\n\nggplot2::ggplot(mpg) +\n  ggplot2::geom_point(ggplot2::aes(x = drv, y = class))\n\n\n\n\nNote: This plot isn’t useful because there are no obvious trends. Categorical variables usually have a small number of values they are limited to, so it only seems like there are 12 observed values.\n\n\n4. Aesthetics\nWithin the aes() function when specifying that color is equal to a column variable then ggplot will add a color key to these variables, as shown below.\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, color = class))\n\n\n\n\nWhen defining color outside aes() then color is equal to a specific color (such as red or blue), and ggplot will make all points that one color, as shown below.\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\nsize:\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, size = class))\n\n\n\n\n(Warning: using size for a discrete variable is not advised.)\nalpha: (transparency)\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, alpha = class))\n\n\n\n\nshape:\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, shape = class))\n\n\n\n\n\n\n\nshapes built into R\n\n\n\n\n5. 3.3.1 Exercises\n\nThe following code is incorrect because color is inside aes(), which is labeling all the points as “blue”.\n\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\nCategorical : manufacturer, model name, trans, drv, fl, and class  Continuous : displ, cty, year of manufacture, number of cylinders, and hwy\n\n\nNotice in the printed data frame the categorical variables are usually character  values, where continuous variables are numeric values such as  or .\n\n\nWhen mapping a continuous variable to an aes() such as color then then there the key also becomes continuos as shown below.\n\n\n# Categorical\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = manufacturer, color = trans))\n\n\n\n# Continuous\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = manufacturer, color = hwy))\n\n\n\n\n\nWhen mapping the same variable to multiple aesthetics then multiple keys are added as shown below.\n\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, size = hwy, color = displ))\n\n\n\n\n\nStroke adjusts the thickness of the boarder (for shapes 21-25) as shown below.\n\n\nggplot2::ggplot(mtcars, ggplot2::aes(wt, mpg)) +\n  ggplot2::geom_point(shape = 21, colour = \"black\", fill = \"pink\", size = 5, stroke = 5)\n\n\n\n\n\nWhen defining something like color to be displ < 5, it sets up a true or false argument for this, and applies one color (blue) to true values less than 5 and red for false values greater than 5.\n\n\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, color = displ < 5))\n\n\n\n\n\n\n6. Facets\nfacet_wrap() should be used for discrete values as shown below:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\nTo facet on a combination of variables use facet_grid() as shown below:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ cyl)\n\n\n\n\nUse + facet_grid(.~cyl) to not facet rows.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(.~ cyl)\n\n\n\n\n\n\n7. 3.5.1 Exercises\n\nWhen you facet a continuous variable you make A LOT of graphs.\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ hwy)\n\n\n\n\n\nThe empty cells in the facet_grid(drv ~ cyl) plot above are showing the empty points in the graph below. For example cars with four wheel drive only have an even number of cylinders so the plot of 4 wheel drive with 5 cylinders is empty because it does not exist.\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))\n\n\n\n\n\nOne of the below plots is shown in rows and the other in columns. The period says not to facet the rows or the columns.\n\n\n# rows\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n#columns\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\n\n\nThe advantages of facet wrap allow for data with various classes or types to be analyzed by such. Additionally it’s difficult for humans to visualize a large amount of color so it is easier to digest the variety of date spread out. The disadvantage of this could be that spreading the data out would make it difficult to compare observations between different categories.\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\nnrow and ncol define the number of rows and columns in the facet wrap.There is also scales, shrink, labeler, as.table, switch, drop, dir, and stip.position. Facet grid doesn’t have these because it is specified in the function instead.\nVariables with more unique levels should be in columns when using facet_grid() because there is more space for columns if the plot is laid out horizontally.\n\n\n\n8. Geometric Objects\nThe side by side graphs below show the same data. The left graph uses the geometric object geom_point() which shows all the points, and the right graphs uses geom_smooth() which creates a best fit line with the data’s standard error without all the data points.\n\n# left graph: geom_point()\na <- ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n# right graph: geom_smooth()\nb <- ggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy))\n# both together\ngrid.arrange(a,b, nrow = 1)\n\n\n\n\nFor different line “shapes” geom_smooth() can be used with different linetypes within aes() as shown below.\n\nggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))\n\n\n\n\nFor the following geoms, you can set the group aesthetic to a categorical variable to draw multiple objects.\n\nc <- ggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n              \nd <- ggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))\n    \ne <- ggplot(data = mpg) +\n  geom_smooth(\n    mapping = aes(x = displ, y = hwy, color = drv),\n    show.legend = FALSE)\ngrid.arrange(c,d,e, nrow = 1)\n\n\n\n\nBelow multiple geometric objects are added to one plot.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n\n\n\n\nDefining the mapping aes() helps reduce repetion, as shown below.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nGlobal Mapping\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n\n\n\n\nSubcompact (subset) mapping\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth(data = filter(mpg, class == \"subcompact\"), se = FALSE)\n\n\n\n\n\n\n9. 3.6.1 Exercises\n\nline chart: geom_line()  boxplot: geom_boxplot()  histogram: geom_histogram()  area chart: geom_area()\nPrediction: the below code will show the various points and lines for drv without any standard error.\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n\n\n\n\n\nshow.legend = FALSE hides the legend box, and was used earlier in this chapter because it changes the size of the graphs, which would make it more difficult to compare to the other graphs."
  },
  {
    "objectID": "01_blog/2021_09_27_Generative-Art-Jasmines/index.html",
    "href": "01_blog/2021_09_27_Generative-Art-Jasmines/index.html",
    "title": "Generative Art with Jasmines",
    "section": "",
    "text": "1. Set-Up\nI will be using jasmines to create art, and dplyr to pipe the code.\n\nlibrary(jasmines)\nlibrary(dplyr) \n\n\n\n2. Randi\nWhen playing around with this package, I initially had something less fluid and full of right angles, but wanted to show more movement in the design. I have a dance background and aside from fluid movement we also focused a lot on circles and rotation. Another reason I like this design is because it reminds me of a flower. I have seven tattoos, two of which are flowers. The two colors I chose are salmon and rosewood. I enjoy different shades of pink, and colors like salmon, and rosewood feel like a more sophisticated pink to me.\n\nuse_seed(5) %>%\n  entity_circle(grain = 1000, size = 10) %>%\n  unfold_warp(iterations = 100) %>%\n  style_ribbon(\n    color = \"#9E4244\",\n    background = \"#FDAB9F\")\n\n\n\n\n\n\n3. Unfolding Circle\n\nuse_seed(1) %>%\n  entity_circle(grain = 1000, size = 4) %>%\n  unfold_warp(iterations = 100) %>%\n  style_ribbon(\n    palette=\"base\",\n    colour = \"ind\",\n    background = \"mistyrose\")\n\n\n\n\n\n\n4. Typophobia\n\nscene_discs(\n  rings = 13, \n  points = 500, \n  size = 5\n  ) %>%\n  mutate(ind = 1:n()\n         ) %>%\n  unfold_warp(\n    iterations = 10,\n    scale = .5, \n    output = \"layer\" \n  ) %>%\n  unfold_tempest(\n    iterations = 10,\n    scale = .01\n  ) %>%\n  style_ribbon(\n    color = \"#48AAAD\",\n    colour = \"ind\",\n    alpha = c(.4,.1),\n    background = \"#016064\" \n  ) \n\n\n\n\n\n\n5. Snake Charmer\n\nuse_seed(4) %>%\n  entity_circle(grain = 10000) %>%\n  unfold_tempest(iterations = 13) %>%\n  style_ribbon(background = \"oldlace\")"
  },
  {
    "objectID": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html",
    "href": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "",
    "text": "Ted Landeras led this event for Portland R user Group where we watched Rich Lannone|| Making Beautiful Tables with {gt}|| RStudio as a group on Youtube, and then went through some other examples."
  },
  {
    "objectID": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-1",
    "href": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-1",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 1",
    "text": "Example 1\n\nhead(rock)\n\n  area    peri     shape perm\n1 4990 2791.90 0.0903296  6.3\n2 7002 3892.60 0.1486220  6.3\n3 7558 3930.66 0.1833120  6.3\n4 7352 3869.32 0.1170630  6.3\n5 7943 3948.54 0.1224170 17.1\n6 7979 4010.15 0.1670450 17.1\n\n\n\nrock %>% # Get 'rock' data\n  head(5) %>% # First 5 lines only\n  gt() # Make a table, it just works.\n\n\n\n\n\n  \n  \n    \n      area\n      peri\n      shape\n      perm\n    \n  \n  \n    4990\n2791.90\n0.0903296\n6.3\n    7002\n3892.60\n0.1486220\n6.3\n    7558\n3930.66\n0.1833120\n6.3\n    7352\n3869.32\n0.1170630\n6.3\n    7943\n3948.54\n0.1224170\n17.1"
  },
  {
    "objectID": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-2",
    "href": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-2",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 2",
    "text": "Example 2\n\nhead(BOD)\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n\n\nBOD %>% # Get the data...\ngt() %>% # use 'gt' to make an awesome table...\n  tab_header( \n    title = \"BOD Table Woooooo!\", # ...with this title\n    subtitle = \"Hooray gt!\") %>% # and this subtitle\n  fmt_number( # A column (numeric data)\n    columns = vars(Time), # What column variable? BOD$Time\n    decimals = 2 # With two decimal places\n    ) %>% \n  fmt_number( # Another column (also numeric data)\n    columns = vars(demand), # What column variable? BOD$demand\n    decimals = 1 # I want this column to have one decimal place\n  ) %>% \n  cols_label(Time = \"Time (hours)\", demand = \"Demand (mg/L)\") # Update labels\n\n\n\n\n\n  \n    \n      BOD Table Woooooo!\n    \n    \n      Hooray gt!\n    \n  \n  \n    \n      Time (hours)\n      Demand (mg/L)\n    \n  \n  \n    1.00\n8.3\n    2.00\n10.3\n    3.00\n19.0\n    4.00\n16.0\n    5.00\n15.6\n    7.00\n19.8"
  },
  {
    "objectID": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-3",
    "href": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-3",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 3",
    "text": "Example 3\n\nBOD %>% # Get the data...\ngt() %>% # use 'gt' to make an awesome table...\n  tab_header( \n    title = \"BOD Table Woooooo!\", # ...with this title\n    subtitle = \"Hooray gt!\") %>% # and this subtitle\n  fmt_number( # A column (numeric data)\n    columns = vars(Time), # What column variable? BOD$Time\n    decimals = 2 # With two decimal places\n    ) %>% \n  fmt_number( # Another column (also numeric data)\n    columns = vars(demand), # What column variable? BOD$demand\n    decimals = 1 # I want this column to have one decimal place\n  ) %>% \n  cols_label(Time = \"Time (hours)\", demand = \"Demand (mg/L)\") # Update labels\n\n\n\n\n\n  \n    \n      BOD Table Woooooo!\n    \n    \n      Hooray gt!\n    \n  \n  \n    \n      Time (hours)\n      Demand (mg/L)\n    \n  \n  \n    1.00\n8.3\n    2.00\n10.3\n    3.00\n19.0\n    4.00\n16.0\n    5.00\n15.6\n    7.00\n19.8"
  },
  {
    "objectID": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-4",
    "href": "01_blog/2022_04_25_PDX-R-Aggregate-Making-Tables-with-gt/index.html#example-4",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 4",
    "text": "Example 4\n\ntooth_length <- ToothGrowth %>% \n  group_by(supp, dose) %>% \n  summarize(\n    mean_len = mean(len)\n  ) %>% \n  as_tibble() \n\n# A gt table: \ntooth_length %>% # Take tooth_length\n  gt() %>% # Make a gt table with it\n  tab_header(\n    title = \"A title just like that\", # Add a title\n    subtitle = \"(with something below it!)\" # And a subtitle\n  ) %>%\n  fmt_passthrough( # Not sure about this but it works...\n    columns = vars(supp) # First column: supp (character)\n  ) %>% \n  fmt_number(\n    columns = vars(mean_len), # Second column: mean_len (numeric)\n    decimals = 2 # With 4 decimal places\n  ) %>% \n  fmt_number(\n    columns = vars(dose), # Third column: dose (numeric)\n    decimals = 2 # With 2 decimal places\n  ) %>% \n  data_color( # Update cell colors...\n    columns = vars(supp), # ...for supp column!\n    colors = scales::col_factor( # <- bc it's a factor\n      palette = c(\n        \"green\",\"cyan\"), # Two factor levels, two colors\n      domain = c(\"OJ\",\"VC\")# Levels\n  )\n  ) %>% \n  data_color( # Update cell colors...\n    columns = vars(dose), # ...for dose column \n    colors = scales::col_numeric( # <- bc it's numeric\n      palette = c(\n        \"yellow\",\"orange\"), # A color scheme (gradient)\n      domain = c(0.5,2) # Column scale endpoints\n  )\n  ) %>% \n  data_color( # Update cell colors...\n    columns = vars(mean_len), # ...for mean_len column\n    colors = scales::col_numeric(\n      palette = c(\n        \"red\", \"purple\"), # Overboard colors! \n      domain = c(7,27) # Column scale endpoints\n  )\n  ) %>% \n  cols_label(supp = \"Supplement\", dose = \"Dosage (mg/d)\", mean_len = \"Mean Tooth Length\") %>% # Make the column headers\n  tab_footnote(\n    footnote = \"Baby footnote test\", # This is the footnote text\n    locations = cells_column_labels(\n      columns = vars(supp) # Associated with column 'supp'\n      )\n    ) %>% \n    tab_footnote(\n    footnote = \"A second footnote\", # Another line of footnote text\n    locations = cells_column_labels( \n      columns = vars(dose) # Associated with column 'dose'\n      )\n    )\n\n\n\n\n\n  \n    \n      A title just like that\n    \n    \n      (with something below it!)\n    \n  \n  \n    \n      Supplement1\n      Dosage (mg/d)2\n      Mean Tooth Length\n    \n  \n  \n    OJ\n0.50\n13.23\n    OJ\n1.00\n22.70\n    OJ\n2.00\n26.06\n    VC\n0.50\n7.98\n    VC\n1.00\n16.77\n    VC\n2.00\n26.14\n  \n  \n  \n    \n      1 Baby footnote test\n    \n    \n      2 A second footnote"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "",
    "text": "Alicia Johnson led this R User Connect Philadelphia to discus the book she co-authored, Bayes Rules!."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#alicia-johnson",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#alicia-johnson",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "1.1. Alicia Johnson",
    "text": "1.1. Alicia Johnson\n\nStatistics Professor.\nAuthor of Bayes Rules!."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#materials",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#materials",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "1.2. Materials",
    "text": "1.2. Materials\nSlides\nGithub Repository"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#what-does-pheads0.5-mean",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#what-does-pheads0.5-mean",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.1. What does P(heads)=0.5 mean?",
    "text": "2.1. What does P(heads)=0.5 mean?\n\nIf I flip this coin over and over, roughly 50% will be heads.\nHeads and Tails are equally plausible.\nBoth a and b make sense.\n\n\nScores: a = 1, b = 3, c = 2\nMajority Responses: C\n\n\nFrequentist Philosophy\n\nlong run outcome\n\nBayesian Philosophy\n\nrelative probability of events\n\nPragmatic Philosophy\n\nboth interpretations make sense"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#what-does-pcandidate-a-wins-0.8-mean",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#what-does-pcandidate-a-wins-0.8-mean",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.2 What does P(candidate A wins) = 0.8 mean?",
    "text": "2.2 What does P(candidate A wins) = 0.8 mean?\n\nIf we observe this election over and over, candidate A will win roughly 80% of the time.\nCandidate A is much more likely to win than to lose (4 times more likely).\nThe pollster’s calculation is wrong.\n\n\nMajority Response: B\nScores: a = 1, b = 3, c = 1\n\n\nFreq.\nBayes\nFreq.\n\nthe event cannot be repeated over and over again"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#alicia-claims-she-can-predict-the-outcome-of-a-coin-flip.-mine-claims-she-can-distinguish-between-a-crown-burger-and-a-vegan-alternative.-both-succeed-in-10-out-of-10-trials.-what-do-you-conclude",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#alicia-claims-she-can-predict-the-outcome-of-a-coin-flip.-mine-claims-she-can-distinguish-between-a-crown-burger-and-a-vegan-alternative.-both-succeed-in-10-out-of-10-trials.-what-do-you-conclude",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.3 Alicia claims she can predict the outcome of a coin flip. Mine claims she can distinguish between a Crown Burger and a Vegan Alternative. Both succeed in 10 out of 10 trials. What do you conclude?",
    "text": "2.3 Alicia claims she can predict the outcome of a coin flip. Mine claims she can distinguish between a Crown Burger and a Vegan Alternative. Both succeed in 10 out of 10 trials. What do you conclude?\n\nYou’re still more confident in Mine’s claim than Alicia’s claim.\nThe evidence supporting Mine’s claim.\n\n\nScore: a = 3, b = 1\n\n\nBayes\nFreq."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#youve-tested-positive-for-a-very-rare-genetic-trait.-if-you-only-get-to-ask-the-doctor-one-question-which-would-it-be",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#youve-tested-positive-for-a-very-rare-genetic-trait.-if-you-only-get-to-ask-the-doctor-one-question-which-would-it-be",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.4 You’ve tested positive for a very rare genetic trait. If you only get to ask the doctor one question, which would it be?",
    "text": "2.4 You’ve tested positive for a very rare genetic trait. If you only get to ask the doctor one question, which would it be?\n\nP(rare trait|+)\nP(+| no rare trait)\n\n\nScore: a = 3, b = 1\n\n\nBayes\n\nasking about uncertainty of hypothesis given certainty of the data\nmore natural question to ask\n\nFreq. = p-value\n\nhard to wrap minds around\nasking about uncertainty in data\nless natural question to ask (since data is certain)"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#goals",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#goals",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.1 Goals",
    "text": "3.1 Goals\n\nLearn to think like Bayesians.\nApply Bayesian thinking in a regression setting."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#set-up",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#set-up",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.2 Set Up",
    "text": "3.2 Set Up\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(bayesrules)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#background",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#background",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.3 Background",
    "text": "3.3 Background\nLet \\(\\pi\\) (“pi”) be the proportion of U.S. adults that believe that climate change is real and caused by people. Thus \\(\\pi\\) is some value between 0 and 1."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-1-specify-a-prior-model",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-1-specify-a-prior-model",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.4 Exercise 1: Specify a Prior Model",
    "text": "3.4 Exercise 1: Specify a Prior Model\nThe first step in learning about \\(\\pi\\) is to specify a prior model for \\(\\pi\\) (i.e. prior to collecting any data). Suppose your friend specifies their understanding of \\(\\pi\\) through the “Beta(2, 20)” model. Plot this Beta model and discuss what it tells you about your friend’s prior understanding. For example:\n\nWhat do they think is the most likely value of \\(\\pi\\)?\n\nWhat range of \\(\\pi\\) values do they think are plausible?\n\n\nplot_beta(alpha = 2, beta = 20)\n\n\n\n\n\n\n\n\nNotes:\n\nproportion between 0 and 1 (not \\(-\\infty\\) to \\(+\\infty\\))\nthis model, beta-2-20, is right skewed\n\n\n3.4.1 What is your friend saying is the most likely value of pi?\nAbout .12 or 12 % or people believe in climate change.\n\nspike of model is best estimate\nlooking at range the prior model drops off above .25, so you friend believes under 25% of people believe in climate change."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#check-out-some-data",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#check-out-some-data",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.5 Check Out Some Data",
    "text": "3.5 Check Out Some Data\nThe second step in learning about \\(\\pi\\), the proportion of U.S. adults that believe that climate change is real and caused by people, is to collect data. Your friend surveys 10 people and 6 believe that climate change is real and caused by people. The likelihood function of \\(\\pi\\) plots the chance of getting this 6-out-of-10 survey result under different possible \\(\\pi\\) values. Based on this plot:\n\nplot_binomial_likelihood(y = 6, n = 10)\n\n\n\n\n\n\n\n\nNotes:\n\nThe next step after creating a model (beta-2-20) we collect data.\nThis plot is showing us what the chance is that we got these survey results under different possible pie values.\n\n\n3.5.1 With what values of \\(\\pi\\) are the 6-out-of-10 results most consistent?\nApprox. 60% of people believe in climate change. Shown by our graph spiking at that value.\n\n\n3.5.2 For what values of \\(\\pi\\) would these 6-out-of-10 results be unlikely?\nOur data would not be very likely to happen for values below .25 and above .9."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#exercist-3-build-the-posterior-model",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#exercist-3-build-the-posterior-model",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.6 Exercist 3: Build the Posterior Model",
    "text": "3.6 Exercist 3: Build the Posterior Model\nIn a Bayesian analysis of \\(\\pi\\), we build a posterior model of \\(\\pi\\) by combining the prior model of \\(\\pi\\) with the data (represented through the likelihood function). Plot all 3 components below. Summarize your observations:\n\nplot_beta_binomial(alpha = 2, beta = 20, y = 6, n = 10)\n\n\n\n\n\n\n\n\nNotes:\n\nDepends on a lot of factors\n\n\n3.6.1 What’s your friend’s posterior understanding of \\(\\pi\\)?\nMy friend’s prior understanding of \\(\\pi\\) is not as low as what it was before, but also not as high as what is suggested in the data.\n\n\n3.6.2 How does their posterior understanding compare to their prior and likelihood? Thus how does their posterior balance the prior and data?\nTheir posterior understanding is higher than their prior knowledge and likelihood. The density of their posterior knowledge is lower than their previous density."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-4-another-friend",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-4-another-friend",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.7 Exercise 4: Another Friend",
    "text": "3.7 Exercise 4: Another Friend\nConsider another friend that saw the same 6-out-of-10 polling data but started with a Beta(1, 1) prior model for \\(\\pi\\):\n\nplot_beta(alpha = 1, beta = 1)\n\n\n\n\n\n\n\n\n\n3.7.1 Describe the new friend’s understanding of \\(\\pi\\). Compared to the first friend, are they more or less sure about \\(\\pi\\)?\nThis is a uniform distribution which maybe indicates the friend thinks everyone believes in Climate Change.\n\n\n3.7.2 Do you think the new friend will have a different posterior model than the first friend? If so, how do you think it will compare?\nYes, I think their posterior model will be higher than the first friend.\nTest your intuition. Use plot_beta_binomial() to explore your new friend’s posterior model of \\(\\pi\\).\n\nplot_beta_binomial(alpha = 1, beta = 1, y = 6, n = 10)\n\n\n\n\n\n\n\n\nNotes:\n\nThis is a shoulder shrug, uncertain prior model. It could really be anything."
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-5-more-data",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-5-more-data",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.8 Exercise 5: More Data",
    "text": "3.8 Exercise 5: More Data\nTest your intuition. Use plot_beta_binomial() to explore your new friend’s posterior model of \\(\\pi\\).\n\ndata(\"pulse_of_the_nation\")\npulse_of_the_nation %>% \n  count(climate_change)\n\n# A tibble: 3 × 2\n  climate_change                    n\n  <fct>                         <int>\n1 Not Real At All                 150\n2 Real and Caused by People       655\n3 Real but not Caused by People   195\n\n\n\n3.8.1 How do you think the additional data will impact your first friend’s posterior understanding of \\(\\pi\\)? What about the second friend’s?\nI think the first friends understanding of \\(\\pi\\) would increase will increase, while the second friends understanding will decrease.\n\n\n3.8.2 Upon seeing the 1000-person survey results, do you think your two friends’ posterior understandings of \\(\\pi\\) will disagree a lot or a little?\nI think the two friends’ posterior understanding will disagree a little.\nTest your intuition. Use plot_beta_binomial() to explore both friends’ posterior models of \\(\\pi\\).\n\n# first friend \nplot_beta_binomial(alpha = 2, beta = 20, y = 655, n = 1000)\n\n\n\n\n\n\n\n# second friend \nplot_beta_binomial(alpha = 1, beta = 1, y = 655, n = 1000)"
  },
  {
    "objectID": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-6-your-turn",
    "href": "01_blog/2022_08_22_Bayes-Rules/index.html#exercise-6-your-turn",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.9 Exercise 6: Your Turn",
    "text": "3.9 Exercise 6: Your Turn\nLet \\(\\pi\\) be the proportion of U.S. adults that believe in ghosts.\n\nUse plot_beta() to tune your prior model of \\(\\pi\\). To this end, think about what values of \\(\\pi\\) you think are most likely and how sure you are.\n\nNote:\n\nalpha and beta must be positive.\nThe prior means falls at alpha/(alpha + beta). Thus when alpha is smaller than beta, the prior mode falls below 0.5.\nIn general, the smaller the alpha and beta, the more variable / less certain the prior.\n\n\nbayesrules::plot_beta(alpha = 20, beta = 10)\n\n\n\n\n\n\n\n\n\nCollect some data. How many of the 1000 pulse_of_the_nation respondents believe in ghosts?\n\n\npulse_of_the_nation %>% \n  count(ghosts)\n\n# A tibble: 2 × 2\n  ghosts     n\n  <fct>  <int>\n1 No       621\n2 Yes      379\n\n\n\nUse plot_beta_binomial() to visualize your prior, data, and posterior.\n\n\nbayesrules::plot_beta_binomial(alpha = 20, beta = 10, y = 378, n = 1000)\n\n\n\n\n\n\n\n\nCheck out the Github Repository for Part 2: Apply Bayesian thinking to a regression model."
  },
  {
    "objectID": "01_blog/2022_05_30_Final-Prep-Geometry/index.html",
    "href": "01_blog/2022_05_30_Final-Prep-Geometry/index.html",
    "title": "Final Prep. Modern College Geometry",
    "section": "",
    "text": "1. Euclidean Geometry\n\nEuclid’s 5th Axiom\nTriangle Congruence: SAS, ASA, SSS, AAS\nAngles and Parallel Lines\n\nvertical angels are congruent\ncorresponding angles are congruent\nalternate interior angles are congruent\nsupplementary angles add to \\(180^\\circ\\)\n\n\nQuadrilaterals: 4 sided figure in the plane, where the edges are straight lines.\nParallelogram: Both pairs of opposite sides are parallel.\nTrapezoid: At least 1 pair of opposite sides are parallel.\nRhombus: Parallelogram, all sides are same length.\nRectangle: Parallelogram whose internal angles are all right angles.\nSquare: Rectangle whose sides are all equal length.\nParallelogram Theorem: Let ABCD be a parallelogram. Then the following are equivalent:\n\nABCD is a parallelogram. (opposite sides are parallel)\n\\(\\angle DAB \\cong \\angle BCD\\) and \\(\\angle ABC\\cong \\angle CDA\\) (angles that are across from each other are congruent)\n\\(AB=CD\\) and \\(BC=DA\\) (opposite sides have equal measure)\n\\(\\overline{AC}\\) and \\(\\overline{BD}\\) bisect each other. (diagnals bisect each other)\n\nAxioms of Area:\n\nTo every polygonal region (space enclosed by straight lines in the plane) there corresponds a unique positive number called \\(\\underline{area}\\).\nIf 2 tirangles are congruent their areas are equal.\nIf \\(R=R_1\\cup R_2\\) and \\(R_1\\cap R_2\\) is a finite number of segments or points, then the area of \\(R\\) is the sum of the areas of \\(R_1+R_2\\).\nThe area of a rectangle is its base times its height.\n\n\n\n2. Similarity\nDilation: shrink or expand by a scaling factor, k, from a center point, P.\nSimilarity: 2 figures are similar if one can be superimposed on the other by a dilation and a sequence of isometries.\n\nAA, SAS\n\n\n\n3. Circles\nCircle: center, radius\nPoints on circle have distance r from the center point O.\nUnit Circle: radius length 1.\nArc: a connected subset of the points on circle.\nChord: line segment connecting 2 points on circle.\nCentral Angle: vertex is center of circle, rays intersect in 2 different points. (pie slice)\nInscribed Angles vertex is on circle, rays intersect circle in 2 points.\nInscribed: verities on circle.\nFor an inscribed square, all four points of a square are on the circle.\nTangent Line: Line that intersects circle at only 1 point.\nRadian: measure of the central angle in a unit circle with arc length of 1.\nInscribed Angle Theorem: An inscribed angle is half of a centeral angle that subtends the same arc.\nCorollary: Any two inscribed angles have the same arc on the circle are congruent.\nPower of the Point Theorem 1: If \\(\\overline{AB}\\) and \\(\\overline{CD}\\) are chords of circle intersecting in x inside a circle. Then \\(Ax\\cdot xB=Cx\\cdot xD\\)\nPower of the Point Theorem 2: LEt P be a point outside a given circle. Suppose we draw two rays from the point P: one ray intersects the circle at points A and B (in that order), and the other intersects the circle at the points C and D (in that order). Then \\(PA\\cdot PB=PC\\cdot PD\\)\n\n\n4. Isometries and Symmetries\nThe set of isometries with composition is a group:\n\nClosure\nAssociativity\nIdentity\nInverses\n\nSymmetry A symmetry is an isometry that sends a geometric figure to itself.\n\n6 symmetries of an equilateral triangle\n8 symmetries of a square\n2n symmetries of a regular polygon\n\n\n\n5. Taxicab Geometry\nEuclidean Distance: \\(d_E(A,B)=\\sqrt{(x_B-x_A)^2+(y_B-y_A)^2}\\)\n\n\\(\\pi\\approx 3.14\\)\n\nTaxicab Distance: \\(d_T=|x_B-x_A|+|y_B-y_A|\\)\n\n\\(\\pi = 4\\)\n\nIsometries for taxicab (traingles):\n\ntranslations\nrotations by \\(90^\\circ k\\) where k is an integer\ncombinations\n\n\n\n6. Spherical Geometry\nNo parallel may be drawn through a point not on a given line.\nEquation of a Sphere: \\(S^2=\\{(x,y,z)\\in\\mathbb{R}^3|x^2+y^2+z^2=\\rho^2\\}\\)\n\ngreat circles are straight lines (equator and longitudes)\n\nDistance of a Sphere: \\(d_s(A,B)=\\rho\\cdot\\text{arc cos}(\\frac{A\\cdot B}{\\rho ^2})\\)\nTriangle Angle Measurements\n\nCan have three right angles\nAll three angles added together will be greater than \\(180^\\circ\\)\n\nArea of a Sphere: \\(\\rho^2\\cdot E\\) (where the excess \\(E=\\alpha+\\beta+\\gamma-180^\\circ\\))\nConsider the surface area of a sphere to be \\(4\\pi\\rho^2\\), then the area of a triangle on a sphere will be a proportion of that.\n\n\n7. Hyperbolic Geometry\nMore than one parallel may be drawn through a point not on a given line.\nInversions about a circle\n\npreserve angles (conformal)\n\nCross Ratio: Given four distinct points (A,B,C,D) in the plane, the cross ratio is define \\((A,B;C,D)=\\frac{AC\\cdot BD}{BC\\cdot AD}\\)"
  },
  {
    "objectID": "01_blog/2021_10_25_Epsilon-min/index.html",
    "href": "01_blog/2021_10_25_Epsilon-min/index.html",
    "title": "Let epsilon = min(x-a, b-x)",
    "section": "",
    "text": "Explanation\nConsider the real number line \\(\\mathbb{R}\\) and some value x which lies between (a,b). When \\(\\epsilon=\\text{min}(x-a,b-x)\\) then \\(\\epsilon\\) is equal to \\(2\\times\\) smaller distance to either a or b. For example, in the picture below x is closer to a, so x-a is smaller than b-x. Therefore \\(\\epsilon=2(x-a)\\).\n\nIf it helps to apply values consider \\(a=1\\), \\(b=4\\), and \\(x=2\\). Then\n\\(\\epsilon=\\text{min}(x-a,b-x)=\\text{min}(2-1,4-2)=\\text{min}(1,2)=2(1)=2\\)"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html",
    "href": "01_blog/2022_11_28_python-beg/index.html",
    "title": "Python Basics",
    "section": "",
    "text": "This post covers topics from Lillian Pierson’s Linkedin Learning course Python for Data Sciene Training Part 1. Topics include Series, Data Frames, Data Visuals, Math and Statistics."
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#import-packages",
    "href": "01_blog/2022_11_28_python-beg/index.html#import-packages",
    "title": "Python Basics",
    "section": "0.0.1 Import Packages",
    "text": "0.0.1 Import Packages\n\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import Series, DataFrame"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#print-working-directory",
    "href": "01_blog/2022_11_28_python-beg/index.html#print-working-directory",
    "title": "Python Basics",
    "section": "0.0.2 Print Working Directory",
    "text": "0.0.2 Print Working Directory\n\n%pwd\n\n'/Users/randi/Desktop/2022/rbolt22/blog/01_blog/2022_11_28_python-beg'"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#extracting",
    "href": "01_blog/2022_11_28_python-beg/index.html#extracting",
    "title": "Python Basics",
    "section": "1.1 Extracting",
    "text": "1.1 Extracting\n\n1.1.1 Select row 7\n\nseries_obj['row 7']\n\n6\n\n\n\n\n1.1.2 Select Elements at Position 0 and 7\n\nseries_obj[[0,7]]\n\nrow 1    0\nrow 8    7\ndtype: int64"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#slicing",
    "href": "01_blog/2022_11_28_python-beg/index.html#slicing",
    "title": "Python Basics",
    "section": "1.2 Slicing",
    "text": "1.2 Slicing\nSelect every row between 3 and 7.\n\nseries_obj['row 3':'row 7']\n\nrow 3    2\nrow 4    3\nrow 5    4\nrow 6    5\nrow 7    6\ndtype: int64"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#scalars",
    "href": "01_blog/2022_11_28_python-beg/index.html#scalars",
    "title": "Python Basics",
    "section": "1.3 Scalars",
    "text": "1.3 Scalars\n\n1.3.1 Print values greater than 5.\n\nseries_obj[series_obj>5]\n\nrow 7    6\nrow 8    7\ndtype: int64\n\n\n\n\n1.3.2 Set row 1 to the value 8.\n\nseries_obj['row 1'] = 8\nseries_obj\n\nrow 1    8\nrow 2    1\nrow 3    2\nrow 4    3\nrow 5    4\nrow 6    5\nrow 7    6\nrow 8    7\ndtype: int64"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#missing-values",
    "href": "01_blog/2022_11_28_python-beg/index.html#missing-values",
    "title": "Python Basics",
    "section": "1.4 Missing Values",
    "text": "1.4 Missing Values\n\n1.4.1 Create a variable of missing values using np.nan\n\nnp.nan: numpy function, not a number (nan)\n\n\nmissing = np.nan\nmissing\n\nnan\n\n\n\n\n1.4.2 Create a new series object with missing values for row 3 and 7.\n\nseries_obj2 = Series(['row 1', 'row 2', missing , 'row 5', 'row 6', missing, 'row 8'])\nseries_obj2\n\n0    row 1\n1    row 2\n2      NaN\n3    row 5\n4    row 6\n5      NaN\n6    row 8\ndtype: object\n\n\n\n\n1.4.3 Find what values are missing using .isnull()\n\nisnull(): pandas function that returns t/f if null\n\n\nseries_obj2.isnull()\n\n0    False\n1    False\n2     True\n3    False\n4    False\n5     True\n6    False\ndtype: bool"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#rename-data",
    "href": "01_blog/2022_11_28_python-beg/index.html#rename-data",
    "title": "Python Basics",
    "section": "1.5 Rename Data",
    "text": "1.5 Rename Data\n\n1.5.1 Name series object.\n\nseries_obj.name =\"added_variable\"\nseries_obj\n\nrow 1    8\nrow 2    1\nrow 3    2\nrow 4    3\nrow 5    4\nrow 6    5\nrow 7    6\nrow 8    7\nName: added_variable, dtype: int64"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#extracting-1",
    "href": "01_blog/2022_11_28_python-beg/index.html#extracting-1",
    "title": "Python Basics",
    "section": "2.1 Extracting",
    "text": "2.1 Extracting\n\n2.1.1 Select values from row 2, row 5, column 5, and column 2.\n\nDF_obj.loc[['row 2', 'row 5'], ['column 5', 'column 2']]\n\n\n\n\n\n  \n    \n      \n      column 5\n      column 2\n    \n  \n  \n    \n      row 2\n      0.402366\n      0.437611\n    \n    \n      row 5\n      0.421004\n      0.559053"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#scalars-1",
    "href": "01_blog/2022_11_28_python-beg/index.html#scalars-1",
    "title": "Python Basics",
    "section": "2.2 Scalars",
    "text": "2.2 Scalars\n\n2.2.1 Return a true or false for all values less than .2\n\nDF_obj < .2\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n    \n  \n  \n    \n      row 1\n      False\n      False\n      False\n      True\n      False\n      True\n    \n    \n      row 2\n      False\n      False\n      False\n      False\n      False\n      True\n    \n    \n      row 3\n      False\n      False\n      True\n      False\n      False\n      False\n    \n    \n      row 4\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      row 5\n      False\n      False\n      True\n      False\n      False\n      False\n    \n    \n      row 6\n      False\n      False\n      False\n      False\n      False\n      False"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#missing-values-1",
    "href": "01_blog/2022_11_28_python-beg/index.html#missing-values-1",
    "title": "Python Basics",
    "section": "2.3 Missing Values",
    "text": "2.3 Missing Values\n\n2.3.1 Set the values in rows 4-5 of column 1, and rows 2-4 of column 6 to missing.\n\niloc: python function used to select a particular cell of the dataset.\n\n\nDF_obj.iloc[3:5, 0] = missing\nDF_obj.iloc[1:4, 5] = missing\nDF_obj\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n    \n  \n  \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n    \n    \n      row 2\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      NaN\n    \n    \n      row 3\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      NaN\n    \n    \n      row 4\n      NaN\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      NaN\n    \n    \n      row 5\n      NaN\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n    \n  \n\n\n\n\n\n\n2.3.2 Replace non values with 0.\n\nfillna: pandas function used to replace missing values.\n\n\n# fill NaN values with 0\nfilled_DF = DF_obj.fillna(0)\nfilled_DF\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n    \n  \n  \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n    \n    \n      row 2\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      0.000000\n    \n    \n      row 3\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      0.000000\n    \n    \n      row 4\n      0.000000\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      0.000000\n    \n    \n      row 5\n      0.000000\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n    \n  \n\n\n\n\n\n\n2.3.3 Count the number of missing values in each column.\n\nDF_obj.isnull().sum()\n\ncolumn 1    2\ncolumn 2    0\ncolumn 3    0\ncolumn 4    0\ncolumn 5    0\ncolumn 6    3\ndtype: int64\n\n\n\n\n2.3.4 Filter out rows with missing values.\n\ndropna(): pandas function that removes rows with missing values.\n\n\nDF_no_NaN_rows = DF_obj.dropna()\nDF_no_NaN_rows\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n    \n  \n  \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n    \n  \n\n\n\n\n\n\n2.3.5 Filter out columns with missing values.\n\nDF_no_NaN_columns = DF_obj.dropna(axis=1)\nDF_no_NaN_columns\n\n\n\n\n\n  \n    \n      \n      column 2\n      column 3\n      column 4\n      column 5\n    \n  \n  \n    \n      row 1\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n    \n    \n      row 2\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n    \n    \n      row 3\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n    \n    \n      row 4\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n    \n    \n      row 5\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n    \n    \n      row 6\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n    \n  \n\n\n\n\n\n\n2.3.6 Fill the missing values with the method ffill.\n\nffill: pandas function fill forward which fills in the lass non-null value in DF.\n\n\nfill_DF = DF_obj.fillna(method='ffill')\nfill_DF\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n    \n  \n  \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n    \n    \n      row 2\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      0.117376\n    \n    \n      row 3\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      0.117376\n    \n    \n      row 4\n      0.447031\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      0.117376\n    \n    \n      row 5\n      0.447031\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#duplicates",
    "href": "01_blog/2022_11_28_python-beg/index.html#duplicates",
    "title": "Python Basics",
    "section": "2.4 Duplicates",
    "text": "2.4 Duplicates\n\n2.4.1 Create a new data frame object.\n\nDF_obj2 = DataFrame({'column 1':[1,1,2,2,3,3,3],\n                   'column 2' :['a', 'a', 'b', 'b', 'c','c','c'],\n                   'column 3': ['A','A','B','B','C','C','C']})\nDF_obj2\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n    \n  \n  \n    \n      0\n      1\n      a\n      A\n    \n    \n      1\n      1\n      a\n      A\n    \n    \n      2\n      2\n      b\n      B\n    \n    \n      3\n      2\n      b\n      B\n    \n    \n      4\n      3\n      c\n      C\n    \n    \n      5\n      3\n      c\n      C\n    \n    \n      6\n      3\n      c\n      C\n    \n  \n\n\n\n\n\n\n2.4.2 Show which rows have duplicates.\n\nduplicated(): pandas function that returns t/f for rows with duplicate values.\n\n\nDF_obj2.duplicated()\n\n0    False\n1     True\n2    False\n3     True\n4    False\n5     True\n6     True\ndtype: bool\n\n\n\n\n2.4.3 Drop duplicates rows.\n\ndrop_duplicates()\n\n\nDF_obj2_row_drop  = DF_obj2.drop_duplicates()\nDF_obj2_row_drop\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n    \n  \n  \n    \n      0\n      1\n      a\n      A\n    \n    \n      2\n      2\n      b\n      B\n    \n    \n      4\n      3\n      c\n      C\n    \n  \n\n\n\n\n\n\n2.4.4 Drop duplicate from column 3.\n\nDF_obj2_c3_drop = DF_obj2.drop_duplicates(['column 3'])\nDF_obj2_c3_drop\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n    \n  \n  \n    \n      0\n      1\n      a\n      A\n    \n    \n      2\n      2\n      b\n      B\n    \n    \n      4\n      3\n      c\n      C"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#adding-data",
    "href": "01_blog/2022_11_28_python-beg/index.html#adding-data",
    "title": "Python Basics",
    "section": "2.5 Adding data",
    "text": "2.5 Adding data\n\n2.5.1 Slice the first 6 rows of series object.\n\nnew_series_obj = series_obj['row 0':'row 6']\nnew_series_obj\n\nrow 1    8\nrow 2    1\nrow 3    2\nrow 4    3\nrow 5    4\nrow 6    5\nName: added_variable, dtype: int64\n\n\n\n\n2.5.2 Add new_series_obj to the end of DF_obj\n\nvariable_added = DataFrame.join(DF_obj, series_obj)\nvariable_added\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n      added_variable\n    \n  \n  \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n      8\n    \n    \n      row 2\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      NaN\n      1\n    \n    \n      row 3\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      NaN\n      2\n    \n    \n      row 4\n      NaN\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      NaN\n      3\n    \n    \n      row 5\n      NaN\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n      4\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n      5\n    \n  \n\n\n\n\n\n\n2.5.3 Use append to add data table to itself retaining index values.\n\nadded_datatable = variable_added.append(variable_added, ignore_index=False)\nadded_datatable\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/426574045.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n      added_variable\n    \n  \n  \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n      8\n    \n    \n      row 2\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      NaN\n      1\n    \n    \n      row 3\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      NaN\n      2\n    \n    \n      row 4\n      NaN\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      NaN\n      3\n    \n    \n      row 5\n      NaN\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n      4\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n      5\n    \n    \n      row 1\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n      8\n    \n    \n      row 2\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      NaN\n      1\n    \n    \n      row 3\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      NaN\n      2\n    \n    \n      row 4\n      NaN\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      NaN\n      3\n    \n    \n      row 5\n      NaN\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n      4\n    \n    \n      row 6\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n      5\n    \n  \n\n\n\n\n\n\n2.5.4 Use append to add data table to itself, resetting index values.\n\nadded_datatable = variable_added.append(variable_added, ignore_index=True)\nadded_datatable\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/4230768127.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n  \n    \n      \n      column 1\n      column 2\n      column 3\n      column 4\n      column 5\n      column 6\n      added_variable\n    \n  \n  \n    \n      0\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n      8\n    \n    \n      1\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      NaN\n      1\n    \n    \n      2\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      NaN\n      2\n    \n    \n      3\n      NaN\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      NaN\n      3\n    \n    \n      4\n      NaN\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n      4\n    \n    \n      5\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n      5\n    \n    \n      6\n      0.870124\n      0.582277\n      0.278839\n      0.185911\n      0.411100\n      0.117376\n      8\n    \n    \n      7\n      0.684969\n      0.437611\n      0.556229\n      0.367080\n      0.402366\n      NaN\n      1\n    \n    \n      8\n      0.447031\n      0.585445\n      0.161985\n      0.520719\n      0.326051\n      NaN\n      2\n    \n    \n      9\n      NaN\n      0.836375\n      0.481343\n      0.516502\n      0.383048\n      NaN\n      3\n    \n    \n      10\n      NaN\n      0.559053\n      0.034450\n      0.719930\n      0.421004\n      0.436935\n      4\n    \n    \n      11\n      0.281701\n      0.900274\n      0.669612\n      0.456069\n      0.289804\n      0.525819\n      5\n    \n  \n\n\n\n\n\n\n2.5.5 Create a 6x6 data frame with values arraged from 0-35, and another 3x5 data frame with values arranged from 0-14.\n\nDF_obj3 = pd.DataFrame(np.arange(36).reshape(6,6))\nDF_obj3\n\nDF_obj4 = pd.DataFrame(np.arange(15).reshape(5,3))\nDF_obj4\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n    \n    \n      1\n      3\n      4\n      5\n    \n    \n      2\n      6\n      7\n      8\n    \n    \n      3\n      9\n      10\n      11\n    \n    \n      4\n      12\n      13\n      14\n    \n  \n\n\n\n\n\n\n2.5.6 Concatenate by adding columns.\n\npd.concat([DF_obj3, DF_obj4], axis = 1)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n      3\n      4\n      5\n      0.0\n      1.0\n      2.0\n    \n    \n      1\n      6\n      7\n      8\n      9\n      10\n      11\n      3.0\n      4.0\n      5.0\n    \n    \n      2\n      12\n      13\n      14\n      15\n      16\n      17\n      6.0\n      7.0\n      8.0\n    \n    \n      3\n      18\n      19\n      20\n      21\n      22\n      23\n      9.0\n      10.0\n      11.0\n    \n    \n      4\n      24\n      25\n      26\n      27\n      28\n      29\n      12.0\n      13.0\n      14.0\n    \n    \n      5\n      30\n      31\n      32\n      33\n      34\n      35\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\n2.5.7 Concatenate by adding rows.\n\npd.concat([DF_obj3, DF_obj4])\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n      3.0\n      4.0\n      5.0\n    \n    \n      1\n      6\n      7\n      8\n      9.0\n      10.0\n      11.0\n    \n    \n      2\n      12\n      13\n      14\n      15.0\n      16.0\n      17.0\n    \n    \n      3\n      18\n      19\n      20\n      21.0\n      22.0\n      23.0\n    \n    \n      4\n      24\n      25\n      26\n      27.0\n      28.0\n      29.0\n    \n    \n      5\n      30\n      31\n      32\n      33.0\n      34.0\n      35.0\n    \n    \n      0\n      0\n      1\n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3\n      4\n      5\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      6\n      7\n      8\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      9\n      10\n      11\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      12\n      13\n      14\n      NaN\n      NaN\n      NaN"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#grouping-and-aggregating-data",
    "href": "01_blog/2022_11_28_python-beg/index.html#grouping-and-aggregating-data",
    "title": "Python Basics",
    "section": "2.6 Grouping and Aggregating Data",
    "text": "2.6 Grouping and Aggregating Data\n\n2.6.1 Read cars csv with python.\n\naddress = 'data/mtcars.csv'\ncars = pd.read_csv(address)\n# assign column names\ncars.columns = ['car_names', 'mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']\ncars.head()\n\n\n\n\n\n  \n    \n      \n      car_names\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n\n2.6.2 Group by cyl and find mean values.\n\ncars_groups = cars.groupby(cars['cyl'])\ncars_groups.mean()\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/3858733335.py:2: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\n\n\n\n\n  \n    \n      \n      mpg\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n    \n      cyl\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      4\n      26.663636\n      105.136364\n      82.636364\n      4.070909\n      2.285727\n      19.137273\n      0.909091\n      0.727273\n      4.090909\n      1.545455\n    \n    \n      6\n      19.742857\n      183.314286\n      122.285714\n      3.585714\n      3.117143\n      17.977143\n      0.571429\n      0.428571\n      3.857143\n      3.428571\n    \n    \n      8\n      15.100000\n      353.100000\n      209.214286\n      3.229286\n      3.999214\n      16.772143\n      0.000000\n      0.142857\n      3.285714\n      3.500000\n    \n  \n\n\n\n\n\n\n2.6.3 Group by am and find mean values.\n\ncars_trans_group = cars.groupby(cars['am'])\ncars_trans_group.mean()\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/564591146.py:2: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      gear\n      carb\n    \n    \n      am\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      17.147368\n      6.947368\n      290.378947\n      160.263158\n      3.286316\n      3.768895\n      18.183158\n      0.368421\n      3.210526\n      2.736842\n    \n    \n      1\n      24.392308\n      5.076923\n      143.530769\n      126.846154\n      4.050000\n      2.411000\n      17.360000\n      0.538462\n      4.384615\n      2.923077"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#line-charts",
    "href": "01_blog/2022_11_28_python-beg/index.html#line-charts",
    "title": "Python Basics",
    "section": "3.1 Line Charts",
    "text": "3.1 Line Charts\n\n3.1.1 Plot a line chart with matplotlib.\n\nx = range(1,10)\ny = [1,2,3,4,0,4,3,2,1]\nplt.plot(x,y)\n\n\n\n\n\n\n3.1.2 Defining axes, limits, and tick marks\n\n# gerate a figure\nfig = plt.figure()\n\n# add axis\nax = fig.add_axes([.1,.1,1,1]) \n\n# add limits \nax.set_xlim([1,9])\nax.set_ylim([0,5])\n\n# set tick marks\nax.set_xticks([0,1,2,4,5,6,8,9,10])\nax.set_yticks([0,1,2,3,4,5])\n\n# plot\nax.plot(x,y)\n\n\n\n\n\n\n3.1.3 Add Grid\n\n# gerate a figure\nfig = plt.figure()\n\n# add axis\nax = fig.add_axes([.1,.1,1,1])\n\n# add limits \nax.set_xlim([1,9])\nax.set_ylim([0,5])\n\n# add grid\nax.grid()\n\n# plot\nax.plot(x,y)\n\n\n\n\n\n\n3.1.4 Create A plot with two lines\n\n# create new variables\nx1 = range(0,10)\ny1 = [10,9,8,7,6,5,4,3,2,1]\n\n# make line plot with two lines\nplt.plot(x,y)\nplt.plot(x1,y1)\n\n\n\n\n\n\n3.1.5 Customizing Line Styles\n\n# make line plot with two lines with style\nplt.plot(x,y, ds ='steps', lw=5)\nplt.plot(x1,y1, ls='--', lw=10)\n\n\n\n\n\n\n3.1.6 Customizing Markers\n\n# make line plot with two lines with markers\nplt.plot(x,y, marker='1', mew='20')\nplt.plot(x1,y1, marker='+', mew=15)\n\n\n\n\n\n\n3.1.7 Generating Multiple Plots\n\n# generate a figure\nfig = plt.figure()\n\n# create a tuple equal to the subplots function defined as 1 row with 2 columns\nfig,(ax1, ax2) = plt.subplots(1,2)\n\n# defining axes\nax1.plot(x)\nax2.plot(x,y)\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n3.1.8 Plot a line chart with Pandas\nUsing the cars data set from 2.6.1:\n\n# select mpg variable\nmpg = cars['mpg']\n\n# print plot\nmpg.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n3.1.9 Plot 3 Variables\n\ndf = cars[['cyl','wt','mpg']]\ndf.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n3.1.10 Define Color\n\n# color\ncolor_theme = ['darkgray', 'lightsalmon', 'powderblue']\n\n# pass in color theme\ndf.plot(color=color_theme)\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n3.1.11 Add Labels (Object Oriented Method)\nNote: Car names are numbers because of the data set I am using.\n\n# create a figure\nfig = plt.figure()\n\n# add axis \nax = fig.add_axes([.1,.1,1,1])\n\n# call plot method\nmpg.plot()\n\n# add tick marks\nax.set_xticks(range(32))\n\n# add lables with 60 degree rotaion\nax.set_xticklabels(cars.car_names, rotation=60, fontsize='medium')\n\n# set title\nax.set_title('Miles per Gallon of Cars in mtcars Dataset')\n\n# set x and y lables\nax.set_xlabel('car names')\nax.set_ylabel('mpg')\n\nText(0, 0.5, 'mpg')\n\n\n\n\n\n\n\n3.1.12 Add Legend (Object Oriented Method)\n\n# create a figure\nfig = plt.figure()\n\n# add axis \nax = fig.add_axes([.1,.1,1,1])\n\n# call plot method\nmpg.plot()\n\n# add tick marks\nax.set_xticks(range(32))\n\n# add lables with 60 degree rotaion\nax.set_xticklabels(cars.car_names, rotation=60, fontsize='medium')\n\n# set title\nax.set_title('Miles per Gallon of Cars in mtcars Dataset')\n\n# set x and y lables\nax.set_xlabel('car names')\nax.set_ylabel('mpg')\n\n# add legend\nax.legend(loc='best')\n\n<matplotlib.legend.Legend at 0x7fb97c8c1c40>\n\n\n\n\n\n\n\n3.1.13 Annotating\n\n# create a figure\nfig = plt.figure()\n\n# add axis \nax = fig.add_axes([.1,.1,1,1])\n\n# call plot method\nmpg.plot()\n\n# add tick marks\nax.set_xticks(range(32))\n\n# add lables with 60 degree rotaion\nax.set_xticklabels(cars.car_names, rotation=60, fontsize='medium')\n\n# set title\nax.set_title('Miles per Gallon of Cars in mtcars Dataset')\n\n# set x and y lables\nax.set_xlabel('car names')\nax.set_ylabel('mpg')\n\n# add legend\nax.legend(loc='best')\n\n# set y limit\nax.set_ylim([0,45])\n\n# create annotation at (19,33.9) with text at (21,35) of an arrow\nax.annotate('Toyota Corolla', xy=(19,33.9), xytext=(21,35), \n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nText(21, 35, 'Toyota Corolla')"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#bar-charts",
    "href": "01_blog/2022_11_28_python-beg/index.html#bar-charts",
    "title": "Python Basics",
    "section": "3.2 Bar Charts",
    "text": "3.2 Bar Charts\n\n3.2.1 Create a bar chart from a list\n\nplt.bar(x,y)\n\n<BarContainer object of 9 artists>\n\n\n\n\n\n\n\n3.2.2 Define bar width and plot color\n\n# widths to adjust default bar width\nwide = [.5,.5,.5,.9,.9,.9,.5,.5,.5]\n\n# change color\ncolor = ['salmon']\n\n# format barchart with adjustments\nplt.bar(x,y, width=wide, color=color, align='center')\n\n<BarContainer object of 9 artists>\n\n\n\n\n\n\n\n3.2.3 Create a bar chart from Pandas object\n\nmpg.plot(kind=\"bar\")\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n3.2.4 Create a horizontal bar chart\n\nmpg.plot(kind=\"barh\")\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n3.2.5 Add labels\n\n# create variables\nx = range(1,10)\ny = [1,2,3,4,.5,4,3,2,1]\n\n# generate barchart \nplt.bar(x,y)\n\n# add labels\nplt.xlabel('your x-axis label')\nplt.ylabel('your y-axis label')\n\nText(0, 0.5, 'your y-axis label')"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#pie-charts",
    "href": "01_blog/2022_11_28_python-beg/index.html#pie-charts",
    "title": "Python Basics",
    "section": "3.3 Pie Charts",
    "text": "3.3 Pie Charts\n\n3.3.1 Create a pie chart.\n\nx = [1,2,3,4,0.5]\n\n# create pie chart\nplt.pie(x)\n\n# show pie chart\nplt.show()\n\n\n\n\n\n\n3.3.2 Define Color\n\n# create color theme with RGB code\ncolor_theme = ['#A9A9A9', '#FFA07A', '#B0E0E6', '#FFE4C4', '#BDB76B']\n\n# call pie function\nplt.pie(x, colors=color_theme)\n\n# show pie chart\nplt.show()\n\n\n\n\n\n\n3.3.3 Add labels (Functional Method)\n\n# create variable\nz = [1,2,3,4,.5]\nveh_type = ['bicycle', 'motorbike', 'car', 'van', 'stroller']\n\n# generate pie chart\nplt.pie(z, labels=veh_type)\n\n# plot pie chart\nplt.show()\n\n\n\n\n\n\n3.3.4 Add Legend\n\n# create pie chart\nplt.pie(x)\n\n# create a legend located in the best location\nplt.legend(veh_type, loc='best')\n\n# show pot\nplt.show()\n\n\n\n\n\n\n3.3.5Saving a pie chart\n\nplt.pie(x)\nplt.savefig('figs/pie-chart.png')\nplt.show()"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#time-series",
    "href": "01_blog/2022_11_28_python-beg/index.html#time-series",
    "title": "Python Basics",
    "section": "3.4 Time Series",
    "text": "3.4 Time Series\n\n3.4.0 Load Data\n\n# address\naddress = 'data/Superstore-Sales.csv'\n# create a dataframe of csv file\ndf = pd.read_csv(address, index_col='Order Date', encoding='cp1252', parse_dates=True)\n\n# look at first 5 records\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Row ID\n      Order ID\n      Order Priority\n      Order Quantity\n      Sales\n      Discount\n      Ship Mode\n      Profit\n      Unit Price\n      Shipping Cost\n      Customer Name\n      Province\n      Region\n      Customer Segment\n      Product Category\n      Product Sub-Category\n      Product Name\n      Product Container\n      Product Base Margin\n      Ship Date\n    \n    \n      Order Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2010-10-13\n      1\n      3\n      Low\n      6\n      261.5400\n      0.04\n      Regular Air\n      -213.25\n      38.94\n      35.00\n      Muhammed MacIntyre\n      Nunavut\n      Nunavut\n      Small Business\n      Office Supplies\n      Storage & Organization\n      Eldon Base for stackable storage shelf, platinum\n      Large Box\n      0.80\n      10/20/2010\n    \n    \n      2012-10-01\n      49\n      293\n      High\n      49\n      10123.0200\n      0.07\n      Delivery Truck\n      457.81\n      208.16\n      68.02\n      Barry French\n      Nunavut\n      Nunavut\n      Consumer\n      Office Supplies\n      Appliances\n      1.7 Cubic Foot Compact \"Cube\" Office Refrigera...\n      Jumbo Drum\n      0.58\n      10/2/2012\n    \n    \n      2012-10-01\n      50\n      293\n      High\n      27\n      244.5700\n      0.01\n      Regular Air\n      46.71\n      8.69\n      2.99\n      Barry French\n      Nunavut\n      Nunavut\n      Consumer\n      Office Supplies\n      Binders and Binder Accessories\n      Cardinal Slant-D® Ring Binder, Heavy Gauge Vinyl\n      Small Box\n      0.39\n      10/3/2012\n    \n    \n      2011-07-10\n      80\n      483\n      High\n      30\n      4965.7595\n      0.08\n      Regular Air\n      1198.97\n      195.99\n      3.99\n      Clay Rozendal\n      Nunavut\n      Nunavut\n      Corporate\n      Technology\n      Telephones and Communication\n      R380\n      Small Box\n      0.58\n      7/12/2011\n    \n    \n      2010-08-28\n      85\n      515\n      Not Specified\n      19\n      394.2700\n      0.08\n      Regular Air\n      30.94\n      21.78\n      5.94\n      Carlos Soltero\n      Nunavut\n      Nunavut\n      Consumer\n      Office Supplies\n      Appliances\n      Holmes HEPA Air Purifier\n      Medium Box\n      0.50\n      8/30/2010\n    \n  \n\n\n\n\n\n\n3.4.1 Use Sample Method to Create Line Chart\n\n# use sample method\ndf2 = df.sample(n=100, random_state=25, axis=0)\n\n# add labels\nplt.xlabel('Order Date')\nplt.ylabel('Order Quantity')\n\n# add title\nplt.title('Superstore Sales')\n\n# select Order Quantity and plot\ndf2['Order Quantity'].plot()\n\n<AxesSubplot: title={'center': 'Superstore Sales'}, xlabel='Order Date', ylabel='Order Quantity'>"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#statistical-plots",
    "href": "01_blog/2022_11_28_python-beg/index.html#statistical-plots",
    "title": "Python Basics",
    "section": "3.5 Statistical Plots",
    "text": "3.5 Statistical Plots\n\n3.5.1 Scatterplot\n\n# create a scatterplot with darkgray dots of size 150\ncars.plot(kind='scatter', x='hp', y='mpg', c=['darkgray'], s=150)\n\n<AxesSubplot: xlabel='hp', ylabel='mpg'>\n\n\n\n\n\n\n\n3.5.2 Boxplots\n\n# create 2 boxplots with matplotlib\ncars.boxplot(column='mpg', by='am')\ncars.boxplot(column='wt', by='am')\n\n<AxesSubplot: title={'center': 'wt'}, xlabel='am'>"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#arrays",
    "href": "01_blog/2022_11_28_python-beg/index.html#arrays",
    "title": "Python Basics",
    "section": "4.1 Arrays",
    "text": "4.1 Arrays\n\n4.1.1 Creating Arrays\n\na = np.array([1,2,3,4,5,6])\nb = np.array([6,5,4,3,2,1])\na\nb\n\narray([6, 5, 4, 3, 2, 1])\n\n\n\n\n4.1.2 Array Arithimetic\n\na*10\na+b\na-b\na*b\na/b\n\narray([0.16666667, 0.4       , 0.75      , 1.33333333, 2.5       ,\n       6.        ])"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#matricies",
    "href": "01_blog/2022_11_28_python-beg/index.html#matricies",
    "title": "Python Basics",
    "section": "4.2 Matricies",
    "text": "4.2 Matricies\n\n4.2.1 Creating Matricies\n\naa = np.array([[2,3,6],[1,3,5],[10,20,30]])\nbb = np.array([[0,1,2],[3,4,5],[6,7,8]])\naa\nbb\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\n\n4.2.2 Multiplying Matricies\n\naa*bb\n\narray([[  0,   3,  12],\n       [  3,  12,  25],\n       [ 60, 140, 240]])\n\n\n\n\n4.2.3 Dot Product of Matricies\n\nnp.dot(aa,bb)\n\narray([[ 45,  56,  67],\n       [ 39,  48,  57],\n       [240, 300, 360]])"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#summary-statistics",
    "href": "01_blog/2022_11_28_python-beg/index.html#summary-statistics",
    "title": "Python Basics",
    "section": "4.3 Summary Statistics",
    "text": "4.3 Summary Statistics\n\n4.3.1 Sum of Column values\n\ncars.sum()\n\ncar_names    Mazda RX4Mazda RX4 WagDatsun 710Hornet 4 Drive...\nmpg                                                      642.9\ncyl                                                        198\ndisp                                                    7383.1\nhp                                                        4694\ndrat                                                    115.09\nwt                                                     102.952\nqsec                                                    571.16\nvs                                                          14\nam                                                          13\ngear                                                       118\ncarb                                                        90\ndtype: object\n\n\n\n\n4.3.2 Sum of Row values\n\ncars.sum(axis=1)\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/1808080884.py:1: FutureWarning:\n\nDropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n\n\n\n0     328.980\n1     329.795\n2     259.580\n3     426.135\n4     590.310\n5     385.540\n6     656.920\n7     270.980\n8     299.570\n9     350.460\n10    349.660\n11    510.740\n12    511.500\n13    509.850\n14    728.560\n15    726.644\n16    725.695\n17    213.850\n18    195.165\n19    206.955\n20    273.775\n21    519.650\n22    506.085\n23    646.280\n24    631.175\n25    208.215\n26    272.570\n27    273.683\n28    670.690\n29    379.590\n30    694.710\n31    288.890\ndtype: float64\n\n\n\n\n4.3.3 Median\n\ncars.median()\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/2356643283.py:1: FutureWarning:\n\nThe default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\nmpg      19.200\ncyl       6.000\ndisp    196.300\nhp      123.000\ndrat      3.695\nwt        3.325\nqsec     17.710\nvs        0.000\nam        0.000\ngear      4.000\ncarb      2.000\ndtype: float64\n\n\n\n\n4.3.4 Mean\n\ncars.mean()\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/1764053374.py:1: FutureWarning:\n\nThe default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\nmpg      20.090625\ncyl       6.187500\ndisp    230.721875\nhp      146.687500\ndrat      3.596563\nwt        3.217250\nqsec     17.848750\nvs        0.437500\nam        0.406250\ngear      3.687500\ncarb      2.812500\ndtype: float64\n\n\n\n\n4.3.5 Max\n\ncars.max()\n\ncar_names    Volvo 142E\nmpg                33.9\ncyl                   8\ndisp              472.0\nhp                  335\ndrat               4.93\nwt                5.424\nqsec               22.9\nvs                    1\nam                    1\ngear                  5\ncarb                  8\ndtype: object\n\n\n\n\n4.3.6 Find index value for row with max value\n\nmpg = cars.mpg\nmpg.idxmax()\n\n19\n\n\n\n\n4.3.7 Standard Deviation\n\ncars.std()\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/2703001680.py:1: FutureWarning:\n\nThe default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\nmpg       6.026948\ncyl       1.785922\ndisp    123.938694\nhp       68.562868\ndrat      0.534679\nwt        0.978457\nqsec      1.786943\nvs        0.504016\nam        0.498991\ngear      0.737804\ncarb      1.615200\ndtype: float64\n\n\n\n\n4.3.8 Variance\n\ncars.var()\n\n/var/folders/90/4rtssdj16dl23f_f66qj0t3w0000gn/T/ipykernel_1775/2053581105.py:1: FutureWarning:\n\nThe default value of numeric_only in DataFrame.var is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\nmpg        36.324103\ncyl         3.189516\ndisp    15360.799829\nhp       4700.866935\ndrat        0.285881\nwt          0.957379\nqsec        3.193166\nvs          0.254032\nam          0.248992\ngear        0.544355\ncarb        2.608871\ndtype: float64\n\n\n\n\n4.3.9 Counts\n\ngear = cars.gear\ngear.value_counts()\n\n3    15\n4    12\n5     5\nName: gear, dtype: int64\n\n\n\n\n4.3.10 Descriptive Statistics\n\ncars.describe()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      count\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.0000\n    \n    \n      mean\n      20.090625\n      6.187500\n      230.721875\n      146.687500\n      3.596563\n      3.217250\n      17.848750\n      0.437500\n      0.406250\n      3.687500\n      2.8125\n    \n    \n      std\n      6.026948\n      1.785922\n      123.938694\n      68.562868\n      0.534679\n      0.978457\n      1.786943\n      0.504016\n      0.498991\n      0.737804\n      1.6152\n    \n    \n      min\n      10.400000\n      4.000000\n      71.100000\n      52.000000\n      2.760000\n      1.513000\n      14.500000\n      0.000000\n      0.000000\n      3.000000\n      1.0000\n    \n    \n      25%\n      15.425000\n      4.000000\n      120.825000\n      96.500000\n      3.080000\n      2.581250\n      16.892500\n      0.000000\n      0.000000\n      3.000000\n      2.0000\n    \n    \n      50%\n      19.200000\n      6.000000\n      196.300000\n      123.000000\n      3.695000\n      3.325000\n      17.710000\n      0.000000\n      0.000000\n      4.000000\n      2.0000\n    \n    \n      75%\n      22.800000\n      8.000000\n      326.000000\n      180.000000\n      3.920000\n      3.610000\n      18.900000\n      1.000000\n      1.000000\n      4.000000\n      4.0000\n    \n    \n      max\n      33.900000\n      8.000000\n      472.000000\n      335.000000\n      4.930000\n      5.424000\n      22.900000\n      1.000000\n      1.000000\n      5.000000\n      8.0000"
  },
  {
    "objectID": "01_blog/2022_11_28_python-beg/index.html#summarizing-categorical-data",
    "href": "01_blog/2022_11_28_python-beg/index.html#summarizing-categorical-data",
    "title": "Python Basics",
    "section": "4.4 Summarizing Categorical Data",
    "text": "4.4 Summarizing Categorical Data\n\n4.4.1 Count Carborators of Each Car\n\ncarb = cars.carb\ncarb.value_counts()\n\n4    10\n2    10\n1     7\n3     3\n6     1\n8     1\nName: carb, dtype: int64\n\n\n\n\n4.4.2 Group By Gear\n\n# subset data\ncars_cat = cars[['cyl', 'vs', 'am', 'gear', 'carb']]\n# Group by gear\ngears_group = cars_cat.groupby('gear')\ngears_group.describe()\n\n\n\n\n\n  \n    \n      \n      cyl\n      vs\n      ...\n      am\n      carb\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      gear\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      3\n      15.0\n      7.466667\n      1.187234\n      4.0\n      8.0\n      8.0\n      8.0\n      8.0\n      15.0\n      0.200000\n      ...\n      0.0\n      0.0\n      15.0\n      2.666667\n      1.175139\n      1.0\n      2.0\n      3.0\n      4.0\n      4.0\n    \n    \n      4\n      12.0\n      4.666667\n      0.984732\n      4.0\n      4.0\n      4.0\n      6.0\n      6.0\n      12.0\n      0.833333\n      ...\n      1.0\n      1.0\n      12.0\n      2.333333\n      1.302678\n      1.0\n      1.0\n      2.0\n      4.0\n      4.0\n    \n    \n      5\n      5.0\n      6.000000\n      2.000000\n      4.0\n      4.0\n      6.0\n      8.0\n      8.0\n      5.0\n      0.200000\n      ...\n      1.0\n      1.0\n      5.0\n      4.400000\n      2.607681\n      2.0\n      2.0\n      4.0\n      6.0\n      8.0\n    \n  \n\n3 rows × 32 columns\n\n\n\n\n\n4.4.3 Transforming Variables to Categorical Data type\n\n# create new column\ncars['group'] = pd.Series(cars.gear, dtype = 'category')\n# look at new variable \ncars['group'].dtypes\n# look at distribution\ncars['group'].value_counts()\n\n3    15\n4    12\n5     5\nName: group, dtype: int64\n\n\n\n\n4.4.4 Describe Categorical Data with Crosstabs\n\npd.crosstab(cars['am'], cars['gear'])\n\n\n\n\n\n  \n    \n      gear\n      3\n      4\n      5\n    \n    \n      am\n      \n      \n      \n    \n  \n  \n    \n      0\n      15\n      4\n      0\n    \n    \n      1\n      0\n      8\n      5"
  },
  {
    "objectID": "01_blog/2022_07_18_Reactable/index.html",
    "href": "01_blog/2022_07_18_Reactable/index.html",
    "title": "Reactable",
    "section": "",
    "text": "1. Set Up\nThis post will use three packages:\n\nrvest: to harvest the data.\ndplyr: to join and tidy data.\nreactable: to make interactive tables.\n\n\nlibrary(rvest) \nlibrary(dplyr)\nlibrary(reactable)\n\n\n\n2. Havest the Data with rvest\nUsing data from Basketball Reference and the rvest package I can harvest current data without having to save CSV’s. For this table I will be pulling in the final two teams which competed in the 2022 NBA Finals:\n\nGolden State Warriors\nBoston Celtics\n\n\n# team name\ngsw <- \"Golden State Warriors\"\nbc <- \"Boston Celtics\"\n\n# team slug\ngsw_slug <- \"GSW\"\nbc_slug <- \"BOS\"\n\n# url\ngsw_url <- base::paste0(\"https://www.basketball-reference.com/teams/\",\n                        gsw_slug,\"/2022.html\")\nbc_url <- base::paste0(\"https://www.basketball-reference.com/teams/\",\n                       bc_slug,\"/2022.html\")\n\nThere is a lot of data available on Basketball Reference, but for this table I will only be looking at each teams 2022 total stats.\n\n# harvest data\ngsw_ttl_stat <- gsw_url %>%\n  read_html %>%\n  html_node(\"#totals\") %>% \n  html_table()\n\nbc_ttl_stat <- bc_url %>%\n  read_html %>%\n  html_node(\"#totals\") %>% \n  html_table()\n# look at data\nutils::head(gsw_ttl_stat)\n\n# A tibble: 6 × 28\n     Rk ``       Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n  <int> <chr>  <int> <int> <int> <int> <int> <int> <dbl> <int> <int> <dbl> <int>\n1     1 Andre…    26    73    73  2329   475  1019 0.466   157   399 0.393   318\n2     2 Jorda…    22    76    51  2283   474  1058 0.448   211   580 0.364   263\n3     3 Steph…    33    64    64  2211   535  1224 0.437   285   750 0.38    250\n4     4 Kevon…    25    82    80  1732   208   364 0.571     0     1 0       208\n5     5 Otto …    28    63    15  1396   193   416 0.464    80   216 0.37    113\n6     6 Draym…    31    46    44  1329   135   257 0.525    16    54 0.296   119\n# … with 15 more variables: `2PA` <int>, `2P%` <dbl>, `eFG%` <dbl>, FT <int>,\n#   FTA <int>, `FT%` <dbl>, ORB <int>, DRB <int>, TRB <int>, AST <int>,\n#   STL <int>, BLK <int>, TOV <int>, PF <int>, PTS <int>\n\nutils::head(bc_ttl_stat)\n\n# A tibble: 6 × 28\n     Rk ``       Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n  <int> <chr>  <int> <int> <int> <int> <int> <int> <dbl> <int> <int> <dbl> <int>\n1     1 Jayso…    23    76    76  2731   708  1564 0.453   230   651 0.353   478\n2     2 Marcu…    27    71    71  2296   300   718 0.418   119   360 0.331   181\n3     3 Jayle…    25    66    66  2220   576  1217 0.473   166   464 0.358   410\n4     4 Al Ho…    35    69    69  2005   266   569 0.467    89   265 0.336   177\n5     5 Grant…    23    77    21  1875   205   432 0.475   106   258 0.411    99\n6     6 Rober…    24    61    61  1804   271   368 0.736     0     1 0       271\n# … with 15 more variables: `2PA` <int>, `2P%` <dbl>, `eFG%` <dbl>, FT <int>,\n#   FTA <int>, `FT%` <dbl>, ORB <int>, DRB <int>, TRB <int>, AST <int>,\n#   STL <int>, BLK <int>, TOV <int>, PF <int>, PTS <int>\n\n\n\n\n3. Tidy Data\nTo tidy the data I want to do 3 things:\n\nRename column 2 to ‘Name’.\nAdd a column with the team name.\nJoin two tables into one.\n\n\n# remane \nbase::names(gsw_ttl_stat)[2] <- \"Name\"\nbase::names(bc_ttl_stat)[2] <- \"Name\"\n\n# add column\ngsw_ttl_stat$Team <- gsw\nbc_ttl_stat$Team <- bc\n\n# merge tables\ntotal_stats <- dplyr::full_join(gsw_ttl_stat, bc_ttl_stat)\n\n#view data  \nutils::head(total_stats)\n\n# A tibble: 6 × 29\n     Rk Name     Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n  <int> <chr>  <int> <int> <int> <int> <int> <int> <dbl> <int> <int> <dbl> <int>\n1     1 Andre…    26    73    73  2329   475  1019 0.466   157   399 0.393   318\n2     2 Jorda…    22    76    51  2283   474  1058 0.448   211   580 0.364   263\n3     3 Steph…    33    64    64  2211   535  1224 0.437   285   750 0.38    250\n4     4 Kevon…    25    82    80  1732   208   364 0.571     0     1 0       208\n5     5 Otto …    28    63    15  1396   193   416 0.464    80   216 0.37    113\n6     6 Draym…    31    46    44  1329   135   257 0.525    16    54 0.296   119\n# … with 16 more variables: `2PA` <int>, `2P%` <dbl>, `eFG%` <dbl>, FT <int>,\n#   FTA <int>, `FT%` <dbl>, ORB <int>, DRB <int>, TRB <int>, AST <int>,\n#   STL <int>, BLK <int>, TOV <int>, PF <int>, PTS <int>, Team <chr>\n\ntotal_stats\n\n# A tibble: 47 × 29\n      Rk Name    Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n   <int> <chr> <int> <int> <int> <int> <int> <int> <dbl> <int> <int> <dbl> <int>\n 1     1 Andr…    26    73    73  2329   475  1019 0.466   157   399 0.393   318\n 2     2 Jord…    22    76    51  2283   474  1058 0.448   211   580 0.364   263\n 3     3 Step…    33    64    64  2211   535  1224 0.437   285   750 0.38    250\n 4     4 Kevo…    25    82    80  1732   208   364 0.571     0     1 0       208\n 5     5 Otto…    28    63    15  1396   193   416 0.464    80   216 0.37    113\n 6     6 Dray…    31    46    44  1329   135   257 0.525    16    54 0.296   119\n 7     7 Dami…    29    63     5  1256   169   383 0.441    63   187 0.337   106\n 8     8 Gary…    29    71    16  1247   212   344 0.616    43   120 0.358   169\n 9     9 Jona…    19    70    12  1185   236   460 0.513    50   149 0.336   186\n10    10 Nema…    33    71     0  1142   160   342 0.468    54   149 0.362   106\n# … with 37 more rows, and 16 more variables: `2PA` <int>, `2P%` <dbl>,\n#   `eFG%` <dbl>, FT <int>, FTA <int>, `FT%` <dbl>, ORB <int>, DRB <int>,\n#   TRB <int>, AST <int>, STL <int>, BLK <int>, TOV <int>, PF <int>, PTS <int>,\n#   Team <chr>\n\n\n\n\n4. Reactable\nNow to make a simple reactable I will do 7 things:\n\nGroup by “Team” name.\nDefine column names.\nInclude boarders around the table and every cell.\nInclude highlight rows that are hovered over.\nMake filterable.\nMake Searchable.\nHave the two teams be the minimum number of rows initally shown.\n\n\nreactable(\n  total_stats,\n  groupBy = \"Team\",\n  columns = list(\n    Rk = colDef(name = \"Rank\"),\n    G = colDef(name = \"Games\"),\n    MP = colDef(name = \"Minutes Played\"),\n    FG = colDef(name = \"Field Goals\"),\n    `3P` = colDef(name = \"3 Point Goals\"),\n    `2P` = colDef(name = \"2 Point Goals\"),\n    FT = colDef(name = \"Free Throws\"),\n    AST = colDef(name = \"Assists\"),\n    STL = colDef(name = \"Steals\"),\n    BLK = colDef(name = \"Blocks\"),\n    PTS = colDef(name = \"Points\"),\n    TOV = colDef(name = \"Turnovers\"),\n    PF = colDef(name = \"Personal Fouls\")\n  ),\n  bordered = TRUE,\n  highlight = TRUE,\n  filterable = TRUE,\n  searchable = TRUE,\n  minRows = 2\n  )\n\n\n\n\n\n\n\n\n5. Sources\nNBA Analytics Tutorial: Using R to Display Player Career Stats\nReactable"
  },
  {
    "objectID": "01_blog/2022_01_24-latex-hacks/index.html",
    "href": "01_blog/2022_01_24-latex-hacks/index.html",
    "title": "Latex",
    "section": "",
    "text": "Basic Symbols :\n\n\\(\\sim\\) : \\sim\n\\(\\circ\\) : \\circ\n\\(\\square\\) : \\square\n\\(\\equiv\\) : \\equiv\n\\(\\cong\\) : \\cong\n\\(\\unlhd\\) : \\unlhd\n\\(\\div\\) : \\div\n\\(\\nless\\) : \\nless\n\\(\\ngtr\\) : ngtr\n\\(\\emptyset\\) : \\emptyset\n\\(\\subseteq\\) : \\subseteq\n\\(a\\choose b\\) : a\\choose b\n\\(\\underset{i\\in I}U\\) : \\underset{i\\in I}U\n\\(\\Leftrightarrow\\) : \\Leftrightarrow\n\\(\\langle\\rangle\\) : \\langle\\rangle\n\\(\\overrightarrow{\\rm AB}\\) : \\overrightarrow{\\rm AB}\n\\(\\underline{\\text{Underline Text}}\\) : \\underline{\\text{Underline Text}}\n\\(\\mathbb{R}\\) : \\mathbb{R}\n\nbb : blackboard bold\n\n\\(\\mathcal{F}\\) : \\mathcal{F}\n\\(\\mathscr{F}\\) : \\mathscr{F}\n\n\n\nGreek :\n\n\\(\\tau\\) : \\tau\n\\(\\rho\\) : \\rho\n\\(\\alpha\\) : \\alpha\n\\(\\beta\\) : \\beta\n\\(\\Gamma\\) : \\Gamma\n\\(\\epsilon\\) : \\epsilon\n\\(\\mathcal{E}\\) : \\mathcal{E}\n\\(\\varepsilon\\) : \\varepsilon\n\\(\\varphi\\) : \\varphi\n\n\n\nInline :\nLimits above and below sums and integrals\n\n\\(\\sum\\limits_{n}^{i}\\int_0^1\\) : \\limits\n\nMatrices and Matrix Equations\n\n\\(\\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}\\) : \\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}\n\n\\(I=[\\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}]\\)\n\\((\\begin{smallmatrix} 1 & 1 \\\\ 1 & 0\\end{smallmatrix})(\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1\\end{smallmatrix})\\ne(\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1\\end{smallmatrix})(\\begin{smallmatrix} 1 & 1 \\\\ 1 & 0\\end{smallmatrix})\\)\n\n\n\n\nMultiple Lines :\nFunction\n\n\\(F(x)=\\begin{cases}1 & x\\geq 0\\\\0 & \\text{otherwise}\\end{cases}\\) : F(x)=\\begin{cases} . . . \\end{cases}\n\n1 & x \\geq 0 \\\\\n0 & \\text{otherwise}\n\n\nMatrix\n\n\\(F(x)=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\\\5\\\\6\\\\\\end{bmatrix}\\) : F(x)=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\\\5\\\\6\\\\\\end{bmatrix}\n\nSeries of Equalities\n\n\\(\\begin{equation}\\label{a}\\begin{split}x &= a+b+c\\\\&=1+2+3\\\\&=6\\end{split}\\end{equation}\\)\n\n: \\begin{equation}\\{label}\\begin{split}... \\end{split}\\end{equation}\n\nx &= a+b+c \\\\\n&= 1+2+3\n&= 6\n\n\n\nPotential Errors :\nSpelling\n\nEx: \\overlien{AB} should be \\overline{AB}\n\nlabel , table\n\n\nMore than two backslashes\n\nEx: Equation will work but \\end{equation} will show at the end. One of the lines has more than two backslashes at the end of at least one of the lines.\n\nSpace before final $\n\nEx: $\\angle ABC $ should be $\\angle ABC$\n\nMore $ on one side of equation than the other\n\nEx: $A^2+B^2=C^2$$ should be $A^2+B^2=C^2$\n\nClosing {}\n\nEx: $\\int\\limits_{1}^{2$ should be $\\int\\limits_{1}^{2}$\n\nUnderset on the wrong side\n\nEx: U\\underset{i\\in I} should be \\underset{i\\in I}U\n\n\n\nAdvice :\nDetextify\n\nIf you don’t know what a symbol is, draw it in Detextify here.\n\nGoogle Docs Equations Boxes\n\nMost latex backslashes work in google doc’s equation boxes. If I have to do a “quick” homework, and dont want to spend a lot of time formatting a pdf in R, I will use google docs and latex in the equations boxes.\n\nNote: This is how I began learning latex.\nWhy learn Latex?\n\nGenerally speaking it looks nicer, especially on reports, projects, and presentations.\nA lot of my peers and professors who wrote math by hand would have problems in their dominate writing hand. Typing math with latex helps spread that tension out to two hands.\nIt saves time in the long run, since updating a line in a typed document is a lot easier than re-writing an entire problem by hand. Not to mention the ability to use copy paste.\n\n\n\nBonus:\nColors\n\n\\(\\color{red}\\text{colored text}\\) : \\color{red}\\text{colored text}"
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html",
    "title": "Midterm Prep. Group Theory",
    "section": "",
    "text": "Notes consist of Sets, Subsets, Operations, Group, Abelian Group, Subgroups, and Homework 1 and 2 questions."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#examples",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#examples",
    "title": "Midterm Prep. Group Theory",
    "section": "Examples",
    "text": "Examples\n\n\\(\\mathbb{N}=\\{1,2,3,...\\}\\) : they exist naturally\n\\(\\mathbb{Z}=\\{...,-3,-2,-1,0,1,2,3,...\\}\\) : includes zero and negatives\n\\(\\mathbb{Q}=\\{\\frac{m}{n}|m,n\\in \\mathbb{Z}\\text{ and }n\\ne 0\\}\\) : integer fractions\n\\(\\mathbb{R}\\) : includes square roots and pie, real analysis starts with \\(\\sqrt{2}\\)\n\\(\\mathbb{C}\\) : includes imaginary numbers\nAsterisk in the superscript means delete zero\nPlus sign in the superscript means only positive values (>0)"
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#properties",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#properties",
    "title": "Midterm Prep. Group Theory",
    "section": "Properties",
    "text": "Properties\n\n* is commutative if \\(a\\ne b\\), \\(a*b=b*a\\) \\(\\forall\\) a,b \\(\\in A\\).\n\n+ and \\(\\cdot\\) are commutative\n- , \\(\\div\\) , funtion composition and matrix multiplication are not commutative\n\n* is associative if \\((a*b)*c=a*(b*c)\\) \\(\\forall\\) a,b,c, \\(\\in A\\).\n\naddition is associative, subtraction is not associative\n\nIf \\(\\exists\\) \\(e\\in A\\) \\(\\Rightarrow\\) \\(e*a=a*e=a\\) \\(\\forall\\) \\(a\\in A\\), the we call e the identity element in A w.r.t. *.\n\n0=e w.r.t. addition\n1=e w.r.t. multiplication\n\nIf \\(e\\in A\\) is the identity w.r.t. * and \\(a,b\\in A\\Rightarrow\\) \\(a*b=b*a=e\\) we call a and b inverses of one another.\n\nthe inverse of \\(a\\in \\mathbb{R}\\) w.r.t. addition is -a since \\(a+(-a)=(-a)+a=0\\)\nthe inverse of \\(a\\in\\mathbb{R}^*\\) w.r.t. multiplication is \\(\\frac{1}{a}\\) since \\(a(\\frac{1}{a})=(\\frac{1}{a})a=1\\)"
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proof-outlines",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proof-outlines",
    "title": "Midterm Prep. Group Theory",
    "section": "Proof Outlines",
    "text": "Proof Outlines\n\nCommutative:\n\\(\\underline{\\text{No}}\\): Give an example, “Let a = 1, and b =2”, and then show \\(a*b\\ne b*a\\).\n\\(\\underline{\\text{Yes}}\\): “For any a,b in the set” and then show \\(a*b=b*a\\).\nAssociative\n\\(\\underline{\\text{No}}\\): Give an example, “Let a=1, b=2, c=3” and then show \\(a*(b*c)\\ne (a*b)*c\\).\n\\(\\underline{\\text{Yes}}\\): “For any a,b,c in the set” and then show \\(a*(b*c)=(a*b)*c\\).\nIdentity\n\\(\\underline{\\text{No}}\\): Suppose that \\(e\\in\\) the given set \\(\\Rightarrow\\) \\(a*e=a\\) \\(\\forall a\\in\\) the given set. Then show \\(a*e=a\\) by plugging e in for b and solving for e. “Since the identity element must be a constant then there is no identiy w.r.t.” the given set. (can’t involve variables)\n\\(\\underline{\\text{Yes}}\\): State what the identity element is w.r.t. the orperation and show that \\(a*e=a\\) and \\(e*a=a\\).\nInverses\n\\(\\underline{\\text{No}}\\): Given an example of an element who doesn’t have an inverse.\nNote: If there is not identity element then there is no inverse.\n\\(\\underline{\\text{Yes}}\\): “Suppose b=\\(a^{-1}\\). Then \\(a*b=e\\)” (where e is the identity found in 3), and then try to solve the eqation for b. Then check \\(b*a=e\\) as well.\nNote: Do not need to check \\(b*a=e\\) if we know * is commutative."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proposition-1",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proposition-1",
    "title": "Midterm Prep. Group Theory",
    "section": "Proposition 1",
    "text": "Proposition 1\nLet G be a group, then G has exactly one identity element."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proposition-2",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proposition-2",
    "title": "Midterm Prep. Group Theory",
    "section": "Proposition 2",
    "text": "Proposition 2\nEvery element of G has exactly one inverse."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#theorem-1-cancellation-law",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#theorem-1-cancellation-law",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 1 (Cancellation Law)",
    "text": "Theorem 1 (Cancellation Law)\nLet G be a group and let \\(a,b,c\\in G\\), then \\(ab=ac\\Rightarrow b=c\\) and \\(ba=ca\\Rightarrow b=c\\)."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#theorem-2",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#theorem-2",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 2",
    "text": "Theorem 2\nLet G be a group and let \\(a,b\\in G\\). If \\(ab=e\\), then a and b are inverses, i.e. \\(a=b^{-1}\\) and \\(b=a^{-1}\\)."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#theorem-3",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#theorem-3",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 3",
    "text": "Theorem 3\nLet G be a group and let \\(a,b\\in G\\) then \\((ab)^{-1}=b^{-1}a^{-1}\\) and \\((a^{-1})^{-1}=a\\).\n\nto show a and b are inverses, show their product is e."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#klein-4-group",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#klein-4-group",
    "title": "Midterm Prep. Group Theory",
    "section": "Klein 4 Group",
    "text": "Klein 4 Group\n(for fintie groups) : \\(a^2=b^2=c^2=e\\)"
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#example",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#example",
    "title": "Midterm Prep. Group Theory",
    "section": "Example",
    "text": "Example\nAddition:\n\n\\(0\\in H\\).\n\\(\\forall\\) a,b \\(\\in H\\) \\(a+b=H\\).\n\\(\\forall\\) \\(a\\in H\\) , \\(-a\\in H\\)."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proof-outlines-two-step-subgroup-test",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#proof-outlines-two-step-subgroup-test",
    "title": "Midterm Prep. Group Theory",
    "section": "Proof Outlines (Two-Step Subgroup Test)",
    "text": "Proof Outlines (Two-Step Subgroup Test)\n\n\\(e\\in H\\)\nfor all a,b \\(\\in H\\), \\(ab^{-1}\\in H\\).\n\nProve something is a subgroup of G.\n\n“Suppose” then show e is 0 or 1 for the subgroup in a short series of equalities, “the additive or multiplicative element 0 or 1 is in” the subgroup.\n“Now take any a,b \\(\\in\\) the subgroup.” Then define a and b potentially for some other integers. Then show \\(ab^{-1}=\\) something identifiable in the subgroup.\n\n“Therefore we’ve shown that” our subgroup “contains the identity element and for all a,b \\(\\in H\\) , \\(ab^{-1}\\in\\)” our subgroup. Thus our subgroup is a subgroup of G."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#example-1",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#example-1",
    "title": "Midterm Prep. Group Theory",
    "section": "Example",
    "text": "Example\nAddition:\n\n\\(0\\in H\\).\nfor all \\(a,b\\in H\\), \\(a+(-b)=a-b\\in H\\)."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#added-notes",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#added-notes",
    "title": "Midterm Prep. Group Theory",
    "section": "Added Notes",
    "text": "Added Notes\n\nIf G is abelian, then \\((ab)^n=a^nb^n\\) for all \\(n\\in \\mathbb{N}\\).\nIn any group \\((a^{-1})^n=(a^n)^{-1}\\) for all \\(n\\in \\mathbb{N}\\).\nIn any group \\(e^{-1}=e\\)."
  },
  {
    "objectID": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#two-step-subgroup-test",
    "href": "01_blog/2022_03_28_Midterm-Prep-Group-Theory/index.html#two-step-subgroup-test",
    "title": "Midterm Prep. Group Theory",
    "section": "Two-step subgroup test",
    "text": "Two-step subgroup test\nLet G be a group. A subset H \\(\\subseteq G\\) is a subgroup of G if\n\n\\(e\\in H\\)\n\n\naddition: show 0 \\(\\in H\\)\n\n\n\\(\\forall a,b,\\in H\\), \\(ab^{-1}\\in H\\).\n\n\naddition: \\(\\forall\\) a,b \\(\\in H\\), \\(a+(-b)=a-b\\in H\\)"
  },
  {
    "objectID": "01_blog/2022_02_28_RStudio-building-a-blog-with-R/index.html",
    "href": "01_blog/2022_02_28_RStudio-building-a-blog-with-R/index.html",
    "title": "RStudio: Building a Blog with R",
    "section": "",
    "text": "1. About Isabella Velesquez\nEmail: isabella.velasquez@rstudio.com\n\nWorks at Posit.\nSeattle Lady Co-Organizer.\nFirst R-Ladies talk in 2018.\n\n\n\n2. Agenda\n\nWhy create a blog?\nDeciding on a topic.\nTools for building a blog.\n\n\n\n3. Why create a blog?\n\nWhen you’re given the same advice 3 times, write a blog post.\nShare what you’ve learned.\nWrite your opinions.\nShare updates, and news.\nExternal blogs (for business)\n\nPosit has 4 different external blogs:\n\nThe Posit Blog\nPosit AI Blog\nTidyverse\nR Views\n\n\nInternal blogs (for business)\n\nShare information more easily and effectively.\nImprove collaboration.\nServing as a bulletin board for projects.\n\n\n\n\n4. Types of Posts\n\nStandard lists\nHow To’s / tutorials\nNew posts\nProblem - and - solution\nFAQ\nCheat sheets\nChecklists\nInfo graphics\nPresentations\nDebates\nInspiration\nInterviews\n\n\n\n5. Seperating Posts\n\nTutorials (learning oriented)\nHow To’s (task oriented)\nExplanation (understanding oriented)\nReference (information oriented)\n\n\n\n6. Building a Blog with R\n\nKnowledge of R and R Markdown.\nVersion Control (Github)\nNetlify\n\n\n\n7. Overall Thoughts\nWhile this was called “Building a blog”, there wasn’t a lot of blog building. It was very business and product information heavy.\n\n\n8. Recommended Blog (from chat)\nMachine Learning Mastery"
  },
  {
    "objectID": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html",
    "href": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "",
    "text": "Notes consists of logic, Euclid’s 5th, Congruence and Length Theorem, Congruence and Angle Theorem, angles and parallel lines, and triangle congruence theorems."
  },
  {
    "objectID": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html#example",
    "href": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html#example",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Example",
    "text": "Example\nConditional Statement: If it is cloudy then it is raining.\nNegation: It could be cloudy and not raining.\nInverse: If it is not cloudy then it is not raining.\nContrapositive: If it is not raining then it is not cloudy.\nConverse. If it is rainy, then it is cloudy."
  },
  {
    "objectID": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html#example-1",
    "href": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html#example-1",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Example",
    "text": "Example\nWhen proving supplementary angles add to \\(180^\\circ\\) we were able to use Euclids 5th element to say that the supplementary interior angles added up to \\(\\geq 180^\\circ\\) because the lines are parallel (and don’t intersect)."
  },
  {
    "objectID": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html#proof-points",
    "href": "01_blog/2022_03_21_Midterm-Prep-Modern-College-Geometry/index.html#proof-points",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Proof Points",
    "text": "Proof Points\n\n“Because \\(\\triangle ABC\\cong \\triangle DEF\\) then there is an isometry f that superimposes angle ABC on angle DEF. Isometries preserve angle measure, so angles ABC and DEF must have had the same measure.”\n“Suppose angles ABC and DEF have the same measure.” Then explain isometry needed to move angle ABC to angle DEF. “Translation, rotation, and reflection are all isometries, so we’ve shown angles ABC and DEF are congruent.”"
  },
  {
    "objectID": "01_blog/2022_01_31_LearnGeom/index.html",
    "href": "01_blog/2022_01_31_LearnGeom/index.html",
    "title": "LearnGeom",
    "section": "",
    "text": "1. Set Up\nTo create coordinate planes, trianges, and line segments I will be using the LearnGeom package.\n\nlibrary(LearnGeom)\n\n\n\n2. Coordinate Plane\nTo create a coordinate plane I will first need to define x and y minimums and maximums, and then plot the planes with the CoordinatePlane() function.\n\nx_min <- 0\nx_max <- 10\ny_min <- 0\ny_max <- 10\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\n\n\nNULL\n\n\n\n\n3. Polygons\nTo create a triangle with labels:\n\nPrint the coordinate plane I just created.\nDefine three points of a triangle.\nUse CreatePolygon() function to create the polygon.\nUse the Draw() function to draw the polygon.\nDefine label = TRUE to show the points of a triangle.\n\n\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\nNULL\n\nP1 <- c(1,4)\nP2 <- c(3,7)\nP3 <- c(4, 1)\nPoly <- LearnGeom::CreatePolygon(P1, P2, P3)\n\n[1] \"Some of the inserted points are collinear. This could lead to a defective polygon.\"\n\nLearnGeom::Draw(Poly, c(\"pink\"), label = TRUE)\n\n\n\n\nTriangle\n\n\n\n\nNULL\n\n\nTo create a trapezoid:\n\nPrint the coordinate plane I just created.\nDefine four points of a trapezoid.\nUse CreatePolygon() function to create the polygon.\n\nNote: The order of points will matter.\n\nUse the Draw() function to draw the polygon.\n\n\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\nNULL\n\nP4 <- c(6, 3)\nP5 <- c(8, 3)\nP6 <- c(9, 8)\nP7 <- c(7, 8)\nPoly2 <- LearnGeom::CreatePolygon(P4, P5, P6, P7)\n\n[1] \"Some of the inserted points are collinear. This could lead to a defective polygon.\"\n\nLearnGeom::Draw(Poly2, c(\"light blue\"))\n\n\n\n\nTrapezoid\n\n\n\n\nNULL\n\n\nWe can also print both polygons on the same graph, shown below.\n\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\nNULL\n\nLearnGeom::Draw(Poly, c(\"pink\"), label = TRUE)\n\nNULL\n\nLearnGeom::Draw(Poly2, c(\"light blue\"))\n\n\n\n\nTriangle and Trapezoid\n\n\n\n\nNULL\n\n\n\n\n4. Angle and Point Line Segments\nTo create a Segment Angle:\n\nPrint the coordinate plane I just created.\nDefine a points where the line originates from.\nDefine the angle of the line.\nDefine the length of the line.\nUse CreateSegmentAngle() function to create the line segment.\nUse the Draw() function to draw the line.\n\n\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\nNULL\n\nP <- c(0,0)\nangle <- 30\nlen <- 10\nSegment <- LearnGeom::CreateSegmentAngle(P, angle, len)\nLearnGeom::Draw(Segment, \"blue\")\n\n\n\n\nNULL\n\n\nSegment Point\nTo create a Segment (with) Point(s):\n\nPrint the coordinate plane.\nDefine two endpoint.\nUse CreateSegmentPoint() function to create the line segment.\nUse the Draw() function to draw the line.\n\n\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\nNULL\n\nP1 <- c(2,8)\nP2 <- c(8,6)\nSegment <- LearnGeom::CreateSegmentPoints(P1, P2)\nLearnGeom::Draw(Segment, \"purple\")\n\n\n\n\nNULL"
  },
  {
    "objectID": "01_blog/2021_11_29_Proof-Idempotent/index.html",
    "href": "01_blog/2021_11_29_Proof-Idempotent/index.html",
    "title": "Prove H and I-H are Idempotent",
    "section": "",
    "text": "Proof\nFor H to be Idempotent then \\(HH=H\\)\n\\[\\begin{equation}\\label{HH=H}\n\\begin{split}\nHH & =[X(X^TX)^{-1}X^T][X(X^TX)^{-1}X^T]\\\\\n& = X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T\\quad\\quad(X^TX)^{-1}X^TX=1\\\\\n& = X(X^TX)^{-1}X^T\\\\\n& = H\n\\end{split}\n\\end{equation}\\]\nTherefore by the series of equalities H is idempotent.\nFor I-H to be idempotent then \\((I-H)(I-H)=I-H\\)\n\\[\\begin{equation}\\label{I-H}\n\\begin{split}\n(I-H)(I-H) & =II-HI-IH+HH\\quad\\quad II=I, HI=IH=H, HH=H\\\\\n& = I-H-H+H\\\\\n& = I-H\n\\end{split}\n\\end{equation}\\]\nTherefor by the series of equalities I-H is idempotent.\nQED."
  },
  {
    "objectID": "01_blog/2021_10_18_Tree-Diagrams/index.html",
    "href": "01_blog/2021_10_18_Tree-Diagrams/index.html",
    "title": "Tree Diagrams",
    "section": "",
    "text": "1. Set-Up\nThis example is from W3-D5 Example 1 of my Statistics-461 Notes, and uses the data.tree package which can create a multiple node object.\n\nlibrary(data.tree)\n\n\n\n2. Creating a Node Object\nAll trees are constructed by tying together Node object, so to start I will create a new Node object for example 1.\n\nex1 <- data.tree::Node$new(\"Example 1\")\n\nFor example 1 we suppose that 1% of the population uses a certain drug. So next I want to AddChild to ex1 to show those who use the drug (d) and do not use the drug (dc, where c means compliment).\n\nd <- ex1$AddChild(\"Uses Drug\", p = 0.01)\ndc <- ex1$AddChild(\"Does Not Use Drug\", p = 0.99)\n\nNow let t be tests positive for the disease. The drug manufacturer claims that \\(P(T|D^C)=0.015\\) and \\(P(T^C|D)=0.005\\). Which means that:\n\\(P(T^C|D^C)=1-P(T|D^C)=1-0.015=0.985\\) and\n\\(P(T|D)=1-P(T^C|D)=1-0.005=0.995\\)\nSo lets add another layer of nodes to example 1.\n\nt <- d$AddChild(\"Positive Test\", p = 0.995)\ntc <- d$AddChild(\"Negative Test\", p = 0.005)\nt <- dc$AddChild(\"Positive Test\", p = 0.015)\ntc<- dc$AddChild(\"Negative Test\", p = 0.985)\n\nAnd then print what information we have.\n\nbase::print(ex1, 'p')\n\n              levelName     p\n1 Example 1                NA\n2  ¦--Uses Drug         0.010\n3  ¦   ¦--Positive Test 0.995\n4  ¦   °--Negative Test 0.005\n5  °--Does Not Use Drug 0.990\n6      ¦--Positive Test 0.015\n7      °--Negative Test 0.985\n\n\nIf the probability column shows as a percentages we can use the SetFormat() function to set the decimal to 3 places.\n\ndata.tree::SetFormat(ex1, \"p\", formatFun = data.tree::FormatFixedDecimal(3))\n\nAnd print the information we have.\n\nbase::print(ex1, 'p')\n\n              levelName     p\n1 Example 1                NA\n2  ¦--Uses Drug         0.010\n3  ¦   ¦--Positive Test 0.995\n4  ¦   °--Negative Test 0.005\n5  °--Does Not Use Drug 0.990\n6      ¦--Positive Test 0.015\n7      °--Negative Test 0.985\n\n\n\n\n3. Conditional Probability\nGiven a positive test, we can find the probability that a person actually uese the drug with the following equation:\n\\(\\begin{equation}\\label{a}\\begin{split}P(D|T) &= \\frac{P(T|D)\\times P(D)}{[P(T|D)\\times P(D)]+[P(T|D^C)\\times P(D^C)]}\\\\&=\\frac{(0.995)(0.01)}{(0.995\\times 0.01)+(0.015\\times 0.99)}\\\\&=\\frac{199}{496}\\\\&\\approx 0.4012\\end{split}\\end{equation}\\)\n\n\n4. Plotting a Tree Diagram\nLastly we can use the plot() function to print a Tree Diagram:\n\nbase::plot(ex1)\n\n\n\n\n\nTo visualize the tree diagram from left to right instead of top to bottom we can use the SetGraphStyle() function as shown below.\n\ndata.tree::SetGraphStyle(ex1, rankdir = \"LR\")\nbase::plot(ex1)"
  },
  {
    "objectID": "01_blog/2022_11_07_NBA-functions/index.html",
    "href": "01_blog/2022_11_07_NBA-functions/index.html",
    "title": "SportsObserveR - Part 1: Scraping Functions",
    "section": "",
    "text": "In this tutorial I will be creating functions to scrape NBA data. The goal here is to prepare these functions to use in a package for future analysis."
  },
  {
    "objectID": "01_blog/2022_11_07_NBA-functions/index.html#about-the-data",
    "href": "01_blog/2022_11_07_NBA-functions/index.html#about-the-data",
    "title": "SportsObserveR - Part 1: Scraping Functions",
    "section": "0_1. About-The-Data",
    "text": "0_1. About-The-Data\nI will be scrapping data from Basketball Reference which gets thier data updated regularly by a handful of contributors and sources. The main reasons I like using this data is because it’s reliable, updated regularly, and similar sites exist for other non-NBA Sports (such as: WNBA, Baseball, Football, and others) if I wanted to expand my research outside the NBA."
  },
  {
    "objectID": "01_blog/2022_11_07_NBA-functions/index.html#package-installs",
    "href": "01_blog/2022_11_07_NBA-functions/index.html#package-installs",
    "title": "SportsObserveR - Part 1: Scraping Functions",
    "section": "0_2. Package Installs",
    "text": "0_2. Package Installs\nThe packages I will be using are rvest to scrape the data and magrittr to pipe it. To install these packages, copy the code below and remove the first comment hash (command - shift - c).\n\n## install packages\n# install.packages(\"rvest\",  \"magrittr\")\n\nThen load:\n\n# load packages \nlibrary(rvest) \nlibrary(magrittr)"
  },
  {
    "objectID": "01_blog/2022_11_07_NBA-functions/index.html#team-statistics",
    "href": "01_blog/2022_11_07_NBA-functions/index.html#team-statistics",
    "title": "SportsObserveR - Part 1: Scraping Functions",
    "section": "1_1. Team Statistics:",
    "text": "1_1. Team Statistics:\nThe first function I’m creating scrapes team statistics, which will need the user to input the teams url slug, the year that team attended or attends the NBA playoffs, and the stats_tb or statistics table that corresponds to what is shown on Basketball Reference. Currently not all tables work, but it should work for: #per_game, #totals, #per_36_minutes, and #advanced.\n\nscrape_team_data <- function(slug, year, stats_tb){\n    \"\n  A function that returns a data frame of team statistics. \n  \n  @param slug is string of three letters that represents the teams url. \n  @param year is a string that corresponds to the NBA finals.\n  @param stats_tb is a string that corresponds to the statistics table on BasketBall Reference such as #per_game, #totals, #per_36_minutes, and #advanced\n  \n  @return a df of team statistics\n  \"\n  # define team page URL\n  url <- base::paste0(\"https://www.basketball-reference.com/teams/\",\n                slug,\"/\", year, \".html\")\n  \n  # Read stats table\n  stats_tb <- url %>%\n  read_html %>%\n  html_node(stats_tb) %>% \n  html_table()\n  \n  # Rename Column 2 to Name \n  base::names(stats_tb)[2] <- \"Name\"\n  \n  # Replace NA values with 0 (for stat functions)\n  stats_tb[base::is.na(stats_tb)] <- 0\n  \n  # make data frame\n  df <- base::data.frame(stats_tb)\n  base::return(df)\n  }\n\n\nExamples\n\nA. Current Blazers Roster\n\nzers_roster <- scrape_team_data(\"POR\",\"2022\",\"#roster\")\nutils::head(zers_roster)\n\n  No.              Name Pos  Ht  Wt         Birth.Date Var.7 Exp\n1  21    Keljin Blevins  SF 6-4 200  November 24, 1995    us   1\n2   4    Greg Brown III  SF 6-9 205  September 1, 2001    us   R\n3  33  Robert Covington  PF 6-7 209  December 14, 1990    us   8\n4  34 Jarron Cumberland  SG 6-5 205 September 22, 1997    us   R\n5  18         Kris Dunn  PG 6-3 205     March 18, 1994    us   5\n6  16         CJ Elleby  SF 6-6 200      June 16, 2000    us   1\n                       College\n1 Southern Miss, Montana State\n2                        Texas\n3              Tennessee State\n4                   Cincinnati\n5                   Providence\n6             Washington State\n\n\n\n\nB. 1997 Chicago Bulls Total Statistics\n\nbulls_totals <- scrape_team_data(\"CHI\", \"1998\", \"#totals\")\nutils::head(bulls_totals)\n\n  Rk           Name Age  G GS   MP  FG  FGA   FG. X3P X3PA  X3P. X2P X2PA  X2P.\n1  1 Michael Jordan  34 82 82 3181 881 1893 0.465  30  126 0.238 851 1767 0.482\n2  2  Dennis Rodman  36 80 66 2856 155  360 0.431   4   23 0.174 151  337 0.448\n3  3     Ron Harper  34 82 82 2284 293  665 0.441  16   84 0.190 277  581 0.477\n4  4     Toni Kukoč  29 74 52 2235 383  841 0.455  63  174 0.362 320  667 0.480\n5  5    Luc Longley  29 58 58 1703 277  609 0.455   0    0 0.000 277  609 0.455\n6  6 Scottie Pippen  32 44 44 1652 315  704 0.447  61  192 0.318 254  512 0.496\n   eFG.  FT FTA   FT. ORB DRB  TRB AST STL BLK TOV  PF  PTS\n1 0.473 565 721 0.784 130 345  475 283 141  45 185 151 2357\n2 0.436  61 111 0.550 421 780 1201 230  47  18 147 238  375\n3 0.453 162 216 0.750 107 183  290 241 108  48  91 181  764\n4 0.493 155 219 0.708 121 206  327 314  76  37 154 149  984\n5 0.455 109 148 0.736 113 228  341 161  34  62 130 206  663\n6 0.491 150 193 0.777  53 174  227 254  79  43 109 116  841\n\n\nHere we can see when Michael Jordan won his 6th ring with the Chicago Bulls he was also the leagues leading point scorer with 2,357 total points that season. Dennis Rodman was also a league leader that season in rebounds collecting a total of 1,201 rebounds."
  },
  {
    "objectID": "01_blog/2022_11_07_NBA-functions/index.html#player-statistics",
    "href": "01_blog/2022_11_07_NBA-functions/index.html#player-statistics",
    "title": "SportsObserveR - Part 1: Scraping Functions",
    "section": "1_2. Player Statistics",
    "text": "1_2. Player Statistics\nThe second function will scrape player statistics. The user will need to input the players name, and the stats_tb or statistics table that corresponds to what is shown on Basketball Reference. Currently not all tables work, but it should work for: #per_game, #totals, #per_36_minutes, and #advanced.\n\nscrape_player_data <- function(name, stats_tb){\n  \"\n  A function that returns a data frame of player statistics. \n  \n  @param name is a string that represnets an NBA players name\n  @param stats_tb is a string that corresponds to the statistics table on BasketBall Reference such as #per_game, #totals, #per_36_minutes, and #advanced\n  \n  @return a df of player statistics\n  \"\n  # make name lower case\n  lower_case_name <- base::tolower(name)\n\n  # split name \n  split_name <- base::strsplit(lower_case_name, \" +\")[[1]]\n\n  # define first and last name\n  first_name <- split_name[[1]]\n  last_name <- split_name[[2]]\n  \n  # first letter of last name\n  letter <- base::substr(last_name, 1,1)\n  \n  # first five letters of last name \n  last_5 <- base::substr(last_name, 1, 5)\n  \n  # first two letters of first name\n  first_2 <- base::substr(first_name, 1,2)\n  \n  # define team page URL\n  url <- base::paste0(\"https://www.basketball-reference.com/players/\",letter ,\"/\",last_5,first_2,\"01.html\")\n  \n  # Read stats table\n  stats_tb <- url %>%\n  read_html %>%\n  html_node(stats_tb) %>% \n  html_table()\n  \n  # Rename Column 2 to Name \n  names(stats_tb)[2] <- \"Name\"\n  \n  # Replace NA values with 0 (for stat functions)\n  stats_tb[base::is.na(stats_tb)] <- 0\n  \n  # make list a dataframe\n  df <- base::data.frame(stats_tb)\n  \n  base::return(df)\n  }\n\n\nExamples\n\nC. Allen Iverson Per Game Stats\n\nai_per_game <- scrape_player_data(\"Allen Iverson\", \"#per_game\")\nhead(ai_per_game)\n\n   Season Name  Tm  Lg Pos  G GS   MP   FG  FGA  FG. X3P X3PA X3P. X2P X2PA\n1 1996-97   21 PHI NBA  PG 76 74 40.1  8.2 19.8 .416 2.0  6.0 .341 6.2 13.8\n2 1997-98   22 PHI NBA  PG 80 80 39.4  8.1 17.6 .461 0.9  2.9 .298 7.2 14.7\n3 1998-99   23 PHI NBA  SG 48 48 41.5  9.1 22.0 .412 1.2  4.1 .291 7.9 17.9\n4 1999-00   24 PHI NBA  SG 70 70 40.8 10.4 24.8 .421 1.3  3.7 .341 9.1 21.0\n5 2000-01   25 PHI NBA  SG 71 71 42.0 10.7 25.5 .420 1.4  4.3 .320 9.4 21.2\n6 2001-02   26 PHI NBA  SG 60 59 43.7 11.1 27.8 .398 1.3  4.5 .291 9.8 23.4\n  X2P. eFG.  FT  FTA  FT. ORB DRB TRB AST STL BLK TOV  PF  PTS\n1 .448 .467 5.0  7.2 .702 1.5 2.6 4.1 7.5 2.1 0.3 4.4 3.1 23.5\n2 .494 .486 4.9  6.7 .729 1.1 2.6 3.7 6.2 2.2 0.3 3.1 2.5 22.0\n3 .440 .439 7.4  9.9 .751 1.4 3.5 4.9 4.6 2.3 0.1 3.5 2.0 26.8\n4 .435 .446 6.3  8.9 .713 1.0 2.8 3.8 4.7 2.1 0.1 3.3 2.3 28.4\n5 .441 .447 8.2 10.1 .814 0.7 3.1 3.8 4.6 2.5 0.3 3.3 2.1 31.1\n6 .419 .422 7.9  9.8 .812 0.7 3.8 4.5 5.5 2.8 0.2 4.0 1.7 31.4\n\n\nNotice that when Allen Iverson won the NBA’s MVP in 2001 he was putting up about 31 points a game.\n\n\nD. Kareem Abdul-Jabbar Totals\n\nkaj_totals <- scrape_player_data(\"Kareem Abdul-Jabbar\", \"#totals\")\nutils::head(kaj_totals)\n\n   Season Name  Tm  Lg Pos  G GS   MP   FG  FGA   FG. X3P X3PA X3P.  X2P X2PA\n1 1969-70   22 MIL NBA   C 82  0 3534  938 1810 0.518   0    0    0  938 1810\n2 1970-71   23 MIL NBA   C 82  0 3288 1063 1843 0.577   0    0    0 1063 1843\n3 1971-72   24 MIL NBA   C 81  0 3583 1159 2019 0.574   0    0    0 1159 2019\n4 1972-73   25 MIL NBA   C 76  0 3254  982 1772 0.554   0    0    0  982 1772\n5 1973-74   26 MIL NBA   C 81  0 3548  948 1759 0.539   0    0    0  948 1759\n6 1974-75   27 MIL NBA   C 65  0 2747  812 1584 0.513   0    0    0  812 1584\n   X2P.  eFG.  FT FTA   FT. ORB DRB  TRB AST STL BLK TOV  PF  PTS Var.31\n1 0.518 0.518 485 743 0.653   0   0 1190 337   0   0   0 283 2361      0\n2 0.577 0.577 470 681 0.690   0   0 1311 272   0   0   0 264 2596      0\n3 0.574 0.574 504 732 0.689   0   0 1346 370   0   0   0 235 2822      0\n4 0.554 0.554 328 460 0.713   0   0 1224 379   0   0   0 208 2292      0\n5 0.539 0.539 295 420 0.702 287 891 1178 386 112 283   0 238 2191      0\n6 0.513 0.513 325 426 0.763 194 718  912 264  65 212   0 205 1949      0\n  Trp.Dbl\n1       0\n2       1\n3       1\n4       2\n5       3\n6       1"
  },
  {
    "objectID": "01_blog/2022_11_07_NBA-functions/index.html#box-scores",
    "href": "01_blog/2022_11_07_NBA-functions/index.html#box-scores",
    "title": "SportsObserveR - Part 1: Scraping Functions",
    "section": "1_3. Box Scores",
    "text": "1_3. Box Scores\nThe last function still needs a bit of work, but will pull box scores of all the NBA games on a given day. The user will need to enter the game_day or day of the games they want box scores for.\nNote: Ideally this function would return a list with each game being its own df, but for now it only prints one data frame that includes all games played on that date. There also seem to be issues when only one game is played, or it is the first game of the season (see examples below), but for now those issues are manageable.\n\nbox_scores <- function(game_day){\n  \"\n  A function that returns a data frame of box scores. \n  \n  @param game_day is a string that represents the date in the form Y-M-D\n  \n  @return a df of box scores from that day.\n  \"\n  # split by dash\n  split_date <- base::strsplit(game_day, \"-\")\n  \n  # year - month - day \n  year <- split_date[[1]][[1]]\n  month <- split_date[[1]][[2]]\n  day <- split_date[[1]][[3]]\n  \n  #url\n  url <- base::paste0(\"https://www.basketball-reference.com/boxscores/?month=\",\n                month ,\"&day=\", day,\"&year=\", year)\n  \n  # read url\n  html <- read_html(url)\n  \n  # extract all the 'div\" items from the html as tables\n  div <- html %>% \n    html_elements(\"div\") %>% \n    html_table()\n  \n  #remove empties\n  div <- div[base::sapply(div, function(i) dim(i)[1]) > 0]\n  \n  # only keep rows == 7\n  div <- div[base::sapply(div, function(i) nrow(i)[1]) == 7]\n  \n  # empty list\n  my_vec <- base::list()\n  \n  #for loop\n  for(i in 1:base::length(div)) {        \n  my_out <- div[[i]][3:5,] \n  my_vec <- c(my_vec, my_out)\n  df <- base::data.frame(my_vec)\n  }\n  \n  df <- df[-1,]\n  \n  base::return(df)\n}\n\n\nExample\n\nE. Box Scores for 10-19-2022 (works correctly)\n\noct_19 <- box_scores(\"2022-10-19\")\noct_19\n\n       X1 X2 X3 X4 X5        X1.1 X2.1 X3.1 X4.1 X5.1    X1.2 X2.2 X3.2 X4.2\n2 Houston 20 30 30 27 New Orleans   32   26   40   32 Orlando   28   27   28\n3 Atlanta 26 33 25 33    Brooklyn   14   36   28   30 Detroit   17   40   34\n  X5.2       X1.3 X2.3 X3.3 X4.3 X5.3     X1.4 X2.4 X3.4 X4.4 X5.4 X6    X1.5\n2   26 Washington   36   24   27   27 New York   23   23   33   29  4 Chicago\n3   22    Indiana   25   27   25   30  Memphis   25   36   24   23  7   Miami\n  X2.5 X3.5 X4.5 X5.5          X1.6 X2.6 X3.6 X4.6 X5.6    X1.7 X2.7 X3.7 X4.7\n2   28   31   37   20 Oklahoma City   22   30   35   21  Dallas   32   30   19\n3   33   26   27   22     Minnesota   35   30   22   28 Phoenix   24   21   31\n  X5.7       X1.8 X2.8 X3.8 X4.8 X5.8        X1.9 X2.9 X3.9 X4.9 X5.9     X1.10\n2   24   Portland   32   19   33   31   Charlotte   38   30   30   31 Cleveland\n3   31 Sacramento   23   32   29   24 San Antonio   22   25   28   27   Toronto\n  X2.10 X3.10 X4.10 X5.10  X1.11 X2.11 X3.11 X4.11 X5.11\n2    22    35    27    21 Denver    30    23    27    22\n3    28    23    25    32   Utah    37    38    19    29\n\n\n\n\nF. Box scores for the first day of the ’22/’23 NBA season (issues)\n\noct_18 <- box_scores(\"2022-10-18\")\noct_18\n\n            X1 X2 X3 X4 X5         X1.1 X2.1 X3.1 X4.1 X5.1               X1.2\n2 Philadelphia 29 34 25 29    LA Lakers   22   30   19   38 Philadelphia 76ers\n3       Boston 24 39 35 28 Golden State   25   34   32   32 Western Conference\n  X2.2 X3.2 X4.2 X5.2    X6    X7   X8   X9 X10 X11 X12  X13 X14 X15  X16 X17\n2    0    1 .000  1.0 117.0 126.0 <NA> <NA>  NA  NA  NA <NA>  NA  NA <NA>  NA\n3    W    L W/L%   GB  PS/G  PA/G <NA> <NA>  NA  NA  NA <NA>  NA  NA <NA>  NA\n  X18 X19 X20 X21 X22  X23  X24  X25  X26  X27  X28  X29  X30  X31 X32 X33 X34\n2  NA  NA  NA  NA  NA <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>  NA  NA  NA\n3  NA  NA  NA  NA  NA <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>  NA  NA  NA\n   X35 X36 X37  X38 X39 X40 X41 X42 X43 X44\n2 <NA>  NA  NA <NA>  NA  NA  NA  NA  NA  NA\n3 <NA>  NA  NA <NA>  NA  NA  NA  NA  NA  NA\n\n\nIssue: For the first game of the season there is an are NA tables that are being pulled in.\nG. First game of the 1992 NBA Finals AKA Michael Jordan’s famous Shrug (issues)\n\nfinals_92_g1 <- box_scores(\"1992-6-3\")\nfinals_92_g1\n\n        X1 X2 X3 X4 X5     X1.1 X2.1 X3.1 X4.1 X5.1\n2 Portland 30 21 17 21 Portland   30   21   17   21\n3  Chicago 33 33 38 18  Chicago   33   33   38   18\n\n\nIssue: For days where only one game is played the one game is printed twice in the data frame."
  },
  {
    "objectID": "01_blog/2021_11_22_Skewness/index.html",
    "href": "01_blog/2021_11_22_Skewness/index.html",
    "title": "Skewness",
    "section": "",
    "text": "Physical Interpretation\nImagine your body is a symmetrical bell curve where your neck is your mode, waist is your median, hips are your mean, and are all stacked on top of each other to create a symmetrical bell curve as shown in the image below.\n\nWhen the hips are to the the right of your neck then you are creating a right skew. When you hips are to the left of your neck then your body bell curve is left skewed as shown in the diagram below.\n\nShakira famously said her hips don’t lie, but my hips dictate skewness."
  },
  {
    "objectID": "01_blog/2022_04_18_Tidy-Spice/index.html",
    "href": "01_blog/2022_04_18_Tidy-Spice/index.html",
    "title": "Tidy Spice",
    "section": "",
    "text": "This post is my reproduction of Julia Silge’s blogpost Topic Modeling for #TidyTuesday Spice Girls Lyrics, with some added inspiration from a blogpost by Ariane Aumaitre called Tutorial: Text analysis and data visualization with Taylor Swift songs."
  },
  {
    "objectID": "01_blog/2022_04_18_Tidy-Spice/index.html#gamma-matrix",
    "href": "01_blog/2022_04_18_Tidy-Spice/index.html#gamma-matrix",
    "title": "Tidy Spice",
    "section": "Gamma Matrix",
    "text": "Gamma Matrix\nGamma Matricies\n\nsong_topics <- tidytext::tidy(topic_model,\n                              matrix = \"gamma\",\n                              document_names = base::rownames(lyrics_sparse)\n)\nsong_topics\n\n# A tibble: 124 × 3\n   document                   topic    gamma\n   <chr>                      <int>    <dbl>\n 1 2 Become 1                     1 0.932   \n 2 Denying                        1 0.00154 \n 3 Do It                          1 0.996   \n 4 Get Down With Me               1 0.300   \n 5 Goodbye                        1 0.000971\n 6 Holler                         1 0.00155 \n 7 If U Can't Dance               1 0.000896\n 8 If You Wanna Have Some Fun     1 0.0171  \n 9 Last Time Lover                1 0.140   \n10 Let Love Lead the Way          1 0.00178 \n# … with 114 more rows\n\n\n\nsong_topics %>%\n  dplyr::mutate(\n    song_name = fct_reorder(document, gamma),\n    topic = base::factor(topic)\n  ) %>%\n  ggplot2::ggplot(ggplot2::aes(gamma, topic, fill = topic)) +\n  ggplot2::geom_col(show.legend = FALSE) +\n  ggplot2::facet_wrap(vars(song_name), ncol = 4) +\n  ggplot2::scale_x_continuous(expand = c(0, 0)) +\n  ggplot2::labs(x = base::expression(gamma), y = \"Topic\")"
  },
  {
    "objectID": "01_blog/2023_01_30_Flashcards/index.html",
    "href": "01_blog/2023_01_30_Flashcards/index.html",
    "title": "Flashcards",
    "section": "",
    "text": "Introduction\nI’ve recently been applying to and partaking in interviews for remote data science positions all over the country. The last couple of first round technical interviews I went on I was asked a range of questions related to data science, statistics, machine learning, probability, linear algebra, and mathematics. To test myself on answering these types of questions more confidently I created these definitions “Flashcards”, which are actually tabset panels, a component layout in quarto.\nEach definition can be viewed by clicking on the “Definition” tab for each word. All words are sorted alphabetically, definitions are generally casual, words will be continually added, and regularly updating.\n\n\nA\n\nA/B TestingDefinition\n\n\n\n\n\nTo compare two versions of something, usually a control (A) and a test variable (B).\n\n\n\n\n\nB\n\nBayes TheoremDefinition\n\n\n\n\n\nA method for calculating conditional probability, or the likelihood of one event occurring based on prior knowledge of conditions that might be related to the event.\n\\[P(A|B)=\\frac{P(A|B)P(A)}{P(B)}=\\frac{\\text{(likelihood)}\\times\\text{(prior)}}{\\text{(evidence)}}\\]\n\n\n\n\nBiasDefinition\n\n\n\n\n\nWhen a model or statistic doesn’t provide a true representation of the population.\n\\[bias=\\mathbb{E}[f'(x)]-f(x)\\]\nBias of the estimated function tells us the capacity of the underlying model to predict the values.\nHigh bias = overly-simplified model, under-fitting, high error on both testing and training data.\n\n\n\n\nBinomial PropertyDefinition\n\n\n\n\n\nThe probability of exactly x successes on n repeated trials in an experiment which has two possible outcomes. \\[P_x={n\\choose x}p^xq^{n-x}\\]\n\n\n\n\n\nC\n\nCategorical DataDefinition\n\n\n\n\n\nData that can be divided into groups or categories such as sex, race, and age.\n\n\n\n\nConditional ProbabilityDefinition\n\n\n\n\n\nThe probability of an event (A) given that another event (B) has already occurred.\n\\[P(A|B)=P(A\\cap B)P(B)\\]\n\n\n\n\nConfusion (Error) MatrixDefinition\n\n\n\n\n\nA technique for summarizing performance measurement for machine learning classification algorithms that makes it easy to see whether the system is confusing classes.\n\n\n\n\n\nD\n\nData ClassificationDefinition\n\n\n\n\n\nOrganizing data by relevant categories according to predefined criteria so that it may be used and protected more efficiently.\n\n\n\n\nData LeakageDefinition\n\n\n\n\n\nWhen information outside the training data is used to create the model.\n\n\n\n\nData ScienceDefinition\n\n\n\n\n\nStudying data to find insight using computer science, mathematics, and statistics.\n\n\n\n\nDatabaseDefinition\n\n\n\n\n\nOrganized collection of data.\n\n\n\n\nDecision TreeDefinition\n\n\n\n\n\nA flowchart that starts with one main idea or question and branches out with potential outcomes of each decision using classification and regression techniques.\n\n\n\n\nDerivativeDefinition\n\n\n\n\n\nRate of change.\n\n\n\n\nDescriptive StatisticsDefinition\n\n\n\n\n\nDescribes features and summaries of data such as mean, and variance.\n\n\n\n\nDeterminateDefinition\n\n\n\n\n\nA scalar function made up of the entries of a square matrix. It is used to find the inverse of a matrix, and has a lot of important properties related to systems of linear equations.\n\\[\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}=ad-bc\\]\n\n\n\n\nDimensionality ReductionDefinition\n\n\n\n\n\nThe technique of reducing the amount of random variables (or features) while retaining as much information as possible. This is done to reduce complexity, improve performance, and make the data easier to visualize.\n\n\n\n\nDiscrete MathematicsDefinition\n\n\n\n\n\nMathematics that deals with distinct, separate values instead of continuous values.\n\n\n\n\n\nE\n\nEigenvalueDefinition\n\n\n\n\n\nA scalar that is used to transform an eigenvalue, and considered as a factor by which it is stretched. Often denoted by \\(\\lambda\\).\n\n\n\n\nEigenvectorDefinition\n\n\n\n\n\nAre non-zero vectors that do not change direction when any linear transformation is applied.\n\n\n\n\n\nG\n\nGamma FunctionDefinition\n\n\n\n\n\nA generalization of the factorial function, it is commonly used to estimate new data points based on known values.\n\\[\\Gamma(x)=(n-1)!\\]\n\\[\\Gamma(z)=\\int_0^\\infty t^{z-1}e^{-t}dt\\]\n\n\n\n\n\nH\n\nHomogenousDefinition\n\n\n\n\n\nAn equation that contains itself, or one of its derivatives.\n\\[f(zx,zy)=z^n f(x,y)\\]\n\n\n\n\nHypothesis TestingDefinition\n\n\n\n\n\nTesting a hypothesis and comparing it against the null.\n\n\n\n\n\nI\n\nInferential StatisticsDefinition\n\n\n\n\n\nUsed to make predictions about population or data.\n\n\n\n\nInterval DataDefinition\n\n\n\n\n\nData that is measured along a scale, where each point is placed at equal distance from one another. Examples would be temperature, or SAT scores.\n\n\n\n\n\nK\n\nK-Mean ClusteringDefinition\n\n\n\n\n\nAn unsupervised learning algorithm which groups an unlabeled dataset into clusters with similar properties such as mean. An example might be to group similar customers and then to target them using different types of marketing.\n\n\n\n\n\nL\n\nLaw of Large NumbersDefinition\n\n\n\n\n\nAs the sample size increases the mean gets closer to the average of the population.\n\n\n\n\nLinear RegressionDefinition\n\n\n\n\n\nUses a liner approach to modeling the relationship between regressor (predictor) variables \\(x\\) and a response variable \\(y\\).\n\\[y=\\beta_0+\\beta_1x+\\epsilon\\]\n\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_nx_n+\\epsilon\\]\n\n\n\n\n\nM\n\nMachine LearningDefinition\n\n\n\n\n\nA method that uses algorithms to build models to make predictions or decisions.\n\n\n\n\nMatrixDefinition\n\n\n\n\n\nA rectangular array of numbers arranged in rows and columns which represent a mathematical expression.\n\n\n\n\n\nN\n\nNeural NetworkDefinition\n\n\n\n\n\nA type of artificial intelligence that uses connected nodes which loosely model the neurons in the brain. Each node, also known as a neuron, is connected by what is called an edge. Both neurons and edges have a weight that adjusts as the model learns, and can increase or decrease the strength of the signal which travels from the first layer (input) to the last layer (output).\n\n\n\nA neural network showing nodes connected by edges, an input layer, hidden layers, and an output layer.\n\n\n\n\n\n\nNominal DataDefinition\n\n\n\n\n\nIs categorical data that groups variables into labeled categories that do not overlap, and cannot be ranked. Nominal data needs to be grouped to be analyzed. Examples would be sex or race.\n\n\n\n\n\nO\n\nOrdinal DataDefinition\n\n\n\n\n\nIs categorical data that has an order or ranking system such as education level, economic status, or satisfaction rating.\n\n\n\n\nOverfittingDefinition\n\n\n\n\n\nWhen machine learning models fit exactly to the training model, and therefore may fail to predict future observations.\n\n\n\n\n\n\nR\n\nRandom Forest ModelDefinition\n\n\n\n\n\nA classification algorithm that consists of many decision trees, and can correct decision trees’ habit of overfitting to their training set.\n\n\n\n\nRatio DataDefinition\n\n\n\n\n\nIs quantitative data that has a true zero such as speed, age, and weight.\n\n\n\n\nRegularizationDefinition\n\n\n\n\n\nA technique to reduce the errors of overfitting by adding extra information.\n\n\n\n\n\nS\n\nSnowflake and Start SchemeDefinition\n\n\n\n\n\nBoth are logical arrangements of a multidimensional database, where the fact table is in the middle of the structure, and it is surrounded by dimension tables. A snowflake scheme has normalized dimension tables meaning there are sub-dimensional tables, whereas a star schema is denormalized and easier to query since there are fewer joins between tables.\n\n\nStar Schema\n\n\nSnowflake Schema\n\n\n\n\n\n\n\nStationary ProcessDefinition\n\n\n\n\n\nA type of stochastic (random) process whose joint probability distribution does not change over time. An example would be white noise.\n\n\n\n\nStatisticsDefinition\n\n\n\n\n\nApplied math used to study data to form a judgment in a case of real world applications.\n\n\n\n\nSystems of Linear EquationsDefinition\n\n\n\n\n\nTwo or more linear equations working together.\n\n\n\n\n\nT\n\nTraining and Test DataDefinition\n\n\n\n\n\nTraining data is a subset of the original data which is used to train machine learning models.\nTest data is another subset of the original data which is independent of the training data, and used to test the accuracy of the model.\n\n\n\n\nTransformationDefinition\n\n\n\n\n\nA linear mapping between two vector spaces that preserves the operations of vector addition and scalar multiplication.\n\n\n\n\nType 1 and Type 2 ErrorDefinition\n\n\n\n\n\nType 1 error is a false positive (rejects the null which is actually true), and a type 2 error is a false negative (fails to reject the null which is actually false).\n\n\n\nLeft image reads “Type 1 error (false positive)”, and shows a doctor telling a man he is pregnant. Right image reads “Type 2 error (false negative)”, and shows a doctor telling a pregnant patient “You’re not pregnant”.\n\n\n\n\n\n\n\nU\n\nUneven or Unbalanced DataDefinition\n\n\n\n\n\nWhen the target variable has more observations in a specific class than the others. It would not be a good idea to use accuracy as a performance measure for highly imbalanced data."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "TidyTuesday is a weekly data analysis and visualization challenge that provides a structured format to practice data wrangling and visualization skills using R programming language. Each week, a new dataset is posted to the TidyTuesday Github repository and participants are encouraged to explore and visualize the data using the principles of tidy data. Continue reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid Dashboard is an interactive analysis tool that specifically examines Covid-19 cases and fatalities in both the United States and my home state of Oregon. What initially began as a class project at the onset of the pandemic, has undergone multiple iterations and refinements to evolve into a fully functional and informative dashboard. Continue reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFour different projects from Statistics 363: Introduction to R, which includes analysis of NYC Flight Data, investigating if Kobe Bryant had “hot hands”, comparing Gas and Hydro power using base R and ggplot2, and exploring states with the highest and lowest numbers of Covid cases. Continue reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA state known for growing all varieties of trees and greenery, it’s no wonder that Oregon was one of the first states to legalize recreational cannabis in 2014. Measure 91 changed the lives of both those affiliated with cannabis and those who benefit from the tax revenue it brings in. While many businesses and industries struggled in 2020 due to COVID-19 the cannabis industry kept growing. Continue reading"
  },
  {
    "objectID": "projects.html#math-338",
    "href": "projects.html#math-338",
    "title": "Randi Bolt",
    "section": "Math 338",
    "text": "Math 338\nNotes from Rebecca Tramel’s Modern College Geometry Winter 2022 class. Topics include axioms, proof writing, Euclid’s Elements, congruence, isometries, triangles, area proofs, circles, group theory for symmetries, Euclidean distance, taxi-cab geometry, spherical geometry, hyperbolic geometry, … Continue reading"
  },
  {
    "objectID": "projects.html#math-344",
    "href": "projects.html#math-344",
    "title": "Randi Bolt",
    "section": "Math 344",
    "text": "Math 344\nNotes from Julie Bracken’s Introduction to Group Theory Winter 2022 class. Topics include set theory, cancellation law, subgroups, cyclic subgroups, injecteive, surjective, bijective, function composition, permutation groups, symmetric groups, dihedral group, cycle decomposition, parity of transposistions, alternating groups, isomorphism, partitions, Lagrange’s Theorem, homomorphism, normal subgroups, kernal, … Continue reading"
  },
  {
    "objectID": "projects.html#stat-451",
    "href": "projects.html#stat-451",
    "title": "Randi Bolt",
    "section": "Stat 451",
    "text": "Stat 451\nNotes from Subash Kochar’s Applied Statistics for Engineers and Scientists Summer 2021 class. Topics include discrete and continous random various, bernoulli random variables, pmf, pdf, expeced value, variance, mean, standard deviation, probablity distributions: Poisson, binomial, uniform, normal, exponential, gamma, chi-squared, Weibull, lognormal, …Continue reading"
  },
  {
    "objectID": "projects.html#stat-461",
    "href": "projects.html#stat-461",
    "title": "Randi Bolt",
    "section": "Stat 461",
    "text": "Stat 461\nNotes from Dorcas Ofori-Boaten’s Introduction to Mathematical Statistics I Fall 2021 class. Topics include mean, median, mode, variance, standard deviation, Empirical Rule, set theory, DeMorgans Law, Distributive Law, counting rules, statistical independence, mutual independence, total law of probability, Bayes Rule, discrete and continuous random variables, probability distribution, expected value, CDF, PMF, PDF, … Continue reading"
  },
  {
    "objectID": "projects.html#stat-462",
    "href": "projects.html#stat-462",
    "title": "Randi Bolt",
    "section": "Stat 462",
    "text": "Stat 462\nNotes from Nadee Jayasena’s Introduction ot Mathematical Statistics II Winter 2022 class. Topics include marginal PDF’s, joint PDF’s and CDF’s, independence of random variables, bivariate probability, conditional distribution, conditional covariance, correlation, univariate transformations, bivariate transformations, Jacobians, method of moment-generating functions, sampling distributions, central limit theorem, … Continue reading"
  },
  {
    "objectID": "projects.html#stat-464",
    "href": "projects.html#stat-464",
    "title": "Randi Bolt",
    "section": "Stat 464",
    "text": "Stat 464\nNotes from Ge Shao’s Applied Regression Analysis Fall 2021 class. Topics include simple linear regression, least-square estimates, hypothesis testing, t-stat, p-stat, sum of squares estimates, mean square error, confidence interval, R squared, multiple linear models, hat matrix, multilinearity, model adequacy checking, residual analysis, PRESS statistic, detecting outliers, correcting model inadequacies, stabilizing variance, DIFFTS, DFBETAS, COVRATIO, …Continue reading"
  },
  {
    "objectID": "projects.html#rbolt22-2022",
    "href": "projects.html#rbolt22-2022",
    "title": "Randi Bolt",
    "section": "rbolt22 (2022)",
    "text": "rbolt22 (2022)\nMade with Quarto … Continue reading"
  },
  {
    "objectID": "projects.html#rbolt2-2022",
    "href": "projects.html#rbolt2-2022",
    "title": "Randi Bolt",
    "section": "rbolt2 (2022)",
    "text": "rbolt2 (2022)\nMade with R using blogdown and deployed with Netlify. Topics include NBA circular barcharts, reproducible research, docker, Shiny Style Guide notes, finals and midterm preparation notes, … Continue reading"
  },
  {
    "objectID": "projects.html#rbolt-2021",
    "href": "projects.html#rbolt-2021",
    "title": "Randi Bolt",
    "section": "rbolt (2021)",
    "text": "rbolt (2021)\nMade with R using blogdown and deployed with Netlify. Topics include using API’s, generative art, idempotent proof, abstract mathematics, transformation and weighting to correct model inadequacies, diagnostics for leverage and influence, …Continue reading"
  },
  {
    "objectID": "projects.html#rbolt.me-2023",
    "href": "projects.html#rbolt.me-2023",
    "title": "Randi Bolt",
    "section": "rbolt.me (2023)",
    "text": "rbolt.me (2023)\nCurrent iteration of my blog. Made using Quarto and deployed with Netlify. Topics include interview preparation, R and SQL, creating a package in R, web scraping, python basics, quarto links, … Continue reading"
  },
  {
    "objectID": "projects.html#covid-analysis",
    "href": "projects.html#covid-analysis",
    "title": "Randi Bolt",
    "section": "Covid Analysis",
    "text": "Covid Analysis\n\nThe Coronavirus is an infectious disease, that shut down the United states on March 15th, 2020 and shut down Oregon on March 23, 2020. While most of the state, county, and world have reopened, Covid still appears to be sticking around longer than expected. This study uses two data sets to look at the Covid both across different states, as well as different counteis in my home state of Oregon… Continue reading"
  },
  {
    "objectID": "projects.html#oregon-grown",
    "href": "projects.html#oregon-grown",
    "title": "Randi Bolt",
    "section": "Oregon Grown",
    "text": "Oregon Grown\n\nA state known for growing all varieties of trees and greenery, it’s no wonder that Oregon was one of the first states to legalize recreational cannabis in 2014. Measure 91 changed the lives of both those affiliated with cannabis and those who benefit from the tax revenue it brings in. While many businesses and industries struggled in 2020 due to COVID-19 the cannabis industry kept growing.\nContinue reading"
  },
  {
    "objectID": "projects.html#rsite",
    "href": "projects.html#rsite",
    "title": "Randi Bolt",
    "section": "Rsite",
    "text": "Rsite\n\nFour different projects from Statistics 363: Introduction to R, which includes analysis of NYC Flight Data, investigating if Kobe Bryant had “hot hands”, comparing Gas and Hydro power using base R and ggplot2, and exploring states with the highest and lowest numbers of Covid cases. Continue reading"
  },
  {
    "objectID": "01_blog/Untitled/index.html#exploritroy-data-analysis",
    "href": "01_blog/Untitled/index.html#exploritroy-data-analysis",
    "title": "The Joy of Analysis",
    "section": "Exploritroy Data Analysis",
    "text": "Exploritroy Data Analysis\n\nAbout “The Joy of Painting”:\n\nThere are 31 seasons.\nThere are 13 episodes per season.\nThere are 403 total episodes.\n\n\n\nShow Code\nepisodes_per_season <- bob_ross %>%\n  dplyr::group_by(season) %>%\n  dplyr::summarise(total_episodes = n()) \n\nepisodes_per_season %>% base::t()\n\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason            1    2    3    4    5    6    7    8    9    10    11    12\ntotal_episodes   13   13   13   13   13   13   13   13   13    13    13    13\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason            13    14    15    16    17    18    19    20    21    22\ntotal_episodes    13    13    13    13    13    13    13    13    13    13\n               [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason            23    24    25    26    27    28    29    30    31\ntotal_episodes    13    13    13    13    13    13    13    13    13\n\n\n\n\nColors used:\n\n138 colors were used on average each season.\n92 was the least amount of colors used in a single season.\n156 was the most amount of colors used in a single season.\n\n\n\nShow Code\nbr_colors <- bob_ross %>%\n  group_by(season) %>%\n  summarise(total_num_colors = sum(num_colors)) \n\nmean_colors <- mean(br_colors$total_num_colors)\nmin_colors <- min(br_colors$total_num_colors)\nmax_colors <- max(br_colors$total_num_colors)\n\nbr_colors %>% t()\n\n\n                 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason              1    2    3    4    5    6    7    8    9    10    11    12\ntotal_num_colors   92  145  128  130  111  136  122  138  140   148   128   144\n                 [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason              13    14    15    16    17    18    19    20    21    22\ntotal_num_colors   137   156   147   134   152   147   145   146   146   151\n                 [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason              23    24    25    26    27    28    29    30    31\ntotal_num_colors   141   138   145   136   131   132   137   149   142\n\n\nShow Code\nggplot(br_colors, aes(x = season, y = total_num_colors)) +\n  geom_col()\n\n\n\n\n\n\n\nSeasonal Color Trends:\n\n\nShow Code\nbr_subset <- bob_ross %>%\n  group_by(season) %>%\n  summarise(black_gesso = sum(Black_Gesso),\n            bright_red = sum(Bright_Red),\n            burnt_umber = sum(Burnt_Umber),\n            carmium_yellow = sum(Cadmium_Yellow),\n            dark_sienna = sum(Dark_Sienna),\n            indian_red = sum(Indian_Red),\n            indian_yellow = sum(Indian_Yellow),\n            liquid_black = sum(Liquid_Black),\n            liquid_clear = sum(Liquid_Clear),\n            midnight_black = sum(Midnight_Black),\n            phthalo_blue = sum(Phthalo_Blue),\n            phthalo_green = sum(Phthalo_Green),\n            prussian_blue = sum(Prussian_Blue),\n            sap_green = sum(Sap_Green),\n            titanium_white = sum(Titanium_White),\n            van_dyke_brown = sum(Van_Dyke_Brown),\n            yellow_orchre = sum(Yellow_Ochre),\n            alizarian_crimson = sum(Alizarin_Crimson)) %>%\n  pivot_longer(!season, names_to = \"color\", values_to = \"count\")\n\nhtml <- c(\n  \"#000000\", \"#DB0000\", \"#8A3324\",\n  \"#FFEC00\", \"#5F2E1F\", \"#CD5C5C\", \n  \"#FFB800\", \"#000000\", \"#FFFFFF\",\n  \"#000000\", \"#0C0040\", \"#102E3C\",\n  \"#021E44\", \"#0A3410\", \"#FFFFFF\",\n  \"#221B15\", \"#C79B00\", \"#4E1500\")\n  \nggplot(br_subset, aes(x = season, y = count)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(vars(color))\n\n\n\n\n\n\nbob_ross %>%\n  filter(Indian_Red == 1) %>%\n  summarise(episode = episode,\n            season = season)\n\n# A tibble: 1 × 2\n  episode season\n    <dbl>  <dbl>\n1       1     22\n\n\n\ntest <- bob_ross[\"color_hex\"][[1,1]]\ntest <- str_split(test, \"', '\")\ntest\n\n[[1]]\n[1] \"['#4E1500\" \"#DB0000\"   \"#FFEC00\"   \"#102E3C\"   \"#021E44\"   \"#0A3410\"  \n[7] \"#FFFFFF\"   \"#221B15']\"\n\n\n\n# base <- crayola |> \n#   dplyr::mutate(\n#     color = forcats::fct_reorder(color, hue2)\n#   ) |> \n#   ggplot(aes(\n#     x = year,\n#     group = color,\n#     fill = color\n#   )) +\n#   scale_fill_identity() +\n#   NULL\n# \n# base + geom_bar(show.legend = FALSE)"
  },
  {
    "objectID": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#exploritroy-data-analysis",
    "href": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#exploritroy-data-analysis",
    "title": "The Joy of Analysis",
    "section": "Exploritroy Data Analysis",
    "text": "Exploritroy Data Analysis\n\nAbout “The Joy of Painting”:\n\nThere are 31 seasons.\nThere are 13 episodes per season.\nThere are 403 total episodes.\n\n\n\nShow Code\nepisodes_per_season <- bob_ross %>%\n  dplyr::group_by(season) %>%\n  dplyr::summarise(total_episodes = n()) \n\nepisodes_per_season %>% base::t()\n\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason            1    2    3    4    5    6    7    8    9    10    11    12\ntotal_episodes   13   13   13   13   13   13   13   13   13    13    13    13\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason            13    14    15    16    17    18    19    20    21    22\ntotal_episodes    13    13    13    13    13    13    13    13    13    13\n               [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason            23    24    25    26    27    28    29    30    31\ntotal_episodes    13    13    13    13    13    13    13    13    13\n\n\n\n\nColors used:\n\n138 colors were used on average each season.\n92 was the least amount of colors used in a single season.\n156 was the most amount of colors used in a single season.\n\n\n\nShow Code\nbr_colors <- bob_ross %>%\n  group_by(season) %>%\n  summarise(total_num_colors = sum(num_colors)) \n\nmean_colors <- mean(br_colors$total_num_colors)\nmin_colors <- min(br_colors$total_num_colors)\nmax_colors <- max(br_colors$total_num_colors)\n\nbr_colors %>% t()\n\n\n                 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason              1    2    3    4    5    6    7    8    9    10    11    12\ntotal_num_colors   92  145  128  130  111  136  122  138  140   148   128   144\n                 [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason              13    14    15    16    17    18    19    20    21    22\ntotal_num_colors   137   156   147   134   152   147   145   146   146   151\n                 [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason              23    24    25    26    27    28    29    30    31\ntotal_num_colors   141   138   145   136   131   132   137   149   142\n\n\nShow Code\nggplot(br_colors, aes(x = season, y = total_num_colors)) +\n  geom_col()\n\n\n\n\n\n\n\nSeasonal Color Trends:\n\n\nShow Code\nbr_subset <- bob_ross %>%\n  group_by(season) %>%\n  summarise(black_gesso = sum(Black_Gesso),\n            bright_red = sum(Bright_Red),\n            burnt_umber = sum(Burnt_Umber),\n            carmium_yellow = sum(Cadmium_Yellow),\n            dark_sienna = sum(Dark_Sienna),\n            indian_red = sum(Indian_Red),\n            indian_yellow = sum(Indian_Yellow),\n            liquid_black = sum(Liquid_Black),\n            liquid_clear = sum(Liquid_Clear),\n            midnight_black = sum(Midnight_Black),\n            phthalo_blue = sum(Phthalo_Blue),\n            phthalo_green = sum(Phthalo_Green),\n            prussian_blue = sum(Prussian_Blue),\n            sap_green = sum(Sap_Green),\n            titanium_white = sum(Titanium_White),\n            van_dyke_brown = sum(Van_Dyke_Brown),\n            yellow_orchre = sum(Yellow_Ochre),\n            alizarian_crimson = sum(Alizarin_Crimson)) %>%\n  pivot_longer(!season, names_to = \"color\", values_to = \"count\")\n\nhtml <- c(\n  \"#000000\", \"#DB0000\", \"#8A3324\",\n  \"#FFEC00\", \"#5F2E1F\", \"#CD5C5C\", \n  \"#FFB800\", \"#000000\", \"#FFFFFF\",\n  \"#000000\", \"#0C0040\", \"#102E3C\",\n  \"#021E44\", \"#0A3410\", \"#FFFFFF\",\n  \"#221B15\", \"#C79B00\", \"#4E1500\")\n  \nggplot(br_subset, aes(x = season, y = count)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(vars(color))\n\n\n\n\n\n\nbob_ross %>%\n  filter(Indian_Red == 1) %>%\n  summarise(episode = episode,\n            season = season)\n\n# A tibble: 1 × 2\n  episode season\n    <dbl>  <dbl>\n1       1     22\n\n\n\ntest <- bob_ross[\"color_hex\"][[1,1]]\ntest <- str_split(test, \"', '\")\ntest\n\n[[1]]\n[1] \"['#4E1500\" \"#DB0000\"   \"#FFEC00\"   \"#102E3C\"   \"#021E44\"   \"#0A3410\"  \n[7] \"#FFFFFF\"   \"#221B15']\"\n\n\n\n# base <- crayola |> \n#   dplyr::mutate(\n#     color = forcats::fct_reorder(color, hue2)\n#   ) |> \n#   ggplot(aes(\n#     x = year,\n#     group = color,\n#     fill = color\n#   )) +\n#   scale_fill_identity() +\n#   NULL\n# \n# base + geom_bar(show.legend = FALSE)"
  },
  {
    "objectID": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#about-the-joy-of-painting",
    "href": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#about-the-joy-of-painting",
    "title": "The Joy of Analysis",
    "section": "About “The Joy of Painting”:",
    "text": "About “The Joy of Painting”:\n\nAboutTable\n\n\n\n31 total seasons.\n13 episodes per season.\n403 total episodes.\n\nWhen we consider the use case above, and that a canvas costs about $3. Then a budge conscious user can assume their budge for paint canvas will be about $40 per season, and about $1,210 for all 403 episodes.\n\n\n\n\nShow Code\nepisodes_per_season <- bob_ross %>%\n  dplyr::group_by(season) %>%\n  dplyr::summarise(total_episodes = n()) \n\nepisodes_per_season %>% base::t()\n\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason            1    2    3    4    5    6    7    8    9    10    11    12\ntotal_episodes   13   13   13   13   13   13   13   13   13    13    13    13\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason            13    14    15    16    17    18    19    20    21    22\ntotal_episodes    13    13    13    13    13    13    13    13    13    13\n               [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason            23    24    25    26    27    28    29    30    31\ntotal_episodes    13    13    13    13    13    13    13    13    13"
  },
  {
    "objectID": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#colors-used",
    "href": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#colors-used",
    "title": "The Joy of Analysis",
    "section": "Colors used:",
    "text": "Colors used:\n\nColorsTableGraph\n\n\n\nOn average 138 colors were used each season.\nThe least amount of colors used in a single season was 92 in season 1.\nThe most amount of colors used in a single season was 156 in season 14.\n\n\n\n\n\nShow Code\nbr_colors <- bob_ross %>%\n  group_by(season) %>%\n  summarise(total_num_colors = sum(num_colors)) \n\nmean_colors <- mean(br_colors$total_num_colors)\nmin_colors <- min(br_colors$total_num_colors)\nmax_colors <- max(br_colors$total_num_colors)\n\nbr_colors %>% t()\n\n\n                 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason              1    2    3    4    5    6    7    8    9    10    11    12\ntotal_num_colors   92  145  128  130  111  136  122  138  140   148   128   144\n                 [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason              13    14    15    16    17    18    19    20    21    22\ntotal_num_colors   137   156   147   134   152   147   145   146   146   151\n                 [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason              23    24    25    26    27    28    29    30    31\ntotal_num_colors   141   138   145   136   131   132   137   149   142\n\n\n\n\n\n\nShow Code\nggplot(br_colors, \n       aes(x = season, \n           y = total_num_colors)) +\n  geom_col(fill = \"#6c464e\") +\n  ggtitle(\"Colors Used Each Season\") +\n  xlab(\"Season\") +\n  ylab(\"Colors Used\") +\n  theme(\n    plot.title = element_text(family = \"Times\"),\n    axis.title.x = element_text(family = \"Times\"),\n    axis.title.y = element_text(family = \"Times\")\n  )"
  },
  {
    "objectID": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html",
    "href": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html",
    "title": "The Joy of Analysis",
    "section": "",
    "text": "This post uses #TidyTuesday data to create a short analysis of the colors used during the popular television show, The Joy of Painting."
  },
  {
    "objectID": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#seasonal-color-trends",
    "href": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#seasonal-color-trends",
    "title": "The Joy of Analysis",
    "section": "Seasonal Color Trends:",
    "text": "Seasonal Color Trends:\n\nTrendsGraph\n\n\n\n\nBurnt Umber was only used in 55 episodes, and stopped being used episode 13 of season 6.\n\n\n\n\n\nBlaze of Color\n\n\n\n\n Indian Red  is the least used color, that was only featured once in episode 1 of season 22.\n\n\n\n\n\nAutumn Images\n\n\n\n\n\n\n\nShow Code\nbr_subset <- bob_ross %>%\n  group_by(season) %>%\n  summarise(black_gesso = sum(Black_Gesso),\n            bright_red = sum(Bright_Red),\n            burnt_umber = sum(Burnt_Umber),\n            carmium_yellow = sum(Cadmium_Yellow),\n            dark_sienna = sum(Dark_Sienna),\n            indian_red = sum(Indian_Red),\n            indian_yellow = sum(Indian_Yellow),\n            liquid_black = sum(Liquid_Black),\n            liquid_clear = sum(Liquid_Clear),\n            midnight_black = sum(Midnight_Black),\n            phthalo_blue = sum(Phthalo_Blue),\n            phthalo_green = sum(Phthalo_Green),\n            prussian_blue = sum(Prussian_Blue),\n            sap_green = sum(Sap_Green),\n            titanium_white = sum(Titanium_White),\n            van_dyke_brown = sum(Van_Dyke_Brown),\n            yellow_orchre = sum(Yellow_Ochre),\n            alizarian_crimson = sum(Alizarin_Crimson)) %>%\n  pivot_longer(!season, names_to = \"color\", values_to = \"count\")\n\nhtml <- c(\n  \"#000000\", \"#DB0000\", \"#8A3324\",\n  \"#FFEC00\", \"#5F2E1F\", \"#CD5C5C\", \n  \"#FFB800\", \"#000000\", \"#FFFFFF\",\n  \"#000000\", \"#0C0040\", \"#102E3C\",\n  \"#021E44\", \"#0A3410\", \"#FFFFFF\",\n  \"#221B15\", \"#C79B00\", \"#4E1500\")\n  \nggplot(br_subset, aes(x = season, y = count)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(vars(color))"
  },
  {
    "objectID": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#color-trends",
    "href": "01_blog/2023_02_21_TidyTuesday-Bob-Ross/index.html#color-trends",
    "title": "The Joy of Analysis",
    "section": "Color Trends:",
    "text": "Color Trends:\n\nTrendsGraph\n\n\n\n Indian Red  is the color that is least used. It was only featured once in season 22 episode 1 in the Painting Autumn Images shown below.\n\n\n\n\n\nAutumn Images is a painting of a mountain in front of big white clouds, and surrounded by trees.\n\n\n\n\n Burnt Umber  was only used in 55 episodes. The last painting it was used in was from season 6 episode 13 and is called Blaze of Color, shown below.\n\n\n\n\n\nBlaze of Color is a painting of a green and dark blue creek surrounded by yellow and light green grass, trees, and a vibrant blue, pink, yellow, orange, and red sunset.\n\n\n\n\n Dark Sienna  may have been used instead of Burnt umber. It can in the episode of the last season, season 31 episode 13, in a painting called Wilderness Day, shown below.\n\n\n\n\n\nWilderness Day is a painting of a dark green meadow that includes trees, bushes, and a misty mountain in the background.\n\n\n\n\n\n\n\nShow Code\na <- bob_ross %>%\n  filter(Burnt_Umber == 1) %>%\n  summarise(episode = episode,\n            season = season,\n            img_src = img_src,\n            painting_title = painting_title,\n            youtube_src = youtube_src)\n\nb <- bob_ross %>%\n  filter(Dark_Sienna == 1) %>%\n  summarise(episode = episode,\n            season = season,\n            img_src = img_src,\n            painting_title = painting_title,\n            youtube_src = youtube_src)\n\nbr_subset <- bob_ross %>%\n  group_by(season) %>%\n  summarise(black_gesso = sum(Black_Gesso),\n    bright_red = sum(Bright_Red),\n    burnt_umber = sum(Burnt_Umber),\n    carmium_yellow = sum(Cadmium_Yellow),\n    dark_sienna = sum(Dark_Sienna),\n    indian_red = sum(Indian_Red),\n    indian_yellow = sum(Indian_Yellow),\n    liquid_black = sum(Liquid_Black),\n    liquid_clear = sum(Liquid_Clear),\n    midnight_black = sum(Midnight_Black),\n    phthalo_blue = sum(Phthalo_Blue),\n    phthalo_green = sum(Phthalo_Green),\n    prussian_blue = sum(Prussian_Blue),\n    sap_green = sum(Sap_Green),\n    titanium_white = sum(Titanium_White),\n    van_dyke_brown = sum(Van_Dyke_Brown),\n    yellow_orchre = sum(Yellow_Ochre),\n    alizarian_crimson = sum(Alizarin_Crimson)) %>%\n  pivot_longer(\n    !season, \n    names_to = \"color\", \n    values_to = \"count\")\n\ncolor_labs <- c(black_gesso = \"Black Gesso\", \n                bright_red = \"Bright Red\", \n                burnt_umber = \"Burnt Umber\",\n                carmium_yellow = \"Carmium Yellow\", \n                dark_sienna = \"Dark Sienna\", \n                indian_red = \"Indian Red\",\n                indian_yellow = \"Indian Yellow\", \n                liquid_black = \"Liquid Black\", \n                liquid_clear = \"Liquid Clear\",\n                midnight_black = \"Midnight Black\", \n                phthalo_blue = \"Phthalo Blue\", \n                phthalo_green = \"Phthalo Green\",\n                prussian_blue = \"Prussian Blue\", \n                sap_green = \"Sap Green\", \n                titanium_white = \"Titanium White\",\n                van_dyke_brown = \"Van Dyke Brown\", \n                yellow_orchre = \"Yellow Orchre\", \n                alizarian_crimson = \"Alizarian Crimson\")\n\n\nhtml_colors <- c(\n  \"#000000\", \"#DB0000\", \"#8A3324\",\n  \"#FFEC00\", \"#5F2E1F\", \"#CD5C5C\", \n  \"#FFB800\", \"#000000\", \"#FFFFFF\",\n  \"#000000\", \"#0C0040\", \"#102E3C\",\n  \"#021E44\", \"#0A3410\", \"#FFFFFF\",\n  \"#221B15\", \"#C79B00\", \"#4E1500\")\n\n# html_colors <- rep(html_colors, 31)\n# br_subset$html_colors <- html_colors\n\nggplot(br_subset, aes(x = season, y = count)) +\n  geom_point() +\n  facet_wrap(\n    vars(color), \n    labeller = labeller(color = color_labs)) +\n  ggtitle(\"Color Trends By Season\") +\n  xlab(\"Season\") +\n  ylab(\"Count\") + \n  theme(\n    plot.title = element_text(family = \"Times\"),\n    axis.title.x = element_text(family = \"Times\"),\n    axis.title.y = element_text(family = \"Times\"),\n    strip.background = element_rect(\n     color=\"#2c1320\", \n     fill= \"#6c464e\", \n     size=1.5, \n     linetype=\"solid\"),\n    strip.text.x = element_text(\n      family = \"Times\",\n      color = \"#feebe2\")\n  )"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html",
    "title": "The Joy of Analysis",
    "section": "",
    "text": "As an artist and TV personality, Bob Ross has captivated audiences for decades with his signature afro, and of course, his beautiful paintings. In this blog post, we will take a closer look at the colors that Bob Ross used during his time on “The Joy of Painting”."
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#about-the-joy-of-painting",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#about-the-joy-of-painting",
    "title": "The Joy of Analysis",
    "section": "About “The Joy of Painting”:",
    "text": "About “The Joy of Painting”:\n\n31 total seasons.\n13 episodes per season.\n403 total episodes.\n\nWhen we consider our use case above, and that a canvas costs about $3. Then a budget conscious user can assume their budget for paint canvas will be about $40 per season, and about $1,210 for all 403 episodes.\nBelow is a table that shows the total number of episodes per season.\n\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nseason            1    2    3    4    5    6    7    8    9    10    11    12\ntotal_episodes   13   13   13   13   13   13   13   13   13    13    13    13\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nseason            13    14    15    16    17    18    19    20    21    22\ntotal_episodes    13    13    13    13    13    13    13    13    13    13\n               [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31]\nseason            23    24    25    26    27    28    29    30    31\ntotal_episodes    13    13    13    13    13    13    13    13    13"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#colors-used",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#colors-used",
    "title": "The Joy of Analysis",
    "section": "Colors Used",
    "text": "Colors Used\n\nOn average 138 colors were used each season.\nThe least amount of colors used in a single season was 92 in season 1.\nThe most amount of colors used in a single season was 156 in season 14.\n\nWhen considering our use case, a budget conscious viewer may consider starting their journey at season 1 where the least amount of colors are used.\nBelow is a table that shows the total number of colors used per season, and a graph that shows the total number of colors used each season stacked by color that was used.\n\n\n     [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]     \n[1,] \"#4E1500\" \"#000000\" \"#DB0000\" \"#8A3324\" \"#FFEC00\" \"#5F2E1F\" \"#CD5C5C\"\n     [,8]      [,9]      [,10]     [,11]     [,12]     [,13]     [,14]    \n[1,] \"#FFB800\" \"#000000\" \"#FFFFFF\" \"#000000\" \"#0C0040\" \"#102E3C\" \"#021E44\"\n     [,15]     [,16]     [,17]     [,18]    \n[1,] \"#0A3410\" \"#FFFFFF\" \"#221B15\" \"#C79B00\""
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#color-trends",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#color-trends",
    "title": "The Joy of Analysis",
    "section": "Color Trends",
    "text": "Color Trends\n\n Indian Red  is the color that is least used. It was only featured once in season 22 episode 1 in the Painting Autumn Images shown below.\n\n\n\n\n\nAutumn Images is a painting of a mountain in front of big white clouds, and surrounded by trees.\n\n\n\n\n Burnt Umber  was only used in 55 episodes. The last painting it was used in was from season 6 episode 13 and is called Blaze of Color, shown below.\n\n\n\n\n\nBlaze of Color is a painting of a green and dark blue creek surrounded by yellow and light green grass, trees, and a vibrant blue, pink, yellow, orange, and red sunset.\n\n\n\n\n Dark Sienna  may have been used instead of Burnt umber. It can in the episode of the last season, season 31 episode 13, in a painting called Wilderness Day, shown below.\n\n\n\n\n\nWilderness Day is a painting of a dark green meadow that includes trees, bushes, and a misty mountain in the background.\n\n\n\nBelow is a graph that shows the number of time a color was used each season."
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#br_subset",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#br_subset",
    "title": "The Joy of Analysis",
    "section": "br_subset",
    "text": "br_subset\nbr_subset : short for Bob Ross subset, this data groups the original bob_ross data by season and then counts how many times each color was used in each season.\n\n\nShow Code\nbr_subset <- bob_ross %>%\n  dplyr::group_by(season) %>%\n  dplyr::summarise(\n    black_gesso = base::sum(Black_Gesso),\n    bright_red = base::sum(Bright_Red),\n    burnt_umber = base::sum(Burnt_Umber),\n    carmium_yellow = base::sum(Cadmium_Yellow),\n    dark_sienna = base::sum(Dark_Sienna),\n    indian_red = base::sum(Indian_Red),\n    indian_yellow = base::sum(Indian_Yellow),\n    liquid_black = base::sum(Liquid_Black),\n    liquid_clear = base::sum(Liquid_Clear),\n    midnight_black = base::sum(Midnight_Black),\n    phthalo_blue = base::sum(Phthalo_Blue),\n    phthalo_green = base::sum(Phthalo_Green),\n    prussian_blue = base::sum(Prussian_Blue),\n    sap_green = base::sum(Sap_Green),\n    titanium_white = base::sum(Titanium_White),\n    van_dyke_brown = base::sum(Van_Dyke_Brown),\n    yellow_orchre = base::sum(Yellow_Ochre),\n    alizarian_crimson = base::sum(Alizarin_Crimson)\n    ) %>%\n  tidyr::pivot_longer(\n    !season, \n    names_to = \"color\", \n    values_to = \"count\")"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#br_colors",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#br_colors",
    "title": "The Joy of Analysis",
    "section": "br_colors",
    "text": "br_colors\nbr_colors : short for Bob Ross colors, this data groups the original bob_ross data by season, and then counts the total amount of color used that season.\n\n\nShow Code\nbr_colors <- bob_ross %>%\n  dplyr::group_by(season) %>%\n  dplyr::summarise(\n    total_num_colors = base::sum(num_colors))"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#summary-stats",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#summary-stats",
    "title": "The Joy of Analysis",
    "section": "Summary Stats",
    "text": "Summary Stats\nmean_colors : Give the average number of colors used a season.\n\n\nShow Code\nmean_colors <- base::mean(br_colors$total_num_colors)\nmean_colors\n\n\n[1] 137.871\n\n\nmin_colors : Gives the minimum amount of colors used in a season.\n\n\nShow Code\nmin_colors <- base::min(br_colors$total_num_colors)\nmin_colors\n\n\n[1] 92\n\n\nmax_colors : Gives the max amount of colors used in a season.\n\n\nShow Code\nmax_colors <- base::max(br_colors$total_num_colors)\nmax_colors\n\n\n[1] 156"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#episode_per_season",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#episode_per_season",
    "title": "The Joy of Analysis",
    "section": "episode_per_season",
    "text": "episode_per_season\nepisode_per_season : groups the original bob_ross data by season and then counts how many episode is in each season.\n\n\nShow Code\nepisodes_per_season <- bob_ross %>%\n  dplyr::group_by(season) %>%\n  dplyr::summarise(total_episodes = dplyr::n())"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#color-specific-details",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#color-specific-details",
    "title": "The Joy of Analysis",
    "section": "Color Specific Details",
    "text": "Color Specific Details\nburnt_umber_details : Gives the episode number, season, img_src, paiting title, and youtube_scr for all episodes that used burnt umber.\n\n\nShow Code\nburnt_umber_details <- bob_ross %>%\n  dplyr::filter(Burnt_Umber == 1) %>%\n  dplyr::summarise(\n    episode = episode,\n    season = season,\n    img_src = img_src,\n    painting_title = painting_title,\n    youtube_src = youtube_src)\n\n\ndark_sienna_details : Gives the episode number, season, img_src, paiting title, and youtube_scr for all episodes that used dark sienna.\n\n\nShow Code\ndark_sienna_details <- bob_ross %>%\n  dplyr::filter(Dark_Sienna == 1) %>%\n  dplyr::summarise(\n    episode = episode,\n    season = season,\n    img_src = img_src,\n    painting_title = painting_title,\n    youtube_src = youtube_src)"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#colors_used_each_season",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#colors_used_each_season",
    "title": "The Joy of Analysis",
    "section": "colors_used_each_season",
    "text": "colors_used_each_season\n\n\nShow Code for colors_used_each_season_graph\nbr_colors <- c(\n  \"#4E1500\", \"#000000\", \"#DB0000\", \n  \"#8A3324\", \"#FFEC00\", \"#5F2E1F\", \n  \"#CD5C5C\", \"#FFB800\", \"#000000\", \n  \"#FFFFFF\", \"#000000\", \"#0C0040\", \n  \"#102E3C\", \"#021E44\", \"#0A3410\", \n  \"#FFFFFF\", \"#221B15\", \"#C79B00\")\ncols_used_each_season <- ggplot(\n  br_subset, \n  aes(\n    x = season,\n    y = count,\n    fill = color)\n  ) +\n  geom_col(\n    stat = \"identity\",\n    position = \"stack\"\n    ) +\n  ggtitle(\"Colors Used Each Season\") +\n  xlab(\"Season\") +\n  ylab(\"Colors Used\") + \n  scale_fill_manual(values = br_colors) +\n  theme(\n    plot.title = element_text(family = \"Times\"),\n    axis.title.x = element_text(family = \"Times\"),\n    axis.title.y = element_text(family = \"Times\")\n  )"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#colors_graph",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#colors_graph",
    "title": "The Joy of Analysis",
    "section": "colors_graph",
    "text": "colors_graph\ncolors_graph : Is a graph that shows the amount of specific colors used each season.\n\n\nShow Code\nbr_colors <- c(\n  \"#4E1500\", \"#000000\", \"#DB0000\", \n  \"#8A3324\", \"#FFEC00\", \"#5F2E1F\", \n  \"#CD5C5C\", \"#FFB800\", \"#000000\", \n  \"#FFFFFF\", \"#000000\", \"#0C0040\", \n  \"#102E3C\", \"#021E44\", \"#0A3410\", \n  \"#FFFFFF\", \"#221B15\", \"#C79B00\")\ncolors_graph <- ggplot2::ggplot(\n  br_subset, \n  aes(\n    x = season,\n    y = count,\n    fill = color)\n  ) +\n  ggplot2::geom_col(\n    stat = \"identity\",\n    position = \"stack\"\n    ) +\n  ggplot2::ggtitle(\"Colors Used Each Season\") +\n  ggplot2::xlab(\"Season\") +\n  ggplot2::ylab(\"Colors Used\") + \n  ggplot2::scale_fill_manual(values = br_colors) +\n  ggplot2::theme(\n    plot.title = element_text(family = \"Times\"),\n    axis.title.x = element_text(family = \"Times\"),\n    axis.title.y = element_text(family = \"Times\")\n  )"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#color_trends",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#color_trends",
    "title": "The Joy of Analysis",
    "section": "color_trends",
    "text": "color_trends\ncolor_trends : Is a graph that shows color trends by season.\n\n\nShow Code\ncolor_labs <- c(black_gesso = \"Black Gesso\", \n                bright_red = \"Bright Red\", \n                burnt_umber = \"Burnt Umber\",\n                carmium_yellow = \"Carmium Yellow\", \n                dark_sienna = \"Dark Sienna\", \n                indian_red = \"Indian Red\",\n                indian_yellow = \"Indian Yellow\", \n                liquid_black = \"Liquid Black\", \n                liquid_clear = \"Liquid Clear\",\n                midnight_black = \"Midnight Black\", \n                phthalo_blue = \"Phthalo Blue\", \n                phthalo_green = \"Phthalo Green\",\n                prussian_blue = \"Prussian Blue\", \n                sap_green = \"Sap Green\", \n                titanium_white = \"Titanium White\",\n                van_dyke_brown = \"Van Dyke Brown\", \n                yellow_orchre = \"Yellow Orchre\", \n                alizarian_crimson = \"Alizarian Crimson\")\n\n# html_colors <- rep(html_colors, 31)\n# br_subset$html_colors <- html_colors\ncolor_trends <- ggplot2::ggplot(\n  br_subset, \n  ggplot2::aes(\n    x = season, \n    y = count)\n  ) +\n  ggplot2::geom_point() +\n  ggplot2::facet_wrap(\n    vars(color), \n    labeller = labeller(color = color_labs)) +\n  ggplot2::ggtitle(\"Color Trends By Season\") +\n  ggplot2::xlab(\"Season\") +\n  ggplot2::ylab(\"Count\") + \n  ggplot2::labs(\n    caption = \"Graph by Randi Bolt\\nData from #TidyTuesday\"\n    ) +\n  ggplot2::theme(\n    plot.title = element_text(family = \"Times\"),\n    axis.title.x = element_text(family = \"Times\"),\n    axis.title.y = element_text(family = \"Times\"),\n    strip.background = element_rect(\n     color=\"#2c1320\", \n     fill= \"#6c464e\", \n     size=1.5, \n     linetype=\"solid\"),\n    strip.text.x = element_text(\n      family = \"Times\",\n      color = \"#feebe2\") \n  )"
  },
  {
    "objectID": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#my-recommendations-would-be",
    "href": "01_blog/2023_02_27_TidyTuesday-Bob-Ross/index.html#my-recommendations-would-be",
    "title": "The Joy of Analysis",
    "section": "My recommendations would be:",
    "text": "My recommendations would be:\n\nEach season has the same amount of episodes, so the user might consider viewing the paintings from each season, and deciding which season’s paintings they like best before selecting the color they will purchase. They will enjoy painting more if they know they will enjoy the end result.\nSeason 1 uses the least amount of colors, and might be a good place to start for someone who is new to painting, and cannot otherwise decide which seasons paintings they enjoy the most.\nConsider using similar colors interchangeably, such as  Dark Sienna  instead of  Burnt Umber  and  Indian Red . The user might need to mix in more black or white, to get a similar result to the painting they are recreating, which is another thing to consider when purchasing supplies."
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html",
    "href": "01_blog/2023_05_02_Portal-Project/index.html",
    "title": "The Portal Project (Week 18)",
    "section": "",
    "text": "In this post, I will guide you through the process of creating a data visualization for the Portal Project’s #TidyTuesday1 challenge in Week 18 of 2023."
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#a.-data",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#a.-data",
    "title": "Week 18: The Portal Project",
    "section": "A. Data",
    "text": "A. Data\n\n\nShow Code for Simulted Data\ndf <- base::data.frame(\n  \"year\" = base::sample(\n    c(1978:2022),\n    size = 399,\n    replace = TRUE),\n  \"month\" = base::sample(\n      c(1,2,3,4,5,6,7,8,9,10,11,12),\n      size = 399,\n      replace = TRUE),\n  \"n\" = base::sample(\n    c(1:20),\n    size = 399,\n    replace = TRUE\n  ))\nutils::head(df)\n\n\n  year month  n\n1 2009    11  9\n2 1989     1  8\n3 1997     7  3\n4 1993     5 20\n5 1999    11  7\n6 1994     1 10"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#b.-graph",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#b.-graph",
    "title": "Week 18: The Portal Project",
    "section": "B. Graph",
    "text": "B. Graph\n\n\nShow Code for Sample Graph\nvis <-  function(df){\n  vis <- ggplot2::ggplot(\n    df, \n    mapping = ggplot2::aes(\n      x = month,\n      y = n,\n      group = year)\n    ) +\n  ggplot2::geom_point() +\n  ggplot2::geom_line() +\n  ggplot2::facet_wrap(~year)\n  return(vis)\n}\nvis(df)"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#style-function",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#style-function",
    "title": "The Portal Project (Week 18)",
    "section": "Style Function",
    "text": "Style Function\n\n\nShow Code for Style Function\n#### Style Function ####\n# A function that inputs a graph, \n# and outputs a stylized graph. \n# \n# @param vis is a graph.\n# @return sty is a stylized graph.\n# \n# Parts: \n# 1. Title, subtitles, labels, caption, alt text.\n# 2. X and Y scale.\n# 3. Add values above points.\n# 4. Format\nsty <- function(vis){\n  sty <- vis +\n    # 1. Title, subtitles, labels, caption, alt text\n    ggplot2::ggtitle(\"Swollen Testicles of Merriam's Kangaroo Rat by Month and Year near Portal, AZ\") +\n    ggplot2::labs(\n      subtitle = \"The Portal Project is a long-term ecological study being conducted near Portal, AZ.\\nThis chart shows the number of Merriam's Kangaroo Rat with swollen testicles by month and year over the past 45 years.\",\n      x = \"Month\",\n      y = \"Number of Rodents\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday and Portal Project\",\n      alt = \"This is a point and line graph showing swollen testicles of Merriam's Kangaroo Rat by months and year over the past 45 years. The x axis shows months, and the y axis shows number of rodents.\") +\n    \n    # 2. x and y scale \n    ggplot2::scale_x_continuous(\n      breaks = seq(1,12,1)) + \n    ggplot2::scale_y_continuous(\n      breaks = seq(0,60,10)) +\n    \n    # 3. add values above points\n    ggplot2::geom_text(\n      ggplot2::aes(\n        label = n,\n        vjust = -0.2)) +\n    \n    # 4. format\n    ggplot2::theme(\n      # title\n      plot.title = element_text(\n        size = 16,\n        face = \"bold\",\n        color = \"#232741\",\n        hjust = .5),\n      # subtitle\n      plot.subtitle = element_text(\n      size = 12,\n      hjust = .5,\n      color = \"#232741\"),\n    # plot background\n    plot.background = element_rect(\n      fill = \"#C58C60\"),\n    # graph background\n    panel.background = element_rect(\n      fill = \"#D3DAD9\"),\n    # x and y axis lables\n    axis.title = element_text(\n      size = 14, \n      color = \"#232741\"),\n    # legend background\n    legend.background = element_rect(\n      fill = \"#C58C60\"),\n    # facet graph titles\n    strip.background = element_rect(\n      fill = \"#A76858\"))\n  return(sty)\n}"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#stylized-graph",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#stylized-graph",
    "title": "The Portal Project (Week 18)",
    "section": "Stylized Graph",
    "text": "Stylized Graph\n\n\nShow Code for Stylized Graph\nsty(vis(df))"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#cleaning-function",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#cleaning-function",
    "title": "The Portal Project (Week 18)",
    "section": "Cleaning Function",
    "text": "Cleaning Function\n\n\nShow Code for Cleaning Function\n#### Cleaning Function ####\n# A function that takes in a df\n# and returns a clean df. \n#\n# @param data is a df.\n# @param clean is a clean df.\n#\n# Parts: \n# 1. Select\n# 2. Filter \n# 3. Group\n# 4. Reframe\n# 5. Unique \nclean <- function(data){\n  clean <- data %>%\n    # 1. Select \n    dplyr::select(\n      month, \n      year, \n      sex, \n      testes, \n      species) %>%\n    # 2. Filter \n    dplyr::filter(\n      sex == \"M\",\n      species == \"DM\",\n      testes == \"S\") %>%\n    # 3. Group\n    dplyr::group_by(\n      year, \n      month) %>%\n    # 4. Refremae\n    dplyr::reframe(\n      month = month,\n      year = year,\n      n = n()) %>%\n    # 5. unique \n    base::unique()\n  return(clean)\n}"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#cleaning-the-data",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#cleaning-the-data",
    "title": "The Portal Project (Week 18)",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\n\n\nShow Code for Data Cleaning\nutils::head(clean(surveys))\n\n\n# A tibble: 6 × 3\n   year month     n\n  <dbl> <dbl> <int>\n1  1978     2     2\n2  1978     3     1\n3  1978     4     2\n4  1978     5     4\n5  1978     6     2\n6  1978     7     4"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#data",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#data",
    "title": "The Portal Project (Week 18)",
    "section": "Data",
    "text": "Data\n\n\nShow Code for Simulted Data\n#### Simulated Data ####\n# Includes:\n# - years between 1978 and 2022\n# - months between 1 and 12\n# - n between 1 and 20\ndf <- base::data.frame(\n  \"year\" = base::sample(\n    c(1978:2022),\n    size = 399,\n    replace = TRUE),\n  \"month\" = base::sample(\n    c(1,2,3,4,5,6,7,8,9,10,11,12),\n    size = 399,\n    replace = TRUE),\n  \"n\" = base::sample(\n    c(1:20),\n    size = 399,\n    replace = TRUE\n  ))\nutils::head(df)\n\n\n  year month  n\n1 1982     8 14\n2 1982     2 14\n3 1987     8  5\n4 1993     2  8\n5 2020     5 19\n6 1996     9 14"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#graph",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#graph",
    "title": "The Portal Project (Week 18)",
    "section": "Graph",
    "text": "Graph\n\n\nShow Code for Sample Graph\nvis(df)"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#read-data",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#read-data",
    "title": "The Portal Project (Week 18)",
    "section": "Read Data",
    "text": "Read Data\n\n\nShow Code for Reading in the Data\n#### Read in the data ####\nsurveys <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-02/surveys.csv')"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#graph-function",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#graph-function",
    "title": "The Portal Project (Week 18)",
    "section": "Graph Function",
    "text": "Graph Function\n\n\nShow Code for Graphing Function\n#### Graph Function ####\n# A function that takes in a df,\n# and outputs a graph.\n# \n# @param df is a dataframe with that \n# include month, year, and n. \n# @return vis is a graph. \n#\n# Parts:\n# 1. Mapping \n# 2. Graph Geometries \n# 3. Facet Wrap\nvis <-  function(df){\n  vis <- ggplot2::ggplot(\n    df, \n    # 1. Mapping\n    mapping = ggplot2::aes(\n      x = month,\n      y = n,\n      group = year)\n    ) +\n  # 2. Graph geoms\n  ggplot2::geom_point() +\n  ggplot2::geom_line() +\n  # 3. Facet Wrap\n  ggplot2::facet_wrap(~year)\n  return(vis)\n}"
  },
  {
    "objectID": "01_blog/2023_05_02_Portal-Project/index.html#clean-data",
    "href": "01_blog/2023_05_02_Portal-Project/index.html#clean-data",
    "title": "The Portal Project (Week 18)",
    "section": "Clean Data",
    "text": "Clean Data\n\n\nShow Code for Data Cleaning\nutils::head(clean(surveys))\n\n\n# A tibble: 6 × 3\n   year month     n\n  <dbl> <dbl> <int>\n1  1978     2     2\n2  1978     3     1\n3  1978     4     2\n4  1978     5     4\n5  1978     6     2\n6  1978     7     4"
  },
  {
    "objectID": "01_blog/2023-02-21_Week_08_TidyTuesday/index.html",
    "href": "01_blog/2023-02-21_Week_08_TidyTuesday/index.html",
    "title": "Week 8 Tidy Tuesday: Bob Ross Paintings",
    "section": "",
    "text": "Bob Ross Paintings\nThe data this week comes from Jared Wilber’s data on Bob Ross Paintings via @frankiethull Bob Ross Colors data package.\n\n\n\nCode\nThis week I wanted to keep the visual fairly simple so that I could focus my efforts on designing the project in a way that felt intuitive, was easy to read, and had a manageable workflow. To achieve these objectives I broke the project into three key functions: cleaning, visualizing, and styling - organized within the ‘Functions’ folder for that weeks submission. The index.R file integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaning FunctionVisual FunctionStyle FunctionIndex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# dplyr: data cleaning functions.\nbase::library(dplyr)\n#### Cleaning Function ####\nclean <- function(df){\n  # clean function\n  clean_df <- df %>%\n    dplyr::group_by(season) %>%\n    dplyr::summarise(\n      total_num_colors = base::sum(num_colors))\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Visual Function ####\nvis <- function(clean_df){\n  vis <- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(\n      x = season,\n      y = total_num_colors)) +\n    ggplot2::geom_point() + \n    ggplot2::geom_segment(\n      ggplot2::aes(\n        x = season,\n        xend = season,\n        y = 0,\n        yend = total_num_colors)) + \n    ggplot2::geom_hline(\n      yintercept=137.871, \n      linetype=\"dashed\", \n      color = \"red\") +\n    ggplot2::geom_text(\n      ggplot2::aes(label = total_num_colors,\n                   vjust = -1)\n    )\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Style Function ####\nsty <- function(vis){\n  sty <- vis +\n    # labels \n    ggplot2::labs(\n      title = \"Colors Used Each Season\",\n      subtitle = \"The dashed red line shows the average number of colors used each season, 137.871.\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Season\",\n      y = \"Colors Used\"\n    ) + \n    # themes\n    ggplot2::theme_classic() + \n    ggplot2::theme(\n      plot.title = element_text(\n        face = \"bold\",\n        hjust = .5),\n      plot.subtitle = element_text(\n        hjust = .5)\n    ) \n  return(sty)\n  }\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# bob_ross: data about and from \"The Joy of Painting\".\ntt_data <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-21/bob_ross.csv')\n\n#### Clean Data ####\nclean_data <- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data <- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis <- sty(vis_data)\n\n#### Save Plot ####\nggplot2::ggsave(\n  base::paste0(\"plot.png\"), \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 8 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo make a lollipop graph using ggplot2 you need to assign geom_point() and geom_segment().\ngoem_hline() was used to created the dashed horizontal red line.\ngeom_text() is used to add the values above the lollipops.\nUsing the TidyTuesdayR package can be problematic.\nIt looks nicer to keep all labels in the labs() function."
  },
  {
    "objectID": "01_blog/2023_04_25_TidyTuesday-London-Marathon/index.html",
    "href": "01_blog/2023_04_25_TidyTuesday-London-Marathon/index.html",
    "title": "Week 17 Tidy Tuesday: London Marathon",
    "section": "",
    "text": "London Marathon\nThe data this week comes from Nicola Rennie’s LondonMarathon R package. This is an R package containing two data sets scraped from Wikipedia (1 November 2022) on London Marathon winners, and some general data. How the dataset was created, and some analysis, is described in Nicola’s post “Scraping London Marathon data with {rvest}”. Thank you for putting this dataset together @nrennie!\n\n\n\nCode\nThis week I wanted to add flags for each nationality, so two more functions are added to this weeks workflow:\n\nflags(): which reads in each flags .pgn, and saves them as a list.\nadd_flag(): reads in the list of flags, and overlays the images on the graph.\n\nAdditionally are the three key functions: cleaning, visualizing, and styling, as well as the index.R file which integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaningVisualizingStylingFlagsAdd Flagsindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(dplyr)\nlibrary(magrittr)\n\n#### Cleaning Function ####\nclean <- function(data){\n  clean_data <- data %>%\n    dplyr::group_by(Nationality) %>%\n    dplyr::summarise(\n      nat_winners = dplyr::n()) %>%\n    dplyr::arrange(\n      dplyr::desc(nat_winners))\n  return(clean_data)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\ngeom_bar_color <- \"#DD733A\"\n\n#### Visual Function ####\nvis <- function(df){\n  vis <- ggplot2::ggplot(\n    df,\n    ggplot2::aes(\n      x = nat_winners,\n      y = stats::reorder(\n        Nationality, \n        nat_winners))) +\n    ggplot2::geom_bar(\n      stat = \"identity\",\n      fill = geom_bar_color)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\nbackground_color <- \"#596D78\"\ntitle_color <- \"#ABB3B8\"\ngraph_color <- \"#ABB3B8\"\nbonus_1 <- \"#4F555B\"\nbonus_2 <- \"#B4AE59\"\n\n#### Style Function ####\nstyle <- function(vis){\n  sty <- vis +\n    # labels\n    ggplot2::labs(\n      title = \"Nationality of London Marathon Winners\",\n      subtitle = \"\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Number of Winners\",\n      y = \"\") +\n    # values next to bars\n    ggplot2::geom_text(\n      ggplot2::aes(label = nat_winners,\n                   hjust = 1.2)) +\n    ggplot2::theme(\n      # title\n      plot.title = element_text(\n        size = 18,\n        face = \"bold\",\n        color = title_color,\n        hjust = .5),\n      # caption \n      plot.caption = element_text(\n        color = title_color),\n      # graph background\n      panel.background = element_rect(\n        fill = graph_color),\n      # x and y axis labels\n      axis.title = element_text(\n        size = 14, \n        color = title_color),\n      # axis ticks text\n      axis.text = element_text(\n        siz = 10,\n        color = title_color),\n      # background\n      plot.backgroun = ggplot2::element_rect(\n        fill = background_color\n      )\n    )\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(png)\n\n#### Load Flags ####\nuk <-  readPNG(\"flags/uk.png\")   \nkenya <-  readPNG(\"flags/kenya.png\") \nusa <-  readPNG(\"flags/usa.png\")\nswi <-  readPNG(\"flags/switzerland.png\")\nethi <-  readPNG(\"flags/ethiopia.png\")\nnor <-  readPNG(\"flags/norway.png\")\nire <-  readPNG(\"flags/ireland.png\")\nmex <-  readPNG(\"flags/mexico.png\")\ngermany <-  readPNG(\"flags/germany.png\") \nport <-  readPNG(\"flags/portugal.png\")\nitaly <-  readPNG(\"flags/italy.png\")\ncan <-  readPNG(\"flags/canada.png\")\nsweden <-  readPNG(\"flags/sweden.png\")\njapan <-  readPNG(\"flags/japan.png\")\nfran <-  readPNG(\"flags/france.png\")\ndenmark <-  readPNG(\"flags/denmark.png\")\naus <-  readPNG(\"flags/australia.png\")\npol <- readPNG(\"flags/poland.png\")\nneth <- readPNG(\"flags/netherlands.png\")\nmor <- readPNG(\"flags/morocco.png\")\nspain <- readPNG(\"flags/spain.png\")\nsu <- readPNG(\"flags/soviet_union.png\")\nchina <- readPNG(\"flags/china.png\")\nbel <- readPNG(\"flags/belgium.png\")\n\n#### Flag List ####\nflag_list <- list(\n  uk, kenya, usa,\n  swi, ethi, nor,\n  ire, mex, germany,\n  port, italy, can,\n  sweden, japan, fran,\n  denmark, aus, pol,\n  neth, mor, spain,\n  su, china, bel)\n\n#### Save Flags ####\nsaveRDS(flag_list, file=\"data/flag_list.RData\")\n\n\n\n\n\n\nShow Code\n#### Load Packages #### \nlibrary(ggplot2)\nlibrary(grid)\n\n#### Load Data ####\nflags_list <- readRDS(\"data/flag_list.RData\")\n\n#### Add Flags ####\nadd_flags <- function(styled_vis){\n  for (i in 1:length(flags_list)){\n    flag_new <- styled_vis + \n      # add static annotation\n      ggplot2::annotation_custom(\n        # cycles through list and stacking flags on graph\n        grid::rasterGrob(\n          flags_list[[i]], \n          width=1, height=1),\n        xmin = -2, xmax = -.1,\n        ymin = 24.55-i, ymax = 25.45-i)\n    styled_vis <- flag_new\n  }\n  return(flag_new)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse) \nbase::library(png)\n#### Load Data ####\nremotes::install_github(\"nrennie/LondonMarathon\")\ndata(winners, package = \"LondonMarathon\")\n\n#### Clean Data ####\ndata <- clean(winners)\n\n#### Create Data Visual ####\ndata_vis <- vis(data)\n\n#### Style Data Visual ####\nsty_vis <- style(data_vis)\n\n#### Add Flags ####\nsty_vis_with_flags <- add_flags(sty_vis)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 17 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nDownload images from wikimedia with utils::download.file(url = \"\", destfile = \"\", mode = \"wb).\nAdd images to data visual with ggplot2::annotation_custion() and grid::rasterGrob.\nggplot2::labs() reads markdown such as newline, \\n."
  },
  {
    "objectID": "01_blog/2023_04_25_W17-TidyTuesday-London-Marathon/index.html",
    "href": "01_blog/2023_04_25_W17-TidyTuesday-London-Marathon/index.html",
    "title": "Week 17 Tidy Tuesday: London Marathon",
    "section": "",
    "text": "London Marathon\nThe data this week comes from Nicola Rennie’s LondonMarathon R package. This is an R package containing two data sets scraped from Wikipedia (1 November 2022) on London Marathon winners, and some general data. How the dataset was created, and some analysis, is described in Nicola’s post “Scraping London Marathon data with {rvest}”. Thank you for putting this dataset together @nrennie!\n\n\n\nCode\nThis week I wanted to add flags for each nationality, so two more functions are added to this weeks workflow:\n\nflags(): which reads in each flags .pgn, and saves them as a list.\nadd_flag(): reads in the list of flags, and overlays the images on the graph.\n\nAdditionally are the three key functions: cleaning, visualizing, and styling, as well as the index.R file which integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaningVisualizingStylingFlagsAdd Flagsindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(dplyr)\nlibrary(magrittr)\n\n#### Cleaning Function ####\nclean <- function(data){\n  clean_data <- data %>%\n    dplyr::group_by(Nationality) %>%\n    dplyr::summarise(\n      nat_winners = dplyr::n()) %>%\n    dplyr::arrange(\n      dplyr::desc(nat_winners))\n  return(clean_data)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\ngeom_bar_color <- \"#DD733A\"\n\n#### Visual Function ####\nvis <- function(df){\n  vis <- ggplot2::ggplot(\n    df,\n    ggplot2::aes(\n      x = nat_winners,\n      y = stats::reorder(\n        Nationality, \n        nat_winners))) +\n    ggplot2::geom_bar(\n      stat = \"identity\",\n      fill = geom_bar_color)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\nbackground_color <- \"#596D78\"\ntitle_color <- \"#ABB3B8\"\ngraph_color <- \"#ABB3B8\"\nbonus_1 <- \"#4F555B\"\nbonus_2 <- \"#B4AE59\"\n\n#### Style Function ####\nstyle <- function(vis){\n  sty <- vis +\n    # labels\n    ggplot2::labs(\n      title = \"Nationality of London Marathon Winners\",\n      subtitle = \"\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Number of Winners\",\n      y = \"\") +\n    # values next to bars\n    ggplot2::geom_text(\n      ggplot2::aes(label = nat_winners,\n                   hjust = 1.2)) +\n    ggplot2::theme(\n      # title\n      plot.title = element_text(\n        size = 18,\n        face = \"bold\",\n        color = title_color,\n        hjust = .5),\n      # caption \n      plot.caption = element_text(\n        color = title_color),\n      # graph background\n      panel.background = element_rect(\n        fill = graph_color),\n      # x and y axis labels\n      axis.title = element_text(\n        size = 14, \n        color = title_color),\n      # axis ticks text\n      axis.text = element_text(\n        siz = 10,\n        color = title_color),\n      # background\n      plot.backgroun = ggplot2::element_rect(\n        fill = background_color\n      )\n    )\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(png)\n\n#### Load Flags ####\nuk <-  readPNG(\"flags/uk.png\")   \nkenya <-  readPNG(\"flags/kenya.png\") \nusa <-  readPNG(\"flags/usa.png\")\nswi <-  readPNG(\"flags/switzerland.png\")\nethi <-  readPNG(\"flags/ethiopia.png\")\nnor <-  readPNG(\"flags/norway.png\")\nire <-  readPNG(\"flags/ireland.png\")\nmex <-  readPNG(\"flags/mexico.png\")\ngermany <-  readPNG(\"flags/germany.png\") \nport <-  readPNG(\"flags/portugal.png\")\nitaly <-  readPNG(\"flags/italy.png\")\ncan <-  readPNG(\"flags/canada.png\")\nsweden <-  readPNG(\"flags/sweden.png\")\njapan <-  readPNG(\"flags/japan.png\")\nfran <-  readPNG(\"flags/france.png\")\ndenmark <-  readPNG(\"flags/denmark.png\")\naus <-  readPNG(\"flags/australia.png\")\npol <- readPNG(\"flags/poland.png\")\nneth <- readPNG(\"flags/netherlands.png\")\nmor <- readPNG(\"flags/morocco.png\")\nspain <- readPNG(\"flags/spain.png\")\nsu <- readPNG(\"flags/soviet_union.png\")\nchina <- readPNG(\"flags/china.png\")\nbel <- readPNG(\"flags/belgium.png\")\n\n#### Flag List ####\nflag_list <- list(\n  uk, kenya, usa,\n  swi, ethi, nor,\n  ire, mex, germany,\n  port, italy, can,\n  sweden, japan, fran,\n  denmark, aus, pol,\n  neth, mor, spain,\n  su, china, bel)\n\n#### Save Flags ####\nsaveRDS(flag_list, file=\"data/flag_list.RData\")\n\n\n\n\n\n\nShow Code\n#### Load Packages #### \nlibrary(ggplot2)\nlibrary(grid)\n\n#### Load Data ####\nflags_list <- readRDS(\"data/flag_list.RData\")\n\n#### Add Flags ####\nadd_flags <- function(styled_vis){\n  for (i in 1:length(flags_list)){\n    flag_new <- styled_vis + \n      # add static annotation\n      ggplot2::annotation_custom(\n        # cycles through list and stacking flags on graph\n        grid::rasterGrob(\n          flags_list[[i]], \n          width=1, height=1),\n        xmin = -2, xmax = -.1,\n        ymin = 24.55-i, ymax = 25.45-i)\n    styled_vis <- flag_new\n  }\n  return(flag_new)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse) \nbase::library(png)\n#### Load Data ####\nremotes::install_github(\"nrennie/LondonMarathon\")\ndata(winners, package = \"LondonMarathon\")\n\n#### Clean Data ####\ndata <- clean(winners)\n\n#### Create Data Visual ####\ndata_vis <- vis(data)\n\n#### Style Data Visual ####\nsty_vis <- style(data_vis)\n\n#### Add Flags ####\nsty_vis_with_flags <- add_flags(sty_vis)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 17 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nDownload images from wikimedia with utils::download.file(url = \"\", destfile = \"\", mode = \"wb).\nAdd images to data visual with ggplot2::annotation_custion() and grid::rasterGrob.\nggplot2::labs() reads markdown such as newline, \\n."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "",
    "text": "In this post, I’ll be sharing my review of the data visuals and analysis presented in the article Gun Law Scorecard."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#introduction",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#introduction",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "Introduction",
    "text": "Introduction\nGun Law Scorecard grades each U.S. state on gun safety, highlighting the correlation between gun regulation and lower gun-related deaths. The article features 9 interactive data visuals that tell the story of gun laws and deaths in the country. In this review, I will describe each data visual’s features and provide three bullet points summarizing my thoughts."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#grading-the-states-map",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#grading-the-states-map",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "1. Grading the States (Map)",
    "text": "1. Grading the States (Map)\n\n\n\n\n\nThe data visualization is a choropleth map that displays the relationship between a state’s gun law strength, gun death rank, gun death rate, and grade for gun safety. Each state is represented with a color-coded gradient that corresponds to its value for each of these factors.\n\n\n\n\n\n\nThe title of each section in this article corresponds to the content depicted in the visual presented within in.\nThe visual explanation consists of four sentences. The first two describe the grading system used in the article.The third presents the thesis that “strong gun laws save lives”. The last sentence explains how the visual works.\nThe interactive features of the visual include: (1) a drop-down menu located beside the “GRADE” title that allows users to switch between “GUN LAW STRENGTH,” “GUN DEATH RANK,” and “GUN DEATH RATE”; (2) the ability to hover over each state with a mouse, revealing its grade, strength, or rank; and (3) pop-up windows that provide a more detailed analysis of each state."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#grading-the-states-table",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#grading-the-states-table",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "2. Grading the States (Table)",
    "text": "2. Grading the States (Table)\n\n\n\n\n\nA table showing Grade, State, Gun Law Strength, Gun Death Rank, and Fun Death Rate.\n\n\n\n\n\n\nGun Law Strength (Ranked) is the left most column. This follows our intuition to have a ranked row as the leftmost column of the table. It is also the more optimistic choice between Law Strength and Death Rate.\nPop-out darkens the rest of the screen, and gives state specific information of what has changed this year and how each state can improve.\n\n\n\n\nThe table shows all 50 states (no option to show less), so it is helpful to be able to switch back to the map to keep scrolling."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#gun-laws-vs.-gun-deaths",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#gun-laws-vs.-gun-deaths",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "3. Gun Laws Vs. Gun Deaths",
    "text": "3. Gun Laws Vs. Gun Deaths\n\n\n\n\n\nA linear regression dot plot comparing Gun Deaths Per 100,000 to Gun Law Strength.\n\n\n\n\n\n\nTitle of the plot tells the viewer the conclusion of the the visual which is, As Gun Laws Weaken, Gun Deaths Rise.\nI’m curious about some of the outlier states such as New Mexico, and New Hampshire.\nIt would be interesting to include more data and run a more in depth linear analysis."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#compare-states",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#compare-states",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "4. Compare States",
    "text": "4. Compare States\n\n\n\n\n\nComparison Table to compare two states on Grade, Gun Law Strength Rank, Gun Death Rate Rank, Fun Deaths Per 100k, and % Difference from National Average.\n\n\n\n\n\n\nStarting comparison is the top ranking state California, and the bottom ranking state Arkansas.\nThe interactive table above was helpful in finding other comparisons such as Massachusetts and Mississippi.\nThis visual and the table are the only two with the option to share to twitter or facebook on the footer of the visual."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#federal-process",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#federal-process",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "5. Federal Process",
    "text": "5. Federal Process\n\n\n\n\n\nTimeline of Federal Progress from 1934 to 2022.\n\n\n\n\n\n\nBold text in left tells what the visual is about which is that in the past 100 years there has only been 6 national gun laws passed. The idea of more national regulation is a theme throughout this article.\nBoth the word “Progress” and the years are written in red. Showing that progress happened these years.\nSimple, effective, and informative. By clicking on the different hears the reader can learn a little bit about the law. There is also more information about each law when clicking on the “Learn More” button under the text on the left."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#best-worst",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#best-worst",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "6. Best & Worst",
    "text": "6. Best & Worst\n\n\n\n\n\nBest and Worst for Strongest Laws, Most Improved, Safest State, Weakest Laws, Biggest Drop, and Deadliest State.\n\n\n\n\n\n\nGood visual to compare the good and the bad. Each has two interactive buttons. One to show a states entire score card, and the other to input that state into the comparison table.\nMetrics for plaques were Strongest/Weakest Laws, Most Improved/ Biggest Drop, and Safest/Deadliest State.\nEach plaque includes an image of a state, but the image (unlike the other buttons below the line) does not pop the user up to the interactive map. Meaning it is purely decorative. Something to consider."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#state-progress-heat-map-chart",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#state-progress-heat-map-chart",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "7. State Progress (Heat Map Chart)",
    "text": "7. State Progress (Heat Map Chart)\n\n\n\n\n\nHeat Map Chart of New Gun Safety Laws in 2022.\n\n\n\n\n\n\nInteresting choice of visual, but helpful in giving a broad understanding of what the state laws related to guns look like.\nWhen hovering over the category name the rectangle changes from blue to red, and a dark blue box pops up to tell the reader the exact number of laws.\nAnother visual I may have considered using for this is a horizontal bar chart. This visual style would make it easier for the reader to notice that both “Access to Guns” and “Other Gun Safety Laws” have 14 new laws in 2022. Similarly the “Domestic Violence” and “Gun Dealer Regulations” both have 6 new gun laws in 2022."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#state-progress-dot-plot",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#state-progress-dot-plot",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "8. State Progress (Dot Plot)",
    "text": "8. State Progress (Dot Plot)\n\n\n\n\n\nDot Plot of New Gun Safety Laws from 2013 to 2022.\n\n\n\n\n\n\nThis visual works well with the one above it, because after reading about the new laws in 2022 one would be curious what the progression over time has been.\nInteractivity is fun, but I would have preferred the amount actively visible above the plot since there are only 10 points.\nI am curious if there exists data that shows which state gun laws may have been created because of a mass shooting event."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#hate-crimes-guns-bar-graph",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#hate-crimes-guns-bar-graph",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "9. Hate Crimes & Guns (Bar Graph)",
    "text": "9. Hate Crimes & Guns (Bar Graph)\n\n\n\n\n\nBar Graph of Hate Crimes Reported to the FBI from 2012 to 2021.\n\n\n\n\n\n\nImportant visual to add that is related to the main thesis of the article.\nSince the hate crimes seem to be on the rise, it may be interesting to view the number of hate crimes by state, or see if there is any relevence to included that data into the scorecard.\nIt may be interesting to see this same visual, but broken down by hate crime, or comparing other categories of gun violence such as suicide, domestic violence, and police violence."
  },
  {
    "objectID": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#final-thoughts",
    "href": "01_blog/2023_05_04_Gun-Law-ScoreCard-Review/index.html#final-thoughts",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are some of my main take away from this analysis:\n\nA consistent color scheme of Red, White, and Blue was present across the article, and data visuals. Red is more prominently used with guns, gun deaths, worst states, and as a highlight, accent, or call to action color. The blue seems more neutral or positive. Something to consider when deciding on color palletes for visuals.\nA “new” metric was created (the scorecard) to tell a consistent story through multiple data visuals.\nInteractive visuals are a great way to engage the reader to investigate their own state, and familiarize themselves with other states.\nOther data that might be interesting to view, gun sales, suicide by gun, mass shootings, state regulations, and gun related hate crimes."
  },
  {
    "objectID": "01_blog/2023_02_21_W08-TidyTuesday-Bob-Ross/index.html",
    "href": "01_blog/2023_02_21_W08-TidyTuesday-Bob-Ross/index.html",
    "title": "Week 8 Tidy Tuesday: Bob Ross Paintings",
    "section": "",
    "text": "Bob Ross Paintings\nThe data this week comes from Jared Wilber’s data on Bob Ross Paintings via @frankiethull Bob Ross Colors data package.\n\n\n\nCode\nThis week I wanted to keep the visual fairly simple so that I could focus my efforts on designing the project in a way that felt intuitive, was easy to read, and had a manageable workflow. To achieve these objectives I broke the project into three key functions: cleaning, visualizing, and styling - organized within the ‘Functions’ folder for that weeks submission. The index.R file integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaning FunctionVisual FunctionStyle FunctionIndex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# dplyr: data cleaning functions.\nbase::library(dplyr)\n#### Cleaning Function ####\nclean <- function(df){\n  # clean function\n  clean_df <- df %>%\n    dplyr::group_by(season) %>%\n    dplyr::summarise(\n      total_num_colors = base::sum(num_colors))\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Visual Function ####\nvis <- function(clean_df){\n  vis <- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(\n      x = season,\n      y = total_num_colors)) +\n    ggplot2::geom_point() + \n    ggplot2::geom_segment(\n      ggplot2::aes(\n        x = season,\n        xend = season,\n        y = 0,\n        yend = total_num_colors)) + \n    ggplot2::geom_hline(\n      yintercept=137.871, \n      linetype=\"dashed\", \n      color = \"red\") +\n    ggplot2::geom_text(\n      ggplot2::aes(label = total_num_colors,\n                   vjust = -1)\n    )\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Style Function ####\nsty <- function(vis){\n  sty <- vis +\n    # labels \n    ggplot2::labs(\n      title = \"Colors Used Each Season\",\n      subtitle = \"The dashed red line shows the average number of colors used each season, 137.871.\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Season\",\n      y = \"Colors Used\"\n    ) + \n    # themes\n    ggplot2::theme_classic() + \n    ggplot2::theme(\n      plot.title = element_text(\n        face = \"bold\",\n        hjust = .5),\n      plot.subtitle = element_text(\n        hjust = .5)\n    ) \n  return(sty)\n  }\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# bob_ross: data about and from \"The Joy of Painting\".\ntt_data <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-21/bob_ross.csv')\n\n#### Clean Data ####\nclean_data <- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data <- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis <- sty(vis_data)\n\n#### Save Plot ####\nggplot2::ggsave(\n  base::paste0(\"plot.png\"), \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 8 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo make a lollipop graph using ggplot2 you need to assign geom_point() and geom_segment().\ngoem_hline() was used to created the dashed horizontal red line.\ngeom_text() is used to add the values above the lollipops.\nUsing the TidyTuesdayR package can be problematic.\nIt looks nicer to keep all labels in the labs() function."
  },
  {
    "objectID": "01_blog/2023_06_13_W24-TidyTuesday-SAFI-Teaching-Data/index.html",
    "href": "01_blog/2023_06_13_W24-TidyTuesday-SAFI-Teaching-Data/index.html",
    "title": "Week 24 Tidy Tuesday: SAFI Teaching Data",
    "section": "",
    "text": "SAFI Data\nThe data this week comes from the SAFI (Studying African Farmer-Led Irrigation) survey, a subset of the data used in the Data Carpentry Social Sciences workshop. So, if you’re looking how to learn how to work with this data, lessons are already available! Data is available through Figshare.\nCITATION: Woodhouse, Philip; Veldwisch, Gert Jan; Brockington, Daniel; Komakech, Hans C.; Manjichi, Angela; Venot, Jean-Philippe (2018): SAFI Survey Results. doi:10.6084/m9.figshare.6262019.v1\n\nSAFI (Studying African Farmer-Led Irrigation) is a currently running project which is looking at farming and irrigation methods. This is survey data relating to households and agriculture in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017 using forms downloaded to Android Smartphones. The survey forms were created using the ODK (Open Data Kit) software via an Excel spreadsheet. The collected data is then sent back to a central server. The server can be used to download the collected data in both JSON and CSV formats. This is a teaching version of the collected data that we will be using. It is not the full dataset.\n\n\nThe survey covered such things as; household features (e.g. construction materials used, number of household members), agricultural practices (e.g. water usage), assets (e.g. number and types of livestock) and details about the household members.\n\n\nThe basic teaching dataset used in these lessons is a subset of the JSON dataset that has been converted into CSV format.\n\n\n\n\nCode\nThis week in the index.R file I tried using the tidytuesdayR package and the variable week <- c(24) in attempt to streamline updating the index file for future weeks. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean <- function(df){\n  # extract data from list\n  extracted_df <- df[[1]]\n  clean_df <- extracted_df %>% \n    dplyr::count(rooms, liv_count)\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis <- function(clean_df){\n  vis <- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = rooms, y = liv_count, fill = n)) +\n    geom_tile()\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(\"ggplot2\")\nbase::library(\"showtext\")\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"Judson\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 <- \"white\"\ncol2 <- \"#3E3D53\"\ncol3 <- \"#2C6E63\"\ncol4 <- \"#FCE100\"\n\n#### Style Function ####\nsty <- function(vis){\n  sty <- vis +\n    # labs\n    ggplot2::labs(\n      title = \"Relationship Between Live Stock Ownership and Number of Rooms in Household\",\n      subtitle = \"This subset of the SAFI (Studying African Farmer-Led Irrigation) data shows that of the 131 people surveyed most lived in a house with 1 room,\\n and had 1 cattle. There is no obvious evidence to indicate that the number of rooms in a home is related to the number of live stock owned.\",\n      caption = \"Randi Bolt - June 2023 \\n#TidyTuesday: SAFI Data - June 2017\",\n      x = \"Rooms\",\n      y = \"Live Stock\",\n      fill = \"Surveyed\")  +\n    # add numbers in boxes\n    ggplot2::geom_text(\n      ggplot2::aes(label = n),\n      color = col1,\n      size = 12,\n      family = \"font\") + \n    # scale color\n    ggplot2::scale_fill_gradient(low = col3, high = col4) +\n    # Axis Breaks \n    ggplot2::scale_x_continuous(\n      breaks = seq(1,8,1)) + \n    ggplot2::scale_y_continuous(\n      breaks = seq(1,5,1)) + \n # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 18,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      axis.title = element_text(\n        size = 24, \n        family = \"font\",\n        color = col1),\n      axis.text = element_text(\n        size = 18, \n        family = \"font\",\n        color = col1),\n      legend.title = element_text(\n        size = 24,\n        family = \"font\",\n        color = col1),\n      legend.text = element_text(\n        size = 16,\n        family = \"font\",\n        color = col1),\n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.background = element_rect(fill = col2))\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\nweek <- c(\"24\")\n#### Load Packages ####\nbase::library(tidyverse)\nbase::library(tidytuesdayR)\n\n#### Load Data ####\ntt_data <- tidytuesdayR::tt_load(2023, week = base::as.integer(week))\n\n\n\n    Downloading file 1 of 1: `safi_data.csv`\n\n\nShow Code\n#### Clean Data ####\nclean_data <- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data <- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis <- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\",\n  dpi = 150)\n\n\n\n\nWeek 24 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nI used ggplot2::geom_tile() to create the heat map graph.\nI used ggplot2::scale_fill_gradient() to assign a “high” color and “low” color, but it might make the graph easier to read if I picked 5 colors instead.\nI would have liked white horizontal lines on this graph, but was only getting the vertical lines to show. Will need to do more research on this in future weeks.\nWhen I use the showtext package to assign fonts to the text on my graphs and go to save them as a .png file, if I don’t update the dpi = value (dots per inch) then I have huge spaces in between lines of text. I found that the larger the dpi value the more space there is between lines of text. 150 dpi seems to be a good dpi for this graph.\nI am still debating using the TidyTuesdayR package. It is nice to have the link to the .csv file avaialble in the code."
  },
  {
    "objectID": "01_blog/2023_05_30_W22-Centenarians-Data/index.html",
    "href": "01_blog/2023_05_30_W22-Centenarians-Data/index.html",
    "title": "Week 22 Tidy Tuesday: Centenarians Data",
    "section": "",
    "text": "Verified Oldest People\nThe data this week comes from the Wikipedia List of the verified oldest people via frankiethull on GitHub. Thank you for the submission, Frank!\n\nThese are lists of the 100 known verified oldest people sorted in descending order by age in years and days. The oldest person ever whose age has been independently verified is Jeanne Calment (1875–1997) of France, who lived to the age of 122 years and 164 days. The oldest verified man ever is Jiroemon Kimura (1897–2013) of Japan, who lived to the age of 116 years and 54 days. The oldest known living person is Maria Branyas of Spain, aged 116 years, 85 days. The oldest known living man is Juan Vicente Pérez of Venezuela, aged 114 years, 1 day. The 100 oldest women have, on average, lived several years longer than the 100 oldest men.\n\n\n\n\nCode\nThis week I wanted to do more to spice up a bar plot. So I gave the visual a dark black background, and highlighted the main findings in red. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %>% pipe function. \n# dplyr: data cleaning functions. \nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean <- function(df){\n  clean_df <- df %>%\n    dplyr::mutate(death_month = base::format(death_date, \"%m\")) %>%\n    dplyr::group_by(death_month) %>%\n    dplyr::summarise(count = dplyr::n()) %>%\n    # highlight jan. \n    mutate(to_highlight = ifelse(death_month == \"01\", \"yes\", \"no\"))\n  clean_df$to_highlight[13] <- \"no\"\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis <- function(clean_df){\n  vis <- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = death_month, y = count, fill = to_highlight)) +\n    geom_bar(stat = \"identity\")\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\nbase::library(ggchicklet)\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"EB Garamond\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 <- \"white\"\ncol2 <- \"#0d0d0d\"\ncol3 <- \"#b22b2e\"\ncol4 <- \"#6d6a6e\"\n\n#### Style Function ####\nsty <- function(vis){\n  sty <- vis +\n    # labs\n    ggplot2::labs(\n      title = \"High Number of Centenarian Deaths in January\",\n      subtitle = \"Of the 200 oldest verified men and women,\\n the most common month for a centarian to die in is January.\",\n      caption = \"\\nRandi Bolt ~ #TidyTuesday - Verified Oldest People ~ May 2023\",\n      x = \"Death Month\",\n      y = \"Count\")  +\n    # ylim\n    ylim(0, 36) +\n    # add numbers above boxes\n    ggplot2::geom_text(\n      ggplot2::aes(\n        label = count,\n        vjust = -.5),\n      color = col1,\n      size = 20\n    ) + \n    # scales \n    ggplot2::scale_x_discrete(\n      labels = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"Alive\")\n    ) +\n    scale_fill_manual(values  = c(\"yes\" = col3, \"no\"= col4), guide = \"none\") + \n    # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 100,\n        family = \"font\",\n        face = \"bold\",\n        hjust = .5,\n        vjust = -5,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 55,\n        family = \"font\",\n        hjust = .5,\n        vjust = -20,\n        color = col1),\n      plot.caption = element_text(\n        size = 30,\n        family = \"font\",\n        hjust = .5,\n        color = col1),\n      axis.title = element_text(\n        size = 50, \n        family = \"font\",\n        face = \"bold\",\n        color = col1),\n      axis.text = element_text(\n        size = 45, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(angle = 25), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank()) +               \n    # ggplot2 barplot with round corners\n    geom_chicklet(radius = grid::unit(3, \"mm\")) \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse)\n\n#### Load Data ####\n# centenarians: 100 verified oldest men and women.\ncentenarians <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-30/centenarians.csv')\n\n#### Clean Data ####\nclean_data <- clean(centenarians)\n\n#### Create Data Visual ####\nvis_data <- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis <- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 22 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo highlight the month of January in the cleaning file I used the dplyr::mutate() function to create a “to_highlight” column for any death month equal to January. Then in the visualize package I assigned the fill aesthetic to the “to_highlight” column.\nTo curve the edges of the boxes (so they look more like tombstones) in the styling file I used ggchicklet::geom_chicklet() to assign a curved radius of 3 milimeters.\nTo angle the labels on the x-axis I used ggplot2::theme(axis.text.x = element_text(angle = 25)).\nSpacing is a little funky, but I think it might work for this visual.\nFrequentist data visuals seem to be the low hanging fruit for these Tid Tuesday submissions, and my goal is to push onto more predictive and model based visual analysis by the end of the summer."
  },
  {
    "objectID": "01_blog/2023_05_30_W22-TidyTuesday-Centenarians-Data/index.html",
    "href": "01_blog/2023_05_30_W22-TidyTuesday-Centenarians-Data/index.html",
    "title": "Week 22 Tidy Tuesday: Centenarians Data",
    "section": "",
    "text": "Verified Oldest People\nThe data this week comes from the Wikipedia List of the verified oldest people via frankiethull on GitHub. Thank you for the submission, Frank!\n\nThese are lists of the 100 known verified oldest people sorted in descending order by age in years and days. The oldest person ever whose age has been independently verified is Jeanne Calment (1875–1997) of France, who lived to the age of 122 years and 164 days. The oldest verified man ever is Jiroemon Kimura (1897–2013) of Japan, who lived to the age of 116 years and 54 days. The oldest known living person is Maria Branyas of Spain, aged 116 years, 85 days. The oldest known living man is Juan Vicente Pérez of Venezuela, aged 114 years, 1 day. The 100 oldest women have, on average, lived several years longer than the 100 oldest men.\n\n\n\n\nCode\nThis week I wanted to do more to spice up a bar plot. So I gave the visual a dark black background, and highlighted the main findings in red. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %>% pipe function. \n# dplyr: data cleaning functions. \nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean <- function(df){\n  clean_df <- df %>%\n    dplyr::mutate(death_month = base::format(death_date, \"%m\")) %>%\n    dplyr::group_by(death_month) %>%\n    dplyr::summarise(count = dplyr::n()) %>%\n    # highlight jan. \n    mutate(to_highlight = ifelse(death_month == \"01\", \"yes\", \"no\"))\n  clean_df$to_highlight[13] <- \"no\"\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis <- function(clean_df){\n  vis <- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = death_month, y = count, fill = to_highlight)) +\n    geom_bar(stat = \"identity\")\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\nbase::library(ggchicklet)\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"EB Garamond\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 <- \"white\"\ncol2 <- \"#0d0d0d\"\ncol3 <- \"#b22b2e\"\ncol4 <- \"#6d6a6e\"\n\n#### Style Function ####\nsty <- function(vis){\n  sty <- vis +\n    # labs\n    ggplot2::labs(\n      title = \"High Number of Centenarian Deaths in January\",\n      subtitle = \"Of the 200 oldest verified men and women,\\n the most common month for a centarian to die in is January.\",\n      caption = \"\\nRandi Bolt ~ #TidyTuesday - Verified Oldest People ~ May 2023\",\n      x = \"Death Month\",\n      y = \"Count\")  +\n    # ylim\n    ylim(0, 36) +\n    # add numbers above boxes\n    ggplot2::geom_text(\n      ggplot2::aes(\n        label = count,\n        vjust = -.5),\n      color = col1,\n      size = 20\n    ) + \n    # scales \n    ggplot2::scale_x_discrete(\n      labels = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"Alive\")\n    ) +\n    scale_fill_manual(values  = c(\"yes\" = col3, \"no\"= col4), guide = \"none\") + \n    # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 100,\n        family = \"font\",\n        face = \"bold\",\n        hjust = .5,\n        vjust = -5,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 55,\n        family = \"font\",\n        hjust = .5,\n        vjust = -20,\n        color = col1),\n      plot.caption = element_text(\n        size = 30,\n        family = \"font\",\n        hjust = .5,\n        color = col1),\n      axis.title = element_text(\n        size = 50, \n        family = \"font\",\n        face = \"bold\",\n        color = col1),\n      axis.text = element_text(\n        size = 45, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(angle = 25), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank()) +               \n    # ggplot2 barplot with round corners\n    geom_chicklet(radius = grid::unit(3, \"mm\")) \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse)\n\n#### Load Data ####\n# centenarians: 100 verified oldest men and women.\ncentenarians <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-30/centenarians.csv')\n\n#### Clean Data ####\nclean_data <- clean(centenarians)\n\n#### Create Data Visual ####\nvis_data <- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis <- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 22 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo highlight the month of January in the cleaning file I used the dplyr::mutate() function to create a “to_highlight” column for any death month equal to January. Then in the visualize package I assigned the fill aesthetic to the “to_highlight” column.\nTo curve the edges of the boxes (so they look more like tombstones) in the styling file I used ggchicklet::geom_chicklet() to assign a curved radius of 3 milimeters.\nTo angle the labels on the x-axis I used ggplot2::theme(axis.text.x = element_text(angle = 25)).\nSpacing is a little funky, but I think it might work for this visual.\nFrequentist data visuals seem to be the low hanging fruit for these Tid Tuesday submissions, and my goal is to push onto more predictive and model based visual analysis by the end of the summer."
  },
  {
    "objectID": "01_blog/2023_06_20_W25-TidyTuesday-UFO-Sightings-Redux/index.html",
    "href": "01_blog/2023_06_20_W25-TidyTuesday-UFO-Sightings-Redux/index.html",
    "title": "Week 25 Tidy Tuesday: UFO Sightings Redux",
    "section": "",
    "text": "UFO Sightings Redux\nThe data this week comes from the National UFO Reporting Center, cleaned and enriched with data from sunrise-sunset.org by Jon Harmon.\nIf this dataset looks familiar, that’s because we used a version of it back in 2019. The new version adds the last several years of data, adds information about time-of-day, and cleans up some errors in the original dataset. We’d love to see visualizations describing the differences between the 2019 dataset and this new dataset!\n\nThe National UFO Reporting Center was founded in 1974 by noted UFO investigator Robert J. Gribble. The Center’s primary function over the past five decades has been to receive, record, and to the greatest degree possible, corroborate and document reports from individuals who have been witness to unusual, possibly UFO-related events. Throughout its history, the Center has processed over 170,000 reports, and has distributed its information to thousands of individuals.\n\n\n\n\nCode\nThis weeks code retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %>% pipe function. \n# dplyr: data cleaning functions.\n# tidyr: data manipulation functions.\nbase::library(dplyr)\nbase::library(magrittr)\nbase::library(tidyr)\n\n#### Cleaning Function ####\nclean <- function(df){\n  # Filter date for Oregon\n  or_df <- df %>%\n    dplyr::filter(state == c(\"OR\")) %>%\n    dplyr::reframe(\n      shape = shape, \n      duration_seconds = duration_seconds,\n      duration_minutes = duration_seconds/60)\n  # remove nas \n  or_df$duration_seconds <- stats::na.omit(or_df$duration_seconds)\n  # Subset for all data 5 minutes or less\n  clean_df <- \n    base::subset(or_df, \n                 duration_seconds < 301) %>%\n    tidyr::replace_na(list(shape = c(\"not available\")))\n    \n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis <- function(clean_df){\n  vis <- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = shape,\n                 y = duration_minutes,\n                 fill = shape,\n                 color = shape)) +\n    ggplot2::geom_boxplot(outlier.shape = NA) +\n    ggplot2::geom_jitter(\n      na.rm = TRUE, \n      color = \"#d2ff46\", \n      alpha = 0.7)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions.\n# showtext: font functions. \nbase::library(\"ggplot2\")\nbase::library(\"showtext\")\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"Orbitron\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 <- \"#d2ff46\"\ncol2 <- \"#0d0d0d\"\ncol3 <- \"white\"\n\n#### Axis Labels ####\naxis_labs <- c(\"changing\", \"orb\", \"formation\", \"oval\", \"unknown\",\n               \"cigar\", \"rectangle\", \"circle\", \"egg\", \"sphere\", \n               \"other\", \"light\", \"triangle\", \"teardrop\", \"cross\",\n               \"diamond\", \"cylinder\", \"disk\", \"fireball\", \"not available\", \n               \"flash\", \"chevron\", \"cone\")\n\n#### Style Function ####\nsty <- function(vis){\n  sty <- vis +\n    # labs\n    ggplot2::labs(\n      title = \"Shapes in Oregon: Short Sightings\",\n      subtitle = \"\\nThe following box plot displays the distribution of UFO sightings lasting less than 5 minutes in Oregon, categorized by \\ntheir reported shape. Among this subset of sightings, Cylinder, Disk, Fireball, Not Available, Flash, Chevron, and Cone \\nwere more frequently reported with durations of less than two minutes. In contrast, the Changing UFO shape was \\nobserved for a broader duration range, spanning from 2 to 5 minutes. \\n\\nThe asterisk (*) denotes the mean duration in minutes.\\n \",\n      caption = \"Randi Bolt - June 2023 \\n#TidyTuesday: UFO Sightings Redux\",\n      x = \"Shape\",\n      y = \"Duration in Minutes\")  +\n    # x and y axis\n    ggplot2::scale_y_continuous(limits = c(0,5)) + \n    ggplot2::scale_x_discrete(limits = axis_labs) +\n    # variable color and fill colors\n    ggplot2::scale_color_viridis_d() +\n    ggplot2::scale_fill_viridis_d(alpha = 0.6) +\n    # mean point\n    ggplot2::stat_summary(\n      fun = mean, \n      geom = \"point\", \n      shape = 8, \n      size = 4) +\n    # theme\n    ggplot2::theme(\n      # labs \n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 18,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      axis.title = element_text(\n        size = 24, \n        family = \"font\",\n        color = col1),\n      axis.text = element_text(\n        size = 18, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(\n        angle = 55,\n        vjust = .7), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.position = \"none\") \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Week ####\nweek <- c(\"25\")\n\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# tt_data: ufo_sightings\ntt_data <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv')\n\n#### Clean Data ####\nclean_data <- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data <- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis <- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\",\n  dpi = 150)\n\n\n\n\nWeek 25 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nI utilized tidyr::replace_na() in the cleaning function to replace missing values (NA) with the string ‘not available’.\nIn the visualizing function, I opted to use ggplot2::geom_jitter() instead of ggplot2::geom_point() due to the nature of the data. Since the data points represent estimates of sighting durations, and they are mostly in minutes rather than specific seconds, using geom_point() would result in straight lines of dots across the box plots. Therefore, I decided to apply jittering using geom_jitter() to add some randomness to the point positions, which I deemed acceptable for visualizing the estimated sighting durations.\nIn the styling function, I utilized ggplot2::scale_x_discrete(limits = axis_labs) to define the order of the x-axis labels. Note that if you misspell a word, the data will not appear on your graph.\nAs the styling function grows longer, I find myself considering the idea of splitting it up. One approach I might explore is separating the labels into their own distinct function. This way, I can assess whether this change significantly improves the readability of the styling function.\nThis visual raises some concerns, and I acknowledge the need to allocate more time for statistical analysis in the future. The following issues can be identified with this visual.\n\n\nLack of statistical significance in the arrangement of UFO shapes along the x-axis: The order was chosen based on aesthetic and readability considerations, rather than any statistical criteria.\nAbsence of statistical analysis for outlier elimination: Since the data was limited to sightings lasting 5 minutes or less, no statistical analysis was conducted to identify and eliminate outliers."
  },
  {
    "objectID": "01_blog/2023_06_09_Python-Commands/index.html",
    "href": "01_blog/2023_06_09_Python-Commands/index.html",
    "title": "Basic Python Commands for Virtual Environments and Package Management",
    "section": "",
    "text": "Introduction\nIn this blog post, we will explore basic Python commands for managing virtual environments and installing packages. These commands will help you set up isolated environments for your Python projects, ensuring clean and organized development.\nThe information provided here is based on the YouTube video titled “Python Tutorial: VENV (Mac & Linux) - How to Use Virtual Environments with the Built-In venv Module” by Corey Schafer.\nNote: Remember not to commit the virtual environment (venv) and include it in the .gitignore file. However, do commit the requirements.txt file for package version control.\nLet’s dive into the essential Python commands:\n\n\n1. Checking Installed Packages in the Global Environment\nTo view the list of packages installed on your machine in the global environment, execute the following command in your terminal:\n\npip3 list\n\n\n\n2. Creating a Virtual Environment\nTo create a virtual environment, use the following command:\n\npython3 -m venv project_name\n\n\n\n3. Creating a Virtual Environment in a Project Folder\nIt’s important to avoid placing your project files within the virtual environment. Instead, create the virtual environment in a project folder by executing the following command:\n\npython3 -m venv project_folder/project_name\n\n\n\n4. Activating the Virtual Environment\nTo activate the virtual environment, run the appropriate command based on your operating system:\n\nactivate virtual enviroment\n\n\n\n5. Checking the Python Version in the Virtual Environment\nTo determine the Python version being used within the virtual environment, use the following command:\n\nwhich python\n\n\n\n6. Checking Installed Packages in the Virtual Environment\nTo view the packages installed within the virtual environment, execute the following command:\n\npip list\n\n\n\n7. Installing Packages in the Virtual Environment\nTo install packages within the virtual environment, use the following command:\n\npip install package_name\n\n\n\n8. Creating a File of Packages and Versions\nTo create a file containing a list of installed packages and their versions, use the following command:\n\npip freeze > requirements.txt\n\n\n\n9. Installing Packages from a Requirements File\nTo install packages from a requirements file, execute the following command:\n\npip install -r requirements.txt\n\n\n\n10. Deactivating the Virtual Environment\nTo deactivate the virtual environment and return to the global environment, use the following command:\n\ndeactivate \n\n\n\nConclusion\nThese basic Python commands provide a solid foundation for managing virtual environments and package installations. By utilizing virtual environments, you can isolate project dependencies and ensure consistent and reproducible development environments. Remember to create and commit a requirements.txt file for version control of your project’s dependencies, while excluding the virtual environment itself from version control."
  }
]